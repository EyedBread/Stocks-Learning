====================
Title: Deep Learning for Stock Prediction
       Using Numerical & Textual Information
Date: June 2016
====================

====================
Abstract
====================
- Most methods use EITHER numerical or textual information
- Proposed method:
      + Converts newspaper articles into their distributed representations via
        Paragraph Vector
      + Models temporal effects of past events of opening prices with LSTM
- Performance is demonstrated on real-world data from Tokyo Stock Exchange

====================
Introduction
====================
- Bag-of-Words representation of textual information is widely used but limited
- Stock prices between companies should be correlated
- Using textual information should be a time-series 
- In this work:
      + Paragraph Vector applied to obtain continuously distributed
        representation of each news article
      + Use open prices of 10 companies with regression to predict their close
        price
      + Predictive model used is LSTM
      + Test setup on real-data

====================
Related Works
====================

--------------------
SPP with Textual
--------------------
- Bag-of-Words was shown to not be sufficient
- Previous approaches had efficiency issues

--------------------
Discussion about Issues
--------------------
- Textual info should be represented as fixed-length vector
      + Without losing semantics of the words
- Model should deal with time series data since stock price data is ts


====================
Proposed Approach
====================
- Has nice model structure displayed in figure

--------------------
Representation of Textual Info
--------------------
- Paragraph Vector technique used
      + Distributed representation by mapping variable length text to a fixed
        length vector

- Paragraph Vector classified into two categories:
      1. Distributed Memory Model of Paragraph Vectors (PV-DM)
      2. Distributed Bag of Words version of Paragraph Vector(PV-DBOW)

- Designed to predict word(s) in a context
- Every article represented as a vector of fixed-length $d_a$
- Vector representation of articles concerning 10 companies are concatenated to
  get vector $p_t$
- If no article is written on a company for a time step, that company's
  representation is set to a zero vector
- If there are multiple articles for a company, the embeddings for that vector
  are averaged

--------------------
Representation of Numerical
--------------------
- Stock prices of all 10 companies at time step $t$ is $n_t$
- Normalize stock price by company:
      + value = (2 * price - (max - min)) / (max - min)
      + price is the stock price of a given company
      + max and min are the maximum and minimum price of company during training
        dataset period
      + This makes the range from [-1, 1]

--------------------
Concatenation of text and numerical
--------------------
- The input vector ($x_t$) of LSTM is the concatenation of article group vector
  $p_t$ and stock prices vector $n_t$
- Textual will have more dimensions, which may skew results in its favor
- To solve this, we scale the size of these vectors to have the same dimensions
- Use a neural network for this scaling
- Thus,
      + $P_t = W_p p_t + b_p$
      + $N_t = W_n n_t + b_n$
      + $x_t = [P_t N_t]$
- Used to shrink $p_t$ and extend $n_t$ to common dimensions
 
====================
Evaluation
====================
- Checked qualitatively whether the vector representation could express events
  shown in news articles

--------------------
Experimental Settings
--------------------
- Used morning addition of Nikkei newspaper from 2001 to 2008 for experiments
- Years 2001 to 2006 is the training data
- 2007 is the validation data
- 2008 is the test data
- To get fixed length vectors, articles parsed into words to get the vocabulary
- References that news article titles alone provide sufficient information
      + Titles are more helpful for prediction compared to the contents
- They used the news article titles

--------------------
Acquiring Dist. Repr.
--------------------
- Converted articles into their distributions
- Combined into article groups
- Distributed representation of articles:
      + Words/sentences with similar meanings are mapped to similar positions


====================
Market Simulation
====================
- Used LSTM
      + minibatch size of 30
      + One layer LSTM -> unrolled 20 steps
      + 50% dropout on the non-recurrent connections
      + 50 epochs used with Adam optimization

====================
Results
====================

--------------------
Effect. Paragraph Vector
--------------------
- Compared to BoW -- paragraph vector won out for all 5 industries
- For only numerical data, paragraph vector one out for all but 1 industry

--------------------
Effect. of LSTM
--------------------
- LSTM beat out the simple RNN, which runs counter to other paper
