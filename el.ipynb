{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Hyperparameters\n",
    "timesteps = 10  # Number of time steps\n",
    "num_features = 5  # Number of features\n",
    "n_epochs = 100  # Number of epochs\n",
    "learning_rate = 0.001\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(num_features, 50, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(50, 50, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm3 = nn.LSTM(50, 50, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.lstm4 = nn.LSTM(50, 50, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(50, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = x[:, -1, :]  # Take the last output, remove this?\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        # x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(num_features, 50, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.gru2 = nn.GRU(50, 50, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.gru3 = nn.GRU(50, 50, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.gru4 = nn.GRU(50, 50, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(50, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.gru3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.gru4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = x[:, -1, :]  # Take the last output, remove this?\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        # x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train.shape (83, 5)\n",
      "data_val.shape (19, 5)\n",
      "data_test.shape (19, 5)\n",
      "x_train.shape (72, 10, 5)\n",
      "y_train.shape (72, 1)\n",
      "x_val.shape (8, 10, 5)\n",
      "y_val.shape (8, 1)\n",
      "x_test.shape (8, 10, 5)\n",
      "y_test.shape (8, 1)\n",
      "[[ 0.          0.296      -0.1366      0.          0.        ]\n",
      " [ 0.2730357   0.          0.         -0.2423      0.        ]\n",
      " [ 0.4326822   0.          0.          0.          0.        ]\n",
      " [ 0.51015723  0.          0.          0.          0.        ]\n",
      " [ 0.48646392  0.          0.          0.          0.        ]\n",
      " [ 0.28262597 -0.17253333  0.          0.          0.        ]\n",
      " [ 0.73016413 -0.1806      0.          0.          0.        ]\n",
      " [ 1.          0.          0.          0.          0.        ]\n",
      " [ 0.8365936   0.          0.          0.          0.        ]\n",
      " [ 0.79484908  0.          0.          0.          0.        ]]\n",
      "[0.89488809]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#We minmaxed these splits to get (73,9,9) in train/val/test\n",
    "split = 0.69  # Adjust to allocate space for validation set\n",
    "val_split = 0.16  # 15% for validation, and implicitly 15% for test due to remaining percentage\n",
    "sequence_length = 11\n",
    "normalise = True\n",
    "batch_size = 100\n",
    "input_dim = 5\n",
    "input_timesteps = 10\n",
    "neurons = 50\n",
    "epochs = 5\n",
    "prediction_len = 1\n",
    "dense_output = 1\n",
    "drop_out = 0\n",
    "\n",
    "dataframe = pd.read_csv(\"data/original_dataset/source_price.csv\")\n",
    "cols = ['Adj Close', 'wsj_mean_compound', 'cnbc_mean_compound', 'fortune_mean_compound', 'reuters_mean_compound']\n",
    "cols_temp = ['date', 'Adj Close', 'wsj_mean_compound', 'cnbc_mean_compound', 'fortune_mean_compound', 'reuters_mean_compound']\n",
    "\n",
    "len_dataframe = dataframe.shape[0]\n",
    "\n",
    "# Split data into train, validation, and test\n",
    "i_split = int(len(dataframe) * split)\n",
    "i_val = int(len(dataframe) * (split + val_split))\n",
    "\n",
    "data_train = dataframe.get(cols).values[:i_split]\n",
    "data_val = dataframe.get(cols).values[i_split:i_val]\n",
    "data_test = dataframe.get(cols).values[i_val:]\n",
    "\n",
    "len_train = len(data_train)\n",
    "len_val = len(data_val)\n",
    "len_test = len(data_test)\n",
    "len_train_windows = None\n",
    "\n",
    "print('data_train.shape', data_train.shape)\n",
    "print('data_val.shape', data_val.shape)\n",
    "print('data_test.shape', data_test.shape)\n",
    "\n",
    "# Process train data\n",
    "data_windows = []\n",
    "for i in range(len_train - sequence_length):\n",
    "    data_windows.append(data_train[i:i+sequence_length])\n",
    "data_windows = np.array(data_windows).astype(float)\n",
    "window_data = data_windows\n",
    "win_num = window_data.shape[0]\n",
    "col_num = window_data.shape[2]\n",
    "normalised_data = []\n",
    "record_min = []\n",
    "record_max = []\n",
    "\n",
    "# Normalize train data\n",
    "for win_i in range(win_num):\n",
    "    normalised_window = []\n",
    "    for col_i in range(0,1):\n",
    "      temp_col = window_data[win_i,:,col_i]\n",
    "      temp_min = min(temp_col)\n",
    "      record_min.append(temp_min)\n",
    "      temp_col = temp_col - temp_min\n",
    "      temp_max = max(temp_col)\n",
    "      record_max.append(temp_max)\n",
    "      temp_col = temp_col / temp_max\n",
    "      normalised_window.append(temp_col)\n",
    "    for col_i in range(1,col_num):\n",
    "      normalised_window.append(window_data[win_i,:,col_i])\n",
    "    normalised_window = np.array(normalised_window).T\n",
    "    normalised_data.append(normalised_window)\n",
    "normalised_data = np.array(normalised_data)\n",
    "x_train = normalised_data[:, :-1]\n",
    "y_train = normalised_data[:, -1, [0]]\n",
    "print('x_train.shape', x_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "\n",
    "# Process validation data\n",
    "data_windows = []\n",
    "for i in range(len_val - sequence_length):\n",
    "    data_windows.append(data_val[i:i+sequence_length])\n",
    "data_windows = np.array(data_windows).astype(float)\n",
    "window_data = data_windows\n",
    "win_num = window_data.shape[0]\n",
    "normalised_data = []\n",
    "\n",
    "# Normalize validation data\n",
    "for win_i in range(win_num):\n",
    "    normalised_window = []\n",
    "    for col_i in range(0,1):\n",
    "      temp_col = window_data[win_i,:,col_i]\n",
    "      temp_min = min(temp_col)\n",
    "      temp_col = temp_col - temp_min\n",
    "      temp_max = max(temp_col)\n",
    "      temp_col = temp_col / temp_max\n",
    "      normalised_window.append(temp_col)\n",
    "    for col_i in range(1,col_num):\n",
    "      normalised_window.append(window_data[win_i,:,col_i])\n",
    "    normalised_window = np.array(normalised_window).T\n",
    "    normalised_data.append(normalised_window)\n",
    "normalised_data = np.array(normalised_data)\n",
    "x_val = normalised_data[:, :-1]\n",
    "y_val = normalised_data[:, -1, [0]]\n",
    "print('x_val.shape', x_val.shape)\n",
    "print('y_val.shape', y_val.shape)\n",
    "\n",
    "# Process test data\n",
    "data_windows = []\n",
    "for i in range(len_test - sequence_length):\n",
    "    data_windows.append(data_test[i:i+sequence_length])\n",
    "data_windows = np.array(data_windows).astype(float)\n",
    "y_test_ori = data_windows[:, -1, [0]]\n",
    "window_data = data_windows\n",
    "win_num = window_data.shape[0]\n",
    "normalised_data = []\n",
    "\n",
    "# Normalize test data\n",
    "for win_i in range(win_num):\n",
    "    normalised_window = []\n",
    "    for col_i in range(0,1):\n",
    "      temp_col = window_data[win_i,:,col_i]\n",
    "      temp_min = min(temp_col)\n",
    "      temp_col = temp_col - temp_min\n",
    "      temp_max = max(temp_col)\n",
    "      temp_col = temp_col / temp_max\n",
    "      normalised_window.append(temp_col)\n",
    "    for col_i in range(1,col_num):\n",
    "      normalised_window.append(window_data[win_i,:,col_i])\n",
    "    normalised_window = np.array(normalised_window).T\n",
    "    normalised_data.append(normalised_window)\n",
    "normalised_data = np.array(normalised_data)\n",
    "x_test = normalised_data[:, :-1]\n",
    "y_test = normalised_data[:, -1, [0]]\n",
    "print('x_test.shape', x_test.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "print(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 10, 5])\n",
      "torch.Size([72, 1])\n",
      "Training LSTM Model\n",
      "Epoch 1/100, Loss: 0.5322182074189186\n",
      "Epoch 2/100, Loss: 0.1443455621600151\n",
      "Epoch 3/100, Loss: 0.13437049090862274\n",
      "Epoch 4/100, Loss: 0.13468450009822847\n",
      "Epoch 5/100, Loss: 0.13123217225074768\n",
      "Epoch 6/100, Loss: 0.13874868750572206\n",
      "Epoch 7/100, Loss: 0.12576577961444854\n",
      "Epoch 8/100, Loss: 0.1366221696138382\n",
      "Epoch 9/100, Loss: 0.13304941952228547\n",
      "Epoch 10/100, Loss: 0.11117208302021027\n",
      "Epoch 11/100, Loss: 0.09449929669499398\n",
      "Epoch 12/100, Loss: 0.11299969553947449\n",
      "Epoch 13/100, Loss: 0.06886934041976929\n",
      "Epoch 14/100, Loss: 0.078715780377388\n",
      "Epoch 15/100, Loss: 0.06360184475779533\n",
      "Epoch 16/100, Loss: 0.06559820920228958\n",
      "Epoch 17/100, Loss: 0.0667670987546444\n",
      "Epoch 18/100, Loss: 0.0811374008655548\n",
      "Epoch 19/100, Loss: 0.0682803712785244\n",
      "Epoch 20/100, Loss: 0.07927219718694686\n",
      "Epoch 21/100, Loss: 0.07804757207632065\n",
      "Epoch 22/100, Loss: 0.07457466945052146\n",
      "Epoch 23/100, Loss: 0.07134479209780693\n",
      "Epoch 24/100, Loss: 0.059826627373695374\n",
      "Epoch 25/100, Loss: 0.06841614991426467\n",
      "Epoch 26/100, Loss: 0.06975079849362373\n",
      "Epoch 27/100, Loss: 0.06337555088102817\n",
      "Epoch 28/100, Loss: 0.06235356256365776\n",
      "Epoch 29/100, Loss: 0.0767235815525055\n",
      "Epoch 30/100, Loss: 0.05933094024658203\n",
      "Epoch 31/100, Loss: 0.08014628142118455\n",
      "Epoch 32/100, Loss: 0.05262504741549492\n",
      "Epoch 33/100, Loss: 0.06322565972805023\n",
      "Epoch 34/100, Loss: 0.06824276223778725\n",
      "Epoch 35/100, Loss: 0.06999977603554726\n",
      "Epoch 36/100, Loss: 0.0721232995390892\n",
      "Epoch 37/100, Loss: 0.06523487232625484\n",
      "Epoch 38/100, Loss: 0.06911524385213852\n",
      "Epoch 39/100, Loss: 0.054904306307435036\n",
      "Epoch 40/100, Loss: 0.06369889304041862\n",
      "Epoch 41/100, Loss: 0.06106405034661293\n",
      "Epoch 42/100, Loss: 0.0573221854865551\n",
      "Epoch 43/100, Loss: 0.06170122399926185\n",
      "Epoch 44/100, Loss: 0.060931889712810515\n",
      "Epoch 45/100, Loss: 0.05247567221522331\n",
      "Epoch 46/100, Loss: 0.06489053517580032\n",
      "Epoch 47/100, Loss: 0.06709446087479591\n",
      "Epoch 48/100, Loss: 0.060060832649469376\n",
      "Epoch 49/100, Loss: 0.07819315642118455\n",
      "Epoch 50/100, Loss: 0.06482262164354324\n",
      "Epoch 51/100, Loss: 0.06378945782780647\n",
      "Epoch 52/100, Loss: 0.06149304062128067\n",
      "Epoch 53/100, Loss: 0.06068165078759193\n",
      "Epoch 54/100, Loss: 0.059734441339969635\n",
      "Epoch 55/100, Loss: 0.07487236186861992\n",
      "Epoch 56/100, Loss: 0.06752377152442932\n",
      "Epoch 57/100, Loss: 0.06111382469534874\n",
      "Epoch 58/100, Loss: 0.0562505591660738\n",
      "Epoch 59/100, Loss: 0.0651159357279539\n",
      "Epoch 60/100, Loss: 0.06207299642264843\n",
      "Epoch 61/100, Loss: 0.06598956435918808\n",
      "Epoch 62/100, Loss: 0.06229736730456352\n",
      "Epoch 63/100, Loss: 0.057994984835386273\n",
      "Epoch 64/100, Loss: 0.061175412684679034\n",
      "Epoch 65/100, Loss: 0.06378580555319786\n",
      "Epoch 66/100, Loss: 0.06674782931804657\n",
      "Epoch 67/100, Loss: 0.05892002135515213\n",
      "Epoch 68/100, Loss: 0.05905392467975616\n",
      "Epoch 69/100, Loss: 0.05820582062005997\n",
      "Epoch 70/100, Loss: 0.07190133556723595\n",
      "Epoch 71/100, Loss: 0.05402869135141373\n",
      "Epoch 72/100, Loss: 0.06762013025581837\n",
      "Epoch 73/100, Loss: 0.052976951003074646\n",
      "Epoch 74/100, Loss: 0.060755263268947604\n",
      "Epoch 75/100, Loss: 0.067214635014534\n",
      "Epoch 76/100, Loss: 0.056664254516363144\n",
      "Epoch 77/100, Loss: 0.06672091260552407\n",
      "Epoch 78/100, Loss: 0.07228768467903138\n",
      "Epoch 79/100, Loss: 0.059145115315914154\n",
      "Epoch 80/100, Loss: 0.06063443943858147\n",
      "Epoch 81/100, Loss: 0.07055287957191467\n",
      "Epoch 82/100, Loss: 0.07358716353774071\n",
      "Epoch 83/100, Loss: 0.056458258628845216\n",
      "Epoch 84/100, Loss: 0.061281779408454896\n",
      "Epoch 85/100, Loss: 0.06255772784352302\n",
      "Epoch 86/100, Loss: 0.05840926095843315\n",
      "Epoch 87/100, Loss: 0.06505399197340012\n",
      "Epoch 88/100, Loss: 0.052587584406137464\n",
      "Epoch 89/100, Loss: 0.059565534442663194\n",
      "Epoch 90/100, Loss: 0.05875135138630867\n",
      "Epoch 91/100, Loss: 0.05440540052950382\n",
      "Epoch 92/100, Loss: 0.05543656460940838\n",
      "Epoch 93/100, Loss: 0.0612654235213995\n",
      "Epoch 94/100, Loss: 0.04983890503644943\n",
      "Epoch 95/100, Loss: 0.05764078460633755\n",
      "Epoch 96/100, Loss: 0.051266148686409\n",
      "Epoch 97/100, Loss: 0.06670912876725196\n",
      "Epoch 98/100, Loss: 0.05295858532190323\n",
      "Epoch 99/100, Loss: 0.05746457725763321\n",
      "Epoch 100/100, Loss: 0.055921406298875806\n",
      "Training GRU Model\n",
      "Epoch 1/100, Loss: 0.17455142587423325\n",
      "Epoch 2/100, Loss: 0.12853624671697617\n",
      "Epoch 3/100, Loss: 0.15610763430595398\n",
      "Epoch 4/100, Loss: 0.1369325339794159\n",
      "Epoch 5/100, Loss: 0.11539153903722763\n",
      "Epoch 6/100, Loss: 0.10254206210374832\n",
      "Epoch 7/100, Loss: 0.08291384279727936\n",
      "Epoch 8/100, Loss: 0.0659722376614809\n",
      "Epoch 9/100, Loss: 0.06440651416778564\n",
      "Epoch 10/100, Loss: 0.06135428100824356\n",
      "Epoch 11/100, Loss: 0.060267110168933866\n",
      "Epoch 12/100, Loss: 0.06820021718740463\n",
      "Epoch 13/100, Loss: 0.06567796766757965\n",
      "Epoch 14/100, Loss: 0.06389633640646934\n",
      "Epoch 15/100, Loss: 0.05903306901454926\n",
      "Epoch 16/100, Loss: 0.06186217702925205\n",
      "Epoch 17/100, Loss: 0.05653477013111115\n",
      "Epoch 18/100, Loss: 0.05836294293403625\n",
      "Epoch 19/100, Loss: 0.06099647358059883\n",
      "Epoch 20/100, Loss: 0.054698878154158595\n",
      "Epoch 21/100, Loss: 0.06044952273368835\n",
      "Epoch 22/100, Loss: 0.06035306565463543\n",
      "Epoch 23/100, Loss: 0.06152967140078545\n",
      "Epoch 24/100, Loss: 0.0656381070613861\n",
      "Epoch 25/100, Loss: 0.0642821654677391\n",
      "Epoch 26/100, Loss: 0.0569216538220644\n",
      "Epoch 27/100, Loss: 0.05525842234492302\n",
      "Epoch 28/100, Loss: 0.06693694218993188\n",
      "Epoch 29/100, Loss: 0.05962459370493889\n",
      "Epoch 30/100, Loss: 0.05533761903643608\n",
      "Epoch 31/100, Loss: 0.04785938411951065\n",
      "Epoch 32/100, Loss: 0.05588715113699436\n",
      "Epoch 33/100, Loss: 0.056049862504005434\n",
      "Epoch 34/100, Loss: 0.05284460261464119\n",
      "Epoch 35/100, Loss: 0.054747709259390834\n",
      "Epoch 36/100, Loss: 0.057789585739374164\n",
      "Epoch 37/100, Loss: 0.053671499341726304\n",
      "Epoch 38/100, Loss: 0.06098851561546326\n",
      "Epoch 39/100, Loss: 0.060635609179735185\n",
      "Epoch 40/100, Loss: 0.05932179614901543\n",
      "Epoch 41/100, Loss: 0.04635529443621635\n",
      "Epoch 42/100, Loss: 0.05619725584983826\n",
      "Epoch 43/100, Loss: 0.05057062990963459\n",
      "Epoch 44/100, Loss: 0.06351299434900284\n",
      "Epoch 45/100, Loss: 0.05793797299265861\n",
      "Epoch 46/100, Loss: 0.053016849979758265\n",
      "Epoch 47/100, Loss: 0.050673984549939634\n",
      "Epoch 48/100, Loss: 0.05208500102162361\n",
      "Epoch 49/100, Loss: 0.05070459097623825\n",
      "Epoch 50/100, Loss: 0.05265311375260353\n",
      "Epoch 51/100, Loss: 0.054218250513076785\n",
      "Epoch 52/100, Loss: 0.048986582458019255\n",
      "Epoch 53/100, Loss: 0.05211001336574554\n",
      "Epoch 54/100, Loss: 0.054534883424639705\n",
      "Epoch 55/100, Loss: 0.04931708797812462\n",
      "Epoch 56/100, Loss: 0.05059661604464054\n",
      "Epoch 57/100, Loss: 0.045685313642024994\n",
      "Epoch 58/100, Loss: 0.04771871529519558\n",
      "Epoch 59/100, Loss: 0.049373218417167665\n",
      "Epoch 60/100, Loss: 0.05368571132421494\n",
      "Epoch 61/100, Loss: 0.045450958609580996\n",
      "Epoch 62/100, Loss: 0.050131089985370636\n",
      "Epoch 63/100, Loss: 0.05121786743402481\n",
      "Epoch 64/100, Loss: 0.04756108783185482\n",
      "Epoch 65/100, Loss: 0.05165090300142765\n",
      "Epoch 66/100, Loss: 0.05467501878738403\n",
      "Epoch 67/100, Loss: 0.04538654666393995\n",
      "Epoch 68/100, Loss: 0.0530106209218502\n",
      "Epoch 69/100, Loss: 0.054114564508199695\n",
      "Epoch 70/100, Loss: 0.04932003319263458\n",
      "Epoch 71/100, Loss: 0.05238404497504234\n",
      "Epoch 72/100, Loss: 0.05057031475007534\n",
      "Epoch 73/100, Loss: 0.04665187261998653\n",
      "Epoch 74/100, Loss: 0.04980410560965538\n",
      "Epoch 75/100, Loss: 0.05464767329394817\n",
      "Epoch 76/100, Loss: 0.0493246890604496\n",
      "Epoch 77/100, Loss: 0.046124371141195296\n",
      "Epoch 78/100, Loss: 0.049848320335149764\n",
      "Epoch 79/100, Loss: 0.04706554636359215\n",
      "Epoch 80/100, Loss: 0.04956867918372154\n",
      "Epoch 81/100, Loss: 0.06199609339237213\n",
      "Epoch 82/100, Loss: 0.04638615772128105\n",
      "Epoch 83/100, Loss: 0.051039953529834745\n",
      "Epoch 84/100, Loss: 0.04665427505970001\n",
      "Epoch 85/100, Loss: 0.04471231438219547\n",
      "Epoch 86/100, Loss: 0.04019458033144474\n",
      "Epoch 87/100, Loss: 0.046589745581150054\n",
      "Epoch 88/100, Loss: 0.04619376808404922\n",
      "Epoch 89/100, Loss: 0.045443498715758326\n",
      "Epoch 90/100, Loss: 0.050467221438884734\n",
      "Epoch 91/100, Loss: 0.05161198228597641\n",
      "Epoch 92/100, Loss: 0.046982644498348235\n",
      "Epoch 93/100, Loss: 0.04597012139856815\n",
      "Epoch 94/100, Loss: 0.05009098276495934\n",
      "Epoch 95/100, Loss: 0.04071677476167679\n",
      "Epoch 96/100, Loss: 0.05406813025474548\n",
      "Epoch 97/100, Loss: 0.04431202635169029\n",
      "Epoch 98/100, Loss: 0.04126068688929081\n",
      "Epoch 99/100, Loss: 0.04351162686944008\n",
      "Epoch 100/100, Loss: 0.04931514039635658\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Instantiate models\n",
    "lstm_model = LSTMModel()\n",
    "gru_model = GRUModel()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "lstm_optimizer = optim.RMSprop(lstm_model.parameters(), lr=0.0008)\n",
    "gru_optimizer = optim.RMSprop(gru_model.parameters(), lr=0.0008)\n",
    "\n",
    "# Convert data to PyTorch tensors and create DataLoader\n",
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, optimizer, criterion, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "# Train the LSTM model\n",
    "print(\"Training LSTM Model\")\n",
    "train_model(lstm_model, lstm_optimizer, criterion, train_loader, n_epochs)\n",
    "\n",
    "# Train the GRU model\n",
    "print(\"Training GRU Model\")\n",
    "train_model(gru_model, gru_optimizer, criterion, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2)\n"
     ]
    }
   ],
   "source": [
    "lstm_val_predictions = lstm_model(torch.tensor(x_val, dtype=torch.float32)).detach().numpy()\n",
    "gru_val_predictions = gru_model(torch.tensor(x_val, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "# Combine predictions to form new training data for the meta-learner\n",
    "meta_X_train = np.concatenate((lstm_val_predictions, gru_val_predictions), axis=1)\n",
    "\n",
    "print(meta_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the meta-learner model\n",
    "# it's a fully-connect neuralnetwork with three layers; the activation function for this model is the Rectified Linear Unit (ReLu).\n",
    "# NOTE: The paper doesn't specify the number of neurons in the hidden layers, so I'm basing on the stanford paper\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, 20)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "        # self.sigmoid = nn.Sigmoid() \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        # x = self.fc4(x)\n",
    "        # x = self.sigmoid(x) #We also asume a sigmoid activation function for the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karlo\\scoop\\apps\\python\\current\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 0.4925\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.4590\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.4271\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.3959\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.3646\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.3327\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.3006\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.2691\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2403\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2175\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.2059\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.2086\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.2182\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.2222\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.2166\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.2052\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.1929\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1830\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.1765\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1730\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.1714\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1707\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1699\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1686\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1665\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1637\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1602\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1564\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1526\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1491\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1461\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.1438\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.1420\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1405\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1388\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1367\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1339\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1306\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1270\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1233\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1199\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1167\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1134\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1099\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1058\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1009\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0953\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0892\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0829\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1557fc26270>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# LSTM MODEL\n",
    "batch_size=100;\n",
    "model = Sequential()\n",
    "model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences = True))\n",
    "model.add(Dropout(drop_out))\n",
    "model.add(LSTM(neurons,return_sequences = True))\n",
    "model.add(LSTM(neurons,return_sequences =False))\n",
    "model.add(Dropout(drop_out))\n",
    "model.add(Dense(dense_output, activation='linear'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error',\n",
    "                optimizer='adam')\n",
    "# Fit the model\n",
    "model.fit(x_train,y_train,epochs=50,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "[[array([0.667582], dtype=float32)], [array([0.5562453], dtype=float32)], [array([0.5648408], dtype=float32)], [array([0.5654941], dtype=float32)], [array([0.5680066], dtype=float32)], [array([0.43610486], dtype=float32)], [array([0.46016136], dtype=float32)], [array([0.41569602], dtype=float32)]]\n",
      "[2672.4819440593915, 2673.004395392071, 2673.5586309218093, 2673.5835552641925, 2673.6794051757097, 2668.647367818824, 2683.8256670624755, 2690.0092952286873]\n",
      "MSE [2172.55369241]\n",
      "accuracy [0.98393374]\n",
      "mean_error_percent [0.01606626]\n"
     ]
    }
   ],
   "source": [
    "from numpy import newaxis\n",
    "#multi sequence predict\n",
    "data=x_test\n",
    "prediction_seqs = []\n",
    "window_size=sequence_length\n",
    "pre_win_num=int(len(data)/prediction_len)\n",
    "\n",
    "for i in range(0,pre_win_num):\n",
    "    curr_frame = data[i*prediction_len]\n",
    "    predicted = []\n",
    "    for j in range(0,prediction_len):\n",
    "      temp=model.predict(curr_frame[newaxis,:,:])[0]\n",
    "      predicted.append(temp)\n",
    "      curr_frame = curr_frame[1:]\n",
    "      curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
    "    prediction_seqs.append(predicted)\n",
    "    \n",
    "print(prediction_seqs)\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "#de_predicted\n",
    "de_predicted=[]\n",
    "len_pre_win=int(len(data)/prediction_len)\n",
    "len_pre=prediction_len\n",
    "\n",
    "m=0\n",
    "for i in range(0,len_pre_win):\n",
    "    for j in range(0,len_pre):\n",
    "      de_predicted.append(prediction_seqs[i][j][0]*record_max[m]+record_min[m])\n",
    "      m=m+1\n",
    "print(de_predicted)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "error = []\n",
    "diff=y_test.shape[0]-prediction_len*pre_win_num\n",
    "\n",
    "for i in range(y_test_ori.shape[0]-diff):\n",
    "    error.append(y_test_ori[i,] - de_predicted[i])\n",
    "    \n",
    "squaredError = []\n",
    "absError = []\n",
    "for val in error:\n",
    "    squaredError.append(val * val) \n",
    "    absError.append(abs(val))\n",
    "\n",
    "error_percent=[]\n",
    "for i in range(len(error)):\n",
    "    val=absError[i]/y_test_ori[i,]\n",
    "    val=abs(val)\n",
    "    error_percent.append(val)\n",
    "\n",
    "mean_error_percent=sum(error_percent) / len(error_percent)\n",
    "accuracy=1-mean_error_percent\n",
    "\n",
    "MSE=sum(squaredError) / len(squaredError)\n",
    "\n",
    "print(\"MSE\",MSE)\n",
    "print('accuracy',accuracy)\n",
    "print('mean_error_percent',mean_error_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.05640774519270053\n",
      "Epoch 2/50, Loss: 0.03342993260594085\n",
      "Epoch 3/50, Loss: 0.028684740311291534\n",
      "Epoch 4/50, Loss: 0.025240618752036426\n",
      "Epoch 5/50, Loss: 0.020162962609902024\n",
      "Epoch 6/50, Loss: 0.014175346193951555\n",
      "Epoch 7/50, Loss: 0.00963102461683718\n",
      "Epoch 8/50, Loss: 0.008472580772831861\n",
      "Epoch 9/50, Loss: 0.008172692522748548\n",
      "Epoch 10/50, Loss: 0.00780616159318015\n",
      "Epoch 11/50, Loss: 0.007440271088853478\n",
      "Epoch 12/50, Loss: 0.007310820368729765\n",
      "Epoch 13/50, Loss: 0.007259445563249756\n",
      "Epoch 14/50, Loss: 0.007205244655779097\n",
      "Epoch 15/50, Loss: 0.007121378777810605\n",
      "Epoch 16/50, Loss: 0.007170323977334192\n",
      "Epoch 17/50, Loss: 0.00703939230152173\n",
      "Epoch 18/50, Loss: 0.006880594108224614\n",
      "Epoch 19/50, Loss: 0.006770172625692794\n",
      "Epoch 20/50, Loss: 0.006862823873234447\n",
      "Epoch 21/50, Loss: 0.006647300157055724\n",
      "Epoch 22/50, Loss: 0.0064241950276482385\n",
      "Epoch 23/50, Loss: 0.006578618430467031\n",
      "Epoch 24/50, Loss: 0.006283667480602162\n",
      "Epoch 25/50, Loss: 0.006078634978621267\n",
      "Epoch 26/50, Loss: 0.006181728856972768\n",
      "Epoch 27/50, Loss: 0.005773608623712789\n",
      "Epoch 28/50, Loss: 0.005832480914250482\n",
      "Epoch 29/50, Loss: 0.005664534016887046\n",
      "Epoch 30/50, Loss: 0.0053884869630564936\n",
      "Epoch 31/50, Loss: 0.005695174163292904\n",
      "Epoch 32/50, Loss: 0.005048276354500558\n",
      "Epoch 33/50, Loss: 0.0056162502510233026\n",
      "Epoch 34/50, Loss: 0.006320061685983092\n",
      "Epoch 35/50, Loss: 0.005301644614405632\n",
      "Epoch 36/50, Loss: 0.005080582319351379\n",
      "Epoch 37/50, Loss: 0.005403159982051875\n",
      "Epoch 38/50, Loss: 0.005551450143684633\n",
      "Epoch 39/50, Loss: 0.0047242612840818765\n",
      "Epoch 40/50, Loss: 0.004681799439595125\n",
      "Epoch 41/50, Loss: 0.0045004763542237924\n",
      "Epoch 42/50, Loss: 0.004216777459078003\n",
      "Epoch 43/50, Loss: 0.004261885844243807\n",
      "Epoch 44/50, Loss: 0.00491992478055181\n",
      "Epoch 45/50, Loss: 0.004337631260796115\n",
      "Epoch 46/50, Loss: 0.004359952081358642\n",
      "Epoch 47/50, Loss: 0.004776536699864664\n",
      "Epoch 48/50, Loss: 0.0041747841352730575\n",
      "Epoch 49/50, Loss: 0.004095969091395091\n",
      "Epoch 50/50, Loss: 0.004442788580490742\n",
      "multi sequence predict loop\n",
      "(1, 10, 5)\n",
      "[[0.3249836]\n",
      " [0.5448637]]\n",
      "lvl1out tensor([[0.2291]], grad_fn=<AddmmBackward0>)\n",
      "predicted [0.43492365]\n",
      "true [1.]\n",
      "(1, 10, 5)\n",
      "[[0.27776697]\n",
      " [0.3174037 ]]\n",
      "lvl1out tensor([[0.3515]], grad_fn=<AddmmBackward0>)\n",
      "predicted [0.29758534]\n",
      "true [0.85971425]\n",
      "(1, 10, 5)\n",
      "[[0.24771091]\n",
      " [0.5419479 ]]\n",
      "lvl1out tensor([[0.2710]], grad_fn=<AddmmBackward0>)\n",
      "predicted [0.3948294]\n",
      "true [1.]\n",
      "(1, 10, 5)\n",
      "[[0.64518964]\n",
      " [0.69649684]]\n",
      "lvl1out tensor([[0.0342]], grad_fn=<AddmmBackward0>)\n",
      "predicted [0.67084324]\n",
      "true [0.74679456]\n",
      "(1, 10, 5)\n",
      "[[0.68985474]\n",
      " [0.6960084 ]]\n",
      "lvl1out tensor([[0.0245]], grad_fn=<AddmmBackward0>)\n",
      "predicted [0.69293153]\n",
      "true [0.45238494]\n",
      "(1, 10, 5)\n",
      "[[0.22308192]\n",
      " [0.25188643]]\n",
      "lvl1out tensor([[0.4630]], grad_fn=<AddmmBackward0>)\n",
      "predicted [0.23748417]\n",
      "true [0.]\n",
      "(1, 10, 5)\n",
      "[[0.07526235]\n",
      " [0.3065507 ]]\n",
      "lvl1out tensor([[0.4984]], grad_fn=<AddmmBackward0>)\n",
      "predicted [0.19090652]\n",
      "true [0.78632182]\n",
      "(1, 10, 5)\n",
      "[[0.04347748]\n",
      " [0.2513619 ]]\n",
      "lvl1out tensor([[0.5482]], grad_fn=<AddmmBackward0>)\n",
      "predicted [0.14741969]\n",
      "true [0.35482241]\n",
      "[[array([0.43492365], dtype=float32)], [array([0.29758534], dtype=float32)], [array([0.3948294], dtype=float32)], [array([0.67084324], dtype=float32)], [array([0.69293153], dtype=float32)], [array([0.23748417], dtype=float32)], [array([0.19090652], dtype=float32)], [array([0.14741969], dtype=float32)]]\n",
      "[2660.1091900701963, 2663.00462299326, 2667.072712628045, 2677.6026140006616, 2678.445280071311, 2661.070007887839, 2677.8482226838987, 2679.42580673719]\n",
      "MSE [2682.01702475]\n",
      "accuracy [0.98190008]\n",
      "mean_error_percent [0.01809992]\n"
     ]
    }
   ],
   "source": [
    "from numpy import newaxis\n",
    "# Train the meta-learner model\n",
    "meta_model = MetaLearner()\n",
    "meta_criterion = nn.MSELoss()\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=0.008)\n",
    "\n",
    "meta_X_train_tensor = torch.tensor(meta_X_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "meta_train_dataset = TensorDataset(meta_X_train_tensor, y_val_tensor)\n",
    "meta_train_loader = DataLoader(meta_train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "train_model(meta_model, meta_optimizer, meta_criterion, meta_train_loader, 50)\n",
    "\n",
    "prediction_len=1\n",
    "#multi sequence predict\n",
    "data=x_test\n",
    "prediction_seqs = []\n",
    "window_size=sequence_length\n",
    "pre_win_num=int(len(data)/prediction_len)\n",
    "\n",
    "print(\"multi sequence predict loop\")\n",
    "for i in range(0,pre_win_num):\n",
    "    curr_frame = data[i*prediction_len]\n",
    "    predicted = []\n",
    "    for j in range(0,prediction_len):\n",
    "        print(curr_frame[newaxis,:,:].shape)\n",
    "        lvl0gru=gru_model(torch.from_numpy(curr_frame[newaxis,:,:]).float())[0]\n",
    "        lvl0lstm=lstm_model(torch.from_numpy(curr_frame[newaxis,:,:]).float())[0]\n",
    "        lvl1in=np.array([lvl0gru.detach().numpy(),lvl0lstm.detach().numpy()])\n",
    "        print(lvl1in)\n",
    "        lvl1in=torch.from_numpy(lvl1in).float()\n",
    "        \n",
    "        lvl1out=meta_model(lvl1in.T)\n",
    "        print(\"lvl1out\", lvl1out)\n",
    "        # print(\"lvl1in shape\", lvl1in.shape)\n",
    "        predicted.append((lvl0gru.detach().numpy() + lvl0lstm.detach().numpy())/2)\n",
    "        print(\"predicted\", predicted[-1])\n",
    "        print(\"true\", y_test[i*prediction_len+j])\n",
    "        curr_frame = curr_frame[1:]\n",
    "        curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
    "    prediction_seqs.append(predicted)\n",
    "    \n",
    "print(prediction_seqs)\n",
    "\n",
    "de_predicted=[]\n",
    "len_pre_win=int(len(data)/prediction_len)\n",
    "len_pre=prediction_len\n",
    "\n",
    "m=0\n",
    "for i in range(0,len_pre_win):\n",
    "    for j in range(0,len_pre):\n",
    "      de_predicted.append(prediction_seqs[i][j][0]*record_max[m]+record_min[m])\n",
    "      m=m+1\n",
    "print(de_predicted)\n",
    "\n",
    "error = []\n",
    "diff=y_test.shape[0]-prediction_len*pre_win_num\n",
    "\n",
    "for i in range(y_test_ori.shape[0]-diff):\n",
    "    error.append(y_test_ori[i,] - de_predicted[i])\n",
    "    \n",
    "squaredError = []\n",
    "absError = []\n",
    "for val in error:\n",
    "    squaredError.append(val * val) \n",
    "    absError.append(abs(val))\n",
    "\n",
    "error_percent=[]\n",
    "for i in range(len(error)):\n",
    "    val=absError[i]/y_test_ori[i,]\n",
    "    val=abs(val)\n",
    "    error_percent.append(val)\n",
    "\n",
    "mean_error_percent=sum(error_percent) / len(error_percent)\n",
    "accuracy=1-mean_error_percent\n",
    "\n",
    "MSE=sum(squaredError) / len(squaredError)\n",
    "\n",
    "print(\"MSE\",MSE)\n",
    "print('accuracy',accuracy)\n",
    "print('mean_error_percent',mean_error_percent)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.16215416e-02 -1.08273580e-02  7.88724380e-02 -3.33307690e-02\n",
      "    7.41420580e-02]\n",
      "  [ 0.00000000e+00  2.02213240e-02  8.19771910e-02  6.92661290e-02\n",
      "    4.28054450e-02]\n",
      "  [ 4.23474869e-01  2.60716760e-02  8.56365200e-02  4.35797100e-03\n",
      "    5.38117060e-02]\n",
      "  [ 8.37290429e-01 -1.65616280e-02  8.80705360e-02 -1.19285710e-02\n",
      "    3.54122690e-02]\n",
      "  [ 9.13405936e-01 -3.79798510e-02  7.17808980e-02 -2.89818180e-02\n",
      "    3.83915790e-02]\n",
      "  [ 9.52854430e-01  2.80393160e-02  8.17107320e-02  1.24772700e-03\n",
      "    5.82385000e-04]\n",
      "  [ 6.47077624e-01 -2.27106150e-02  7.51553040e-02 -1.34521740e-02\n",
      "    1.95304100e-03]\n",
      "  [ 8.27303424e-01  3.01822090e-02  8.28230080e-02 -1.89333330e-02\n",
      "    1.82742480e-02]\n",
      "  [ 7.89161754e-01 -2.19021860e-02  7.31291990e-02 -9.77190480e-02\n",
      "   -2.19987100e-03]\n",
      "  [ 6.71959238e-01 -4.14457100e-03  7.02473440e-02 -2.83441860e-02\n",
      "    2.11152410e-02]]\n",
      "\n",
      " [[ 0.00000000e+00  2.02213240e-02  8.19771910e-02  6.92661290e-02\n",
      "    4.28054450e-02]\n",
      "  [ 4.23474869e-01  2.60716760e-02  8.56365200e-02  4.35797100e-03\n",
      "    5.38117060e-02]\n",
      "  [ 8.37290429e-01 -1.65616280e-02  8.80705360e-02 -1.19285710e-02\n",
      "    3.54122690e-02]\n",
      "  [ 9.13405936e-01 -3.79798510e-02  7.17808980e-02 -2.89818180e-02\n",
      "    3.83915790e-02]\n",
      "  [ 9.52854430e-01  2.80393160e-02  8.17107320e-02  1.24772700e-03\n",
      "    5.82385000e-04]\n",
      "  [ 6.47077624e-01 -2.27106150e-02  7.51553040e-02 -1.34521740e-02\n",
      "    1.95304100e-03]\n",
      "  [ 8.27303424e-01  3.01822090e-02  8.28230080e-02 -1.89333330e-02\n",
      "    1.82742480e-02]\n",
      "  [ 7.89161754e-01 -2.19021860e-02  7.31291990e-02 -9.77190480e-02\n",
      "   -2.19987100e-03]\n",
      "  [ 6.71959238e-01 -4.14457100e-03  7.02473440e-02 -2.83441860e-02\n",
      "    2.11152410e-02]\n",
      "  [ 1.00000000e+00  2.97644630e-02  8.04385060e-02  7.84476190e-02\n",
      "   -9.07568900e-03]]\n",
      "\n",
      " [[ 0.00000000e+00  2.60716760e-02  8.56365200e-02  4.35797100e-03\n",
      "    5.38117060e-02]\n",
      "  [ 7.12113493e-01 -1.65616280e-02  8.80705360e-02 -1.19285710e-02\n",
      "    3.54122690e-02]\n",
      "  [ 8.43096676e-01 -3.79798510e-02  7.17808980e-02 -2.89818180e-02\n",
      "    3.83915790e-02]\n",
      "  [ 9.10981521e-01  2.80393160e-02  8.17107320e-02  1.24772700e-03\n",
      "    5.82385000e-04]\n",
      "  [ 3.84786254e-01 -2.27106150e-02  7.51553040e-02 -1.34521740e-02\n",
      "    1.95304100e-03]\n",
      "  [ 6.94927380e-01  3.01822090e-02  8.28230080e-02 -1.89333330e-02\n",
      "    1.82742480e-02]\n",
      "  [ 6.29291380e-01 -2.19021860e-02  7.31291990e-02 -9.77190480e-02\n",
      "   -2.19987100e-03]\n",
      "  [ 4.27603718e-01 -4.14457100e-03  7.02473440e-02 -2.83441860e-02\n",
      "    2.11152410e-02]\n",
      "  [ 9.92111859e-01  2.97644630e-02  8.04385060e-02  7.84476190e-02\n",
      "   -9.07568900e-03]\n",
      "  [ 7.50701465e-01 -2.34718750e-02  6.92551810e-02  5.61754400e-03\n",
      "   -1.44423630e-02]]\n",
      "\n",
      " [[ 5.32054495e-01 -1.65616280e-02  8.80705360e-02 -1.19285710e-02\n",
      "    3.54122690e-02]\n",
      "  [ 7.44961284e-01 -3.79798510e-02  7.17808980e-02 -2.89818180e-02\n",
      "    3.83915790e-02]\n",
      "  [ 8.55304795e-01  2.80393160e-02  8.17107320e-02  1.24772700e-03\n",
      "    5.82385000e-04]\n",
      "  [ 0.00000000e+00 -2.27106150e-02  7.51553040e-02 -1.34521740e-02\n",
      "    1.95304100e-03]\n",
      "  [ 5.04119306e-01  3.01822090e-02  8.28230080e-02 -1.89333330e-02\n",
      "    1.82742480e-02]\n",
      "  [ 3.97431183e-01 -2.19021860e-02  7.31291990e-02 -9.77190480e-02\n",
      "   -2.19987100e-03]\n",
      "  [ 6.95977049e-02 -4.14457100e-03  7.02473440e-02 -2.83441860e-02\n",
      "    2.11152410e-02]\n",
      "  [ 9.87178211e-01  2.97644630e-02  8.04385060e-02  7.84476190e-02\n",
      "   -9.07568900e-03]\n",
      "  [ 5.94777365e-01 -2.34718750e-02  6.92551810e-02  5.61754400e-03\n",
      "   -1.44423630e-02]\n",
      "  [ 1.00000000e+00  2.57458560e-02  6.70460380e-02 -7.47919350e-02\n",
      "    6.90967100e-03]]\n",
      "\n",
      " [[ 7.44961284e-01 -3.79798510e-02  7.17808980e-02 -2.89818180e-02\n",
      "    3.83915790e-02]\n",
      "  [ 8.55304795e-01  2.80393160e-02  8.17107320e-02  1.24772700e-03\n",
      "    5.82385000e-04]\n",
      "  [ 0.00000000e+00 -2.27106150e-02  7.51553040e-02 -1.34521740e-02\n",
      "    1.95304100e-03]\n",
      "  [ 5.04119306e-01  3.01822090e-02  8.28230080e-02 -1.89333330e-02\n",
      "    1.82742480e-02]\n",
      "  [ 3.97431183e-01 -2.19021860e-02  7.31291990e-02 -9.77190480e-02\n",
      "   -2.19987100e-03]\n",
      "  [ 6.95977049e-02 -4.14457100e-03  7.02473440e-02 -2.83441860e-02\n",
      "    2.11152410e-02]\n",
      "  [ 9.87178211e-01  2.97644630e-02  8.04385060e-02  7.84476190e-02\n",
      "   -9.07568900e-03]\n",
      "  [ 5.94777365e-01 -2.34718750e-02  6.92551810e-02  5.61754400e-03\n",
      "   -1.44423630e-02]\n",
      "  [ 1.00000000e+00  2.57458560e-02  6.70460380e-02 -7.47919350e-02\n",
      "    6.90967100e-03]\n",
      "  [ 7.46794564e-01  2.84989900e-02  4.05946650e-02 -5.92966100e-02\n",
      "    1.65081400e-03]]\n",
      "\n",
      " [[ 9.27235530e-01  2.80393160e-02  8.17107320e-02  1.24772700e-03\n",
      "    5.82385000e-04]\n",
      "  [ 4.97118992e-01 -2.27106150e-02  7.51553040e-02 -1.34521740e-02\n",
      "    1.95304100e-03]\n",
      "  [ 7.50631016e-01  3.01822090e-02  8.28230080e-02 -1.89333330e-02\n",
      "    1.82742480e-02]\n",
      "  [ 6.96979585e-01 -2.19021860e-02  7.31291990e-02 -9.77190480e-02\n",
      "   -2.19987100e-03]\n",
      "  [ 5.32118356e-01 -4.14457100e-03  7.02473440e-02 -2.83441860e-02\n",
      "    2.11152410e-02]\n",
      "  [ 9.93552166e-01  2.97644630e-02  8.04385060e-02  7.84476190e-02\n",
      "   -9.07568900e-03]\n",
      "  [ 7.96221233e-01 -2.34718750e-02  6.92551810e-02  5.61754400e-03\n",
      "   -1.44423630e-02]\n",
      "  [ 1.00000000e+00  2.57458560e-02  6.70460380e-02 -7.47919350e-02\n",
      "    6.90967100e-03]\n",
      "  [ 8.72667795e-01  2.84989900e-02  4.05946650e-02 -5.92966100e-02\n",
      "    1.65081400e-03]\n",
      "  [ 7.24614789e-01  3.02902300e-02  4.74332790e-02  1.15500000e-02\n",
      "   -2.51903090e-02]]\n",
      "\n",
      " [[ 4.97118992e-01 -2.27106150e-02  7.51553040e-02 -1.34521740e-02\n",
      "    1.95304100e-03]\n",
      "  [ 7.50631016e-01  3.01822090e-02  8.28230080e-02 -1.89333330e-02\n",
      "    1.82742480e-02]\n",
      "  [ 6.96979585e-01 -2.19021860e-02  7.31291990e-02 -9.77190480e-02\n",
      "   -2.19987100e-03]\n",
      "  [ 5.32118356e-01 -4.14457100e-03  7.02473440e-02 -2.83441860e-02\n",
      "    2.11152410e-02]\n",
      "  [ 9.93552166e-01  2.97644630e-02  8.04385060e-02  7.84476190e-02\n",
      "   -9.07568900e-03]\n",
      "  [ 7.96221233e-01 -2.34718750e-02  6.92551810e-02  5.61754400e-03\n",
      "   -1.44423630e-02]\n",
      "  [ 1.00000000e+00  2.57458560e-02  6.70460380e-02 -7.47919350e-02\n",
      "    6.90967100e-03]\n",
      "  [ 8.72667795e-01  2.84989900e-02  4.05946650e-02 -5.92966100e-02\n",
      "    1.65081400e-03]\n",
      "  [ 7.24614789e-01  3.02902300e-02  4.74332790e-02  1.15500000e-02\n",
      "   -2.51903090e-02]\n",
      "  [ 0.00000000e+00 -5.27955880e-02  7.04423850e-02 -2.57214290e-02\n",
      "   -3.55684430e-02]]\n",
      "\n",
      " [[ 7.50631016e-01  3.01822090e-02  8.28230080e-02 -1.89333330e-02\n",
      "    1.82742480e-02]\n",
      "  [ 6.96979585e-01 -2.19021860e-02  7.31291990e-02 -9.77190480e-02\n",
      "   -2.19987100e-03]\n",
      "  [ 5.32118356e-01 -4.14457100e-03  7.02473440e-02 -2.83441860e-02\n",
      "    2.11152410e-02]\n",
      "  [ 9.93552166e-01  2.97644630e-02  8.04385060e-02  7.84476190e-02\n",
      "   -9.07568900e-03]\n",
      "  [ 7.96221233e-01 -2.34718750e-02  6.92551810e-02  5.61754400e-03\n",
      "   -1.44423630e-02]\n",
      "  [ 1.00000000e+00  2.57458560e-02  6.70460380e-02 -7.47919350e-02\n",
      "    6.90967100e-03]\n",
      "  [ 8.72667795e-01  2.84989900e-02  4.05946650e-02 -5.92966100e-02\n",
      "    1.65081400e-03]\n",
      "  [ 7.24614789e-01  3.02902300e-02  4.74332790e-02  1.15500000e-02\n",
      "   -2.51903090e-02]\n",
      "  [ 0.00000000e+00 -5.27955880e-02  7.04423850e-02 -2.57214290e-02\n",
      "   -3.55684430e-02]\n",
      "  [ 7.86321816e-01 -1.73672730e-02  3.81189660e-02 -7.69645160e-02\n",
      "   -6.31774110e-02]]]\n",
      "(8, 1)\n",
      "[2643.9266661805405, 2662.345201964934, 2660.138610953653, 2653.7095265161047, 2654.6236482614736, 2669.8364608738343, 2686.49048114328, 2698.976290018135]\n",
      "[[2733.01001 ]\n",
      " [2724.439941]\n",
      " [2733.290039]\n",
      " [2727.76001 ]\n",
      " [2721.330078]\n",
      " [2689.860107]\n",
      " [2724.01001 ]\n",
      " [2705.27002 ]]\n",
      "Price Fluctuations: [1, 0, 0, 1, 1, 1, 1]\n",
      "Price Fluctuations Actual: [0, 1, 0, 0, 0, 1, 0]\n",
      "Accuracy: 0.2857142857142857\n",
      "Precision: 0.2\n",
      "Recall: 0.5\n",
      "F1 Score: 0.28571428571428575\n",
      "MSE: 3362.841768450763\n"
     ]
    }
   ],
   "source": [
    "#  the test dataset will be input into the sub-models again to produce intermediate test data for the meta-learner. Afterward, the meta-learner will use the intermediate test predictions from the sub-models to make the final predictions.\n",
    "print(x_test)\n",
    "lstm_test_predictions = lstm_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "gru_test_predictions = gru_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "meta_X_test = np.concatenate((lstm_test_predictions, gru_test_predictions), axis=1)\n",
    "meta_X_test_tensor = torch.tensor(meta_X_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "meta_test_predictions = meta_model(meta_X_test_tensor).detach().numpy()\n",
    "print(meta_test_predictions.shape)\n",
    "\n",
    "de_predicted=[]\n",
    "len_pre_win=int(len(data)/prediction_len)\n",
    "len_pre=prediction_len\n",
    "\n",
    "m=0\n",
    "for i in range(0,len_pre_win):\n",
    "    for j in range(0,len_pre):\n",
    "      de_predicted.append(meta_test_predictions[i][j]*record_max[m]+record_min[m])\n",
    "      m=m+1\n",
    "print(de_predicted)\n",
    "\n",
    "# print(meta_test_predictions)\n",
    "print(y_test_ori)\n",
    "\n",
    "# Calculate price fluctuations and determine if the price increases (1) or decreases (0)\n",
    "price_fluctuations = []\n",
    "for i in range(1, len(de_predicted)):\n",
    "    if de_predicted[i] > de_predicted[i-1]:\n",
    "        price_fluctuations.append(1)\n",
    "    else:\n",
    "        price_fluctuations.append(0)\n",
    "\n",
    "print(\"Price Fluctuations:\", price_fluctuations)\n",
    "\n",
    "price_fluctuations_actual = []\n",
    "for i in range(1, len(y_test_ori)):\n",
    "    if y_test_ori[i] > y_test_ori[i-1]:\n",
    "        price_fluctuations_actual.append(1)\n",
    "    else:\n",
    "        price_fluctuations_actual.append(0)\n",
    "\n",
    "print(\"Price Fluctuations Actual:\", price_fluctuations_actual)\n",
    "\n",
    "# Calculate accuracy, precision, and recall and f1\n",
    "true_positive = 0\n",
    "true_negative = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "for i in range(len(price_fluctuations)):\n",
    "    if price_fluctuations[i] == 1 and price_fluctuations_actual[i] == 1:\n",
    "        true_positive += 1\n",
    "    elif price_fluctuations[i] == 0 and price_fluctuations_actual[i] == 0:\n",
    "        true_negative += 1\n",
    "    elif price_fluctuations[i] == 1 and price_fluctuations_actual[i] == 0:\n",
    "        false_positive += 1\n",
    "    else:\n",
    "        false_negative += 1\n",
    "\n",
    "accuracy = (true_positive + true_negative) / len(price_fluctuations)\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "MSE = np.mean((de_predicted- y_test_ori) ** 2)\n",
    "print(f'MSE: {MSE}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
