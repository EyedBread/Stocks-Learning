{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm1): LSTM(5, 50, batch_first=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (lstm2): LSTM(50, 50, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (lstm3): LSTM(50, 50, batch_first=True)\n",
      "  (dropout3): Dropout(p=0.2, inplace=False)\n",
      "  (lstm4): LSTM(50, 50, batch_first=True)\n",
      "  (dropout4): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Hyperparameters\n",
    "timesteps = 10  # Number of time steps\n",
    "num_features = 5  # Number of features\n",
    "n_epochs = 100  # Number of epochs\n",
    "learning_rate = 0.001\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(num_features, 50, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(50, 50, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm3 = nn.LSTM(50, 50, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.lstm4 = nn.LSTM(50, 50, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(50, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = x[:, -1, :]  # Take the last output, remove this?\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# Print out input and output shape\n",
    "model = LSTMModel()\n",
    "print(model)\n",
    "x = torch.randn(1, 10, 5)\n",
    "print(model(x).shape)\n",
    "\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(num_features, 50, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.gru2 = nn.GRU(50, 50, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.gru3 = nn.GRU(50, 50, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.gru4 = nn.GRU(50, 50, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(50, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.gru3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.gru4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = x[:, -1, :]  # Take the last output, remove this?\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  wsj_mean_compound  cnbc_mean_compound  fortune_mean_compound  \\\n",
      "0   2017/12/7              0.296             -0.1366                 0.0000   \n",
      "1   2017/12/8              0.000              0.0000                -0.2423   \n",
      "2  2017/12/11              0.000              0.0000                 0.0000   \n",
      "3  2017/12/12              0.000              0.0000                 0.0000   \n",
      "4  2017/12/13              0.000              0.0000                 0.0000   \n",
      "\n",
      "   reuters_mean_compound    Adj Close  \n",
      "0                    0.0  2636.979980  \n",
      "1                    0.0  2651.500000  \n",
      "2                    0.0  2659.989990  \n",
      "3                    0.0  2664.110107  \n",
      "4                    0.0  2662.850098  \n",
      "(83, 6)\n",
      "[[0.91548214 0.         0.82728766 0.10191495 0.19179757]\n",
      " [0.34690741 0.31576514 0.40678881 0.10191495 0.2415458 ]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.27063404]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.28475031]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.28043329]]\n",
      "     Adj Close\n",
      "0  2636.979980\n",
      "1  2651.500000\n",
      "2  2659.989990\n",
      "3  2664.110107\n",
      "4  2662.850098\n",
      "(110, 10, 5)\n",
      "(110,)\n",
      "(73, 10, 5)\n",
      "[[ 2.9600000e-01 -1.3660000e-01  0.0000000e+00  0.0000000e+00\n",
      "   2.6369800e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00 -2.4230000e-01  0.0000000e+00\n",
      "   2.6515000e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6599900e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6641101e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6628501e+03]\n",
      " [-1.7253333e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6520100e+03]\n",
      " [-1.8060000e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6758101e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6901599e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6814700e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6792500e+03]]\n",
      "[[ 2.9600000e-01 -1.3660000e-01  0.0000000e+00  0.0000000e+00\n",
      "   2.6369800e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00 -2.4230000e-01  0.0000000e+00\n",
      "   2.6515000e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6599900e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6641101e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6628501e+03]\n",
      " [-1.7253333e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6520100e+03]\n",
      " [-1.8060000e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6758101e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6901599e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6814700e+03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   2.6792500e+03]]\n",
      "float32\n",
      "(73,)\n",
      "(18, 10, 5)\n",
      "(18,)\n",
      "(19, 10, 5)\n",
      "(19,)\n"
     ]
    }
   ],
   "source": [
    "import input\n",
    "\n",
    "df = input.load_data('data/original_dataset/source_price.csv')\n",
    "# Partition data into training, validation and test sets. Training data should be from date 12/07/2017 to 04/09/2018, validation data (from 04/10/2018 to 05/04/2018), and test data (from 05/07/2018 to 06/01/2018)\n",
    "print(df.head())\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Hardcodidly extracting the exact dates for the partitioning\n",
    "df_train = df.loc[0:82]\n",
    "df_val = df.loc[83:102]\n",
    "df_test = df.loc[103:]\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "# print(df_val.head())\n",
    "# print(df_val.tail())\n",
    "# print(df_test.head())\n",
    "# print(df_val)\n",
    "\n",
    "df_train = df_train.drop(columns=['date'])\n",
    "df_val = df_val.drop(columns=['date'])\n",
    "df_test = df_test.drop(columns=['date'])\n",
    "\n",
    "\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "df_train = sc.fit_transform(df_train)\n",
    "df_val = sc.transform(df_val)\n",
    "df_test = sc.transform(df_test)\n",
    "print(df_train[0:5])\n",
    "# print(df_train.shape)\n",
    "\n",
    "def create_sequences_numpy_classification(data, n_days):\n",
    "    X, y = [], []\n",
    "    for i in range(n_days, len(data) - 1): \n",
    "        X.append(data.to_numpy()[i-n_days:i])\n",
    "        y.append(1 if data[['Adj Close']].to_numpy()[i, -1] - data[['Adj Close']].to_numpy()[i-1, -1] > 0 else 0) #Classification task\n",
    "    # Delete the first column of X\n",
    "    X = np.delete(X, 0, axis=2)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "print(df[['Adj Close']].head())\n",
    "\n",
    "# df = \n",
    "\n",
    "X, y = create_sequences_numpy_classification(df, timesteps)\n",
    "\n",
    "X_train = X[:83 - timesteps].astype(np.float32) \n",
    "y_train = y[:83 - timesteps].astype(np.float32) \n",
    "X_val = X[83 - timesteps:101 - timesteps].astype(np.float32) \n",
    "y_val = y[83 - timesteps:101 - timesteps].astype(np.float32) \n",
    "X_test = X[101 - timesteps:].astype(np.float32) \n",
    "y_test = y[101 - timesteps:].astype(np.float32) \n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_train.shape)\n",
    "print(X_train[0])\n",
    "print(X_train[0])\n",
    "print(X_train.dtype)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train.shape (93, 5)\n",
      "data_test.shape (28, 5)\n",
      "y_test_ori.shape (18, 1)\n",
      "x_test.shape (18, 9, 5)\n",
      "y_test.shape (18, 1)\n",
      "x_train.shape (83, 9, 5)\n",
      "y_train.shape (83, 1)\n"
     ]
    }
   ],
   "source": [
    "# # TEST\n",
    "# split = (0.77);\n",
    "# sequence_length=10;\n",
    "# normalise= True\n",
    "# input_dim=5\n",
    "# input_timesteps=9\n",
    "# neurons=50\n",
    "# epochs=5\n",
    "# prediction_len=1\n",
    "# dense_output=1\n",
    "# drop_out=0\n",
    "\n",
    "# dataframe = pd.read_csv(\"data/original_dataset/source_price.csv\")\n",
    "# cols = ['Adj Close','wsj_mean_compound','cnbc_mean_compound','fortune_mean_compound',\n",
    "#           'reuters_mean_compound']\n",
    "\n",
    "# len_dataframe=dataframe.shape[0]\n",
    "\n",
    "# i_split = int(len(dataframe) * split)\n",
    "# data_train = dataframe.get(cols).values[:i_split]\n",
    "# data_test  = dataframe.get(cols).values[i_split:]\n",
    "# len_train  = len(data_train)\n",
    "# len_test   = len(data_test)\n",
    "# len_train_windows = None\n",
    "# print('data_train.shape',data_train.shape)\n",
    "# print('data_test.shape',data_test.shape)\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n",
    "# data_train[0:5]\n",
    "\n",
    "\n",
    "# # In[16]:\n",
    "\n",
    "\n",
    "# data_test[0:5]\n",
    "\n",
    "\n",
    "# # In[19]:\n",
    "\n",
    "\n",
    "# #get_test_data   \n",
    "# data_windows = []\n",
    "# for i in range(len_test - sequence_length):\n",
    "#     data_windows.append(data_test[i:i+sequence_length])\n",
    "# data_windows = np.array(data_windows).astype(float)\n",
    "#  # get original y_test\n",
    "# y_test_ori = data_windows[:, -1, [0]]\n",
    "# print('y_test_ori.shape',y_test_ori.shape)\n",
    "\n",
    "# window_data=data_windows\n",
    "# win_num=window_data.shape[0]\n",
    "# col_num=window_data.shape[2]\n",
    "# normalised_data = []\n",
    "# record_min=[]\n",
    "# record_max=[]\n",
    "\n",
    "# #normalize\n",
    "# for win_i in range(0,win_num):\n",
    "#     normalised_window = []\n",
    "#     for col_i in range(0,1):#col_num):\n",
    "#       temp_col=window_data[win_i,:,col_i]\n",
    "#       temp_min=min(temp_col)\n",
    "#       if col_i==0:\n",
    "#         record_min.append(temp_min)#record min\n",
    "#       temp_col=temp_col-temp_min\n",
    "#       temp_max=max(temp_col)\n",
    "#       if col_i==0:\n",
    "#         record_max.append(temp_max)#record max\n",
    "#       temp_col=temp_col/temp_max\n",
    "#       normalised_window.append(temp_col)\n",
    "#     for col_i in range(1,col_num):\n",
    "#       temp_col=window_data[win_i,:,col_i]\n",
    "#       normalised_window.append(temp_col)\n",
    "#     normalised_window = np.array(normalised_window).T\n",
    "#     normalised_data.append(normalised_window)\n",
    "# normalised_data=np.array(normalised_data)\n",
    "\n",
    "# # normalised_data=window_data\n",
    "# data_windows=normalised_data#get_test_data\n",
    "# x_test = data_windows[:, :-1]\n",
    "# y_test = data_windows[:, -1, [0]]\n",
    "\n",
    "\n",
    "# print('x_test.shape',x_test.shape)\n",
    "# print('y_test.shape',y_test.shape)\n",
    "\n",
    "# #get_train_data \n",
    "# data_windows = []\n",
    "# for i in range(len_train - sequence_length):\n",
    "#     data_windows.append(data_train[i:i+sequence_length])\n",
    "# data_windows = np.array(data_windows).astype(float)\n",
    "  \n",
    "# window_data=data_windows\n",
    "# win_num=window_data.shape[0]\n",
    "# col_num=window_data.shape[2]\n",
    "\n",
    "# normalised_data = []\n",
    "\n",
    "# for win_i in range(0,win_num):\n",
    "#     normalised_window = []\n",
    "#     for col_i in range(0,1):#col_num):\n",
    "#       temp_col=window_data[win_i,:,col_i]\n",
    "#       temp_min=min(temp_col)\n",
    "#       if col_i==0:\n",
    "#         record_min.append(temp_min)#record min\n",
    "#       temp_col=temp_col-temp_min\n",
    "#       temp_max=max(temp_col)\n",
    "#       if col_i==0:\n",
    "#         record_max.append(temp_max)#record max\n",
    "#       temp_col=temp_col/temp_max\n",
    "#       normalised_window.append(temp_col)\n",
    "#     for col_i in range(1,col_num):\n",
    "#       temp_col=window_data[win_i,:,col_i]\n",
    "#       normalised_window.append(temp_col)\n",
    "#     normalised_window = np.array(normalised_window).T\n",
    "#     normalised_data.append(normalised_window)\n",
    "# normalised_data=np.array(normalised_data)\n",
    "\n",
    "# # normalised_data=window_data\n",
    "# data_windows=normalised_data\n",
    "# x_train = data_windows[:, :-1]\n",
    "# y_train = data_windows[:, -1,[0]]\n",
    "# print('x_train.shape',x_train.shape)\n",
    "# print('y_train.shape',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 10, 5])\n",
      "torch.Size([73])\n",
      "Training LSTM Model\n",
      "Epoch 1/100, Loss: 0.24574368198712668\n",
      "Epoch 2/100, Loss: 0.2527195264895757\n",
      "Epoch 3/100, Loss: 0.24848264455795288\n",
      "Epoch 4/100, Loss: 0.24863005677858988\n",
      "Epoch 5/100, Loss: 0.2432712862888972\n",
      "Epoch 6/100, Loss: 0.24843832353750864\n",
      "Epoch 7/100, Loss: 0.2512618800004323\n",
      "Epoch 8/100, Loss: 0.24457769095897675\n",
      "Epoch 9/100, Loss: 0.2426991065343221\n",
      "Epoch 10/100, Loss: 0.24718846877415976\n",
      "Epoch 11/100, Loss: 0.2441386729478836\n",
      "Epoch 12/100, Loss: 0.2451886534690857\n",
      "Epoch 13/100, Loss: 0.245221475760142\n",
      "Epoch 14/100, Loss: 0.24247212707996368\n",
      "Epoch 15/100, Loss: 0.24664328495661417\n",
      "Epoch 16/100, Loss: 0.2537573923667272\n",
      "Epoch 17/100, Loss: 0.24687449634075165\n",
      "Epoch 18/100, Loss: 0.252289742231369\n",
      "Epoch 19/100, Loss: 0.24709496398766836\n",
      "Epoch 20/100, Loss: 0.24323380986849466\n",
      "Epoch 21/100, Loss: 0.24151990314324698\n",
      "Epoch 22/100, Loss: 0.2524384409189224\n",
      "Epoch 23/100, Loss: 0.24658349653085074\n",
      "Epoch 24/100, Loss: 0.24309500555197397\n",
      "Epoch 25/100, Loss: 0.24216807385285696\n",
      "Epoch 26/100, Loss: 0.25045005480448407\n",
      "Epoch 27/100, Loss: 0.24712203939755759\n",
      "Epoch 28/100, Loss: 0.24355452259381613\n",
      "Epoch 29/100, Loss: 0.2423200805981954\n",
      "Epoch 30/100, Loss: 0.2396145462989807\n",
      "Epoch 31/100, Loss: 0.23926766713460287\n",
      "Epoch 32/100, Loss: 0.2394975076119105\n",
      "Epoch 33/100, Loss: 0.24635487298170725\n",
      "Epoch 34/100, Loss: 0.25869304438432056\n",
      "Epoch 35/100, Loss: 0.2533791760603587\n",
      "Epoch 36/100, Loss: 0.24123760064442953\n",
      "Epoch 37/100, Loss: 0.24602019786834717\n",
      "Epoch 38/100, Loss: 0.23820026218891144\n",
      "Epoch 39/100, Loss: 0.24491647879282633\n",
      "Epoch 40/100, Loss: 0.2447520593802134\n",
      "Epoch 41/100, Loss: 0.241341695189476\n",
      "Epoch 42/100, Loss: 0.241219828526179\n",
      "Epoch 43/100, Loss: 0.243939479192098\n",
      "Epoch 44/100, Loss: 0.24704286952813467\n",
      "Epoch 45/100, Loss: 0.25006746749083203\n",
      "Epoch 46/100, Loss: 0.24283376336097717\n",
      "Epoch 47/100, Loss: 0.24711671968301138\n",
      "Epoch 48/100, Loss: 0.2463262379169464\n",
      "Epoch 49/100, Loss: 0.24342954655488333\n",
      "Epoch 50/100, Loss: 0.25017114480336505\n",
      "Epoch 51/100, Loss: 0.24815248946348825\n",
      "Epoch 52/100, Loss: 0.24394831558068594\n",
      "Epoch 53/100, Loss: 0.2471099297205607\n",
      "Epoch 54/100, Loss: 0.23734070857365927\n",
      "Epoch 55/100, Loss: 0.25446993112564087\n",
      "Epoch 56/100, Loss: 0.2538186311721802\n",
      "Epoch 57/100, Loss: 0.2507905264695485\n",
      "Epoch 58/100, Loss: 0.23788331945737204\n",
      "Epoch 59/100, Loss: 0.24934764703114828\n",
      "Epoch 60/100, Loss: 0.24598221480846405\n",
      "Epoch 61/100, Loss: 0.24323452512423197\n",
      "Epoch 62/100, Loss: 0.24930832783381143\n",
      "Epoch 63/100, Loss: 0.24391268690427145\n",
      "Epoch 64/100, Loss: 0.2493459681669871\n",
      "Epoch 65/100, Loss: 0.23980501294136047\n",
      "Epoch 66/100, Loss: 0.24525396525859833\n",
      "Epoch 67/100, Loss: 0.2499154955148697\n",
      "Epoch 68/100, Loss: 0.25102633237838745\n",
      "Epoch 69/100, Loss: 0.23891010880470276\n",
      "Epoch 70/100, Loss: 0.2491464763879776\n",
      "Epoch 71/100, Loss: 0.24083949128786722\n",
      "Epoch 72/100, Loss: 0.23490899304548898\n",
      "Epoch 73/100, Loss: 0.24630464613437653\n",
      "Epoch 74/100, Loss: 0.25122064848740894\n",
      "Epoch 75/100, Loss: 0.24684464434782663\n",
      "Epoch 76/100, Loss: 0.2455556591351827\n",
      "Epoch 77/100, Loss: 0.254141961534818\n",
      "Epoch 78/100, Loss: 0.24441024661064148\n",
      "Epoch 79/100, Loss: 0.24101315438747406\n",
      "Epoch 80/100, Loss: 0.2555312067270279\n",
      "Epoch 81/100, Loss: 0.24542817970116934\n",
      "Epoch 82/100, Loss: 0.24322778483231863\n",
      "Epoch 83/100, Loss: 0.24804282685120901\n",
      "Epoch 84/100, Loss: 0.23916642864545187\n",
      "Epoch 85/100, Loss: 0.24279842774073282\n",
      "Epoch 86/100, Loss: 0.24745183686415353\n",
      "Epoch 87/100, Loss: 0.24954891701539358\n",
      "Epoch 88/100, Loss: 0.24521078169345856\n",
      "Epoch 89/100, Loss: 0.2456673483053843\n",
      "Epoch 90/100, Loss: 0.24641722937424979\n",
      "Epoch 91/100, Loss: 0.25046725074450177\n",
      "Epoch 92/100, Loss: 0.2484575609366099\n",
      "Epoch 93/100, Loss: 0.24725693464279175\n",
      "Epoch 94/100, Loss: 0.24875166515509287\n",
      "Epoch 95/100, Loss: 0.2626411517461141\n",
      "Epoch 96/100, Loss: 0.2502132107814153\n",
      "Epoch 97/100, Loss: 0.2478520075480143\n",
      "Epoch 98/100, Loss: 0.24980305135250092\n",
      "Epoch 99/100, Loss: 0.241800457239151\n",
      "Epoch 100/100, Loss: 0.24690491954485574\n",
      "Training GRU Model\n",
      "Epoch 1/100, Loss: 0.24729718267917633\n",
      "Epoch 2/100, Loss: 0.24261427422364554\n",
      "Epoch 3/100, Loss: 0.24982471267382303\n",
      "Epoch 4/100, Loss: 0.2443881779909134\n",
      "Epoch 5/100, Loss: 0.2476935088634491\n",
      "Epoch 6/100, Loss: 0.251974493265152\n",
      "Epoch 7/100, Loss: 0.24907422065734863\n",
      "Epoch 8/100, Loss: 0.24801299969355264\n",
      "Epoch 9/100, Loss: 0.2513914753993352\n",
      "Epoch 10/100, Loss: 0.24443204204241434\n",
      "Epoch 11/100, Loss: 0.2493060032526652\n",
      "Epoch 12/100, Loss: 0.24497001866499582\n",
      "Epoch 13/100, Loss: 0.2391565889120102\n",
      "Epoch 14/100, Loss: 0.24135167400042215\n",
      "Epoch 15/100, Loss: 0.24053439994653067\n",
      "Epoch 16/100, Loss: 0.23547857999801636\n",
      "Epoch 17/100, Loss: 0.2508993595838547\n",
      "Epoch 18/100, Loss: 0.2518153389294942\n",
      "Epoch 19/100, Loss: 0.24540809790293375\n",
      "Epoch 20/100, Loss: 0.24371978640556335\n",
      "Epoch 21/100, Loss: 0.2438456416130066\n",
      "Epoch 22/100, Loss: 0.2424492488304774\n",
      "Epoch 23/100, Loss: 0.24616581201553345\n",
      "Epoch 24/100, Loss: 0.24049163858095804\n",
      "Epoch 25/100, Loss: 0.24780883391698202\n",
      "Epoch 26/100, Loss: 0.2519398828347524\n",
      "Epoch 27/100, Loss: 0.2530271957317988\n",
      "Epoch 28/100, Loss: 0.25827304025491077\n",
      "Epoch 29/100, Loss: 0.24049186209837595\n",
      "Epoch 30/100, Loss: 0.24477412303288779\n",
      "Epoch 31/100, Loss: 0.25159278015295666\n",
      "Epoch 32/100, Loss: 0.24478392799695334\n",
      "Epoch 33/100, Loss: 0.24524568021297455\n",
      "Epoch 34/100, Loss: 0.2440971384445826\n",
      "Epoch 35/100, Loss: 0.2443781942129135\n",
      "Epoch 36/100, Loss: 0.24587753415107727\n",
      "Epoch 37/100, Loss: 0.24664909640947977\n",
      "Epoch 38/100, Loss: 0.23726596434911093\n",
      "Epoch 39/100, Loss: 0.2431160161892573\n",
      "Epoch 40/100, Loss: 0.25299110015233356\n",
      "Epoch 41/100, Loss: 0.2507600337266922\n",
      "Epoch 42/100, Loss: 0.24435550471146902\n",
      "Epoch 43/100, Loss: 0.24086294074853262\n",
      "Epoch 44/100, Loss: 0.2508652110894521\n",
      "Epoch 45/100, Loss: 0.24304253856341043\n",
      "Epoch 46/100, Loss: 0.25103546182314557\n",
      "Epoch 47/100, Loss: 0.25546689331531525\n",
      "Epoch 48/100, Loss: 0.24684024353822073\n",
      "Epoch 49/100, Loss: 0.24822087585926056\n",
      "Epoch 50/100, Loss: 0.24153748154640198\n",
      "Epoch 51/100, Loss: 0.2401983141899109\n",
      "Epoch 52/100, Loss: 0.2458376189072927\n",
      "Epoch 53/100, Loss: 0.240265890955925\n",
      "Epoch 54/100, Loss: 0.237801323334376\n",
      "Epoch 55/100, Loss: 0.25385678311189014\n",
      "Epoch 56/100, Loss: 0.25005069375038147\n",
      "Epoch 57/100, Loss: 0.25019961098829907\n",
      "Epoch 58/100, Loss: 0.25251778960227966\n",
      "Epoch 59/100, Loss: 0.23319732149442038\n",
      "Epoch 60/100, Loss: 0.24472053349018097\n",
      "Epoch 61/100, Loss: 0.24248885611693063\n",
      "Epoch 62/100, Loss: 0.2554685076077779\n",
      "Epoch 63/100, Loss: 0.24414545794328055\n",
      "Epoch 64/100, Loss: 0.23898414770762125\n",
      "Epoch 65/100, Loss: 0.2526007543007533\n",
      "Epoch 66/100, Loss: 0.25137411057949066\n",
      "Epoch 67/100, Loss: 0.24732476969559988\n",
      "Epoch 68/100, Loss: 0.24387885133425394\n",
      "Epoch 69/100, Loss: 0.25155744949976605\n",
      "Epoch 70/100, Loss: 0.2514315148194631\n",
      "Epoch 71/100, Loss: 0.24163269499937692\n",
      "Epoch 72/100, Loss: 0.2483878235022227\n",
      "Epoch 73/100, Loss: 0.24796523650487265\n",
      "Epoch 74/100, Loss: 0.24735830227533975\n",
      "Epoch 75/100, Loss: 0.24106532335281372\n",
      "Epoch 76/100, Loss: 0.24448290467262268\n",
      "Epoch 77/100, Loss: 0.2617167929808299\n",
      "Epoch 78/100, Loss: 0.24242204427719116\n",
      "Epoch 79/100, Loss: 0.24725312987963358\n",
      "Epoch 80/100, Loss: 0.25013454258441925\n",
      "Epoch 81/100, Loss: 0.23830585181713104\n",
      "Epoch 82/100, Loss: 0.2449048509200414\n",
      "Epoch 83/100, Loss: 0.24272323648134866\n",
      "Epoch 84/100, Loss: 0.24408491452534994\n",
      "Epoch 85/100, Loss: 0.24207782248655954\n",
      "Epoch 86/100, Loss: 0.2519759734471639\n",
      "Epoch 87/100, Loss: 0.2397533804178238\n",
      "Epoch 88/100, Loss: 0.24479958911736807\n",
      "Epoch 89/100, Loss: 0.24737909932931265\n",
      "Epoch 90/100, Loss: 0.2545651892820994\n",
      "Epoch 91/100, Loss: 0.2449598511060079\n",
      "Epoch 92/100, Loss: 0.24039405087629953\n",
      "Epoch 93/100, Loss: 0.24648662408192953\n",
      "Epoch 94/100, Loss: 0.24959171811739603\n",
      "Epoch 95/100, Loss: 0.24480407436688742\n",
      "Epoch 96/100, Loss: 0.25596361855665845\n",
      "Epoch 97/100, Loss: 0.24498171110947928\n",
      "Epoch 98/100, Loss: 0.2495816151301066\n",
      "Epoch 99/100, Loss: 0.24224295715490976\n",
      "Epoch 100/100, Loss: 0.25030704339345294\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Instantiate models\n",
    "lstm_model = LSTMModel()\n",
    "gru_model = GRUModel()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "gru_optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch tensors and create DataLoader\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, optimizer, criterion, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "# Train the LSTM model\n",
    "print(\"Training LSTM Model\")\n",
    "train_model(lstm_model, lstm_optimizer, criterion, train_loader, n_epochs)\n",
    "\n",
    "# Train the GRU model\n",
    "print(\"Training GRU Model\")\n",
    "train_model(gru_model, gru_optimizer, criterion, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 2)\n"
     ]
    }
   ],
   "source": [
    "lstm_val_predictions = lstm_model(torch.tensor(X_val, dtype=torch.float32)).detach().numpy()\n",
    "gru_val_predictions = gru_model(torch.tensor(X_val, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "# Combine predictions to form new training data for the meta-learner\n",
    "meta_X_train = np.concatenate((lstm_val_predictions, gru_val_predictions), axis=1)\n",
    "\n",
    "print(meta_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the meta-learner model\n",
    "# it's a fully-connect neuralnetwork with three layers; the activation function for this model is the Rectified Linear Unit (ReLu).\n",
    "# NOTE: The paper doesn't specify the number of neurons in the hidden layers, so I'm basing on the stanford paper\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 30)\n",
    "        self.fc2 = nn.Linear(30, 25)\n",
    "        self.fc3 = nn.Linear(25, 20)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x) #We also asume a sigmoid activation function for the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6965428590774536\n",
      "Epoch 2/100, Loss: 0.6960894465446472\n",
      "Epoch 3/100, Loss: 0.6956515908241272\n",
      "Epoch 4/100, Loss: 0.6952292323112488\n",
      "Epoch 5/100, Loss: 0.6948218941688538\n",
      "Epoch 6/100, Loss: 0.6944290399551392\n",
      "Epoch 7/100, Loss: 0.6940500736236572\n",
      "Epoch 8/100, Loss: 0.6937713027000427\n",
      "Epoch 9/100, Loss: 0.6935024261474609\n",
      "Epoch 10/100, Loss: 0.6932463049888611\n",
      "Epoch 11/100, Loss: 0.6930198073387146\n",
      "Epoch 12/100, Loss: 0.6927861571311951\n",
      "Epoch 13/100, Loss: 0.6925565600395203\n",
      "Epoch 14/100, Loss: 0.6923313140869141\n",
      "Epoch 15/100, Loss: 0.6921104788780212\n",
      "Epoch 16/100, Loss: 0.6918939352035522\n",
      "Epoch 17/100, Loss: 0.6916818618774414\n",
      "Epoch 18/100, Loss: 0.6914741396903992\n",
      "Epoch 19/100, Loss: 0.6912704110145569\n",
      "Epoch 20/100, Loss: 0.6910598278045654\n",
      "Epoch 21/100, Loss: 0.6908572912216187\n",
      "Epoch 22/100, Loss: 0.6906581521034241\n",
      "Epoch 23/100, Loss: 0.6904626488685608\n",
      "Epoch 24/100, Loss: 0.6902710199356079\n",
      "Epoch 25/100, Loss: 0.6900836229324341\n",
      "Epoch 26/100, Loss: 0.6899004578590393\n",
      "Epoch 27/100, Loss: 0.6897242665290833\n",
      "Epoch 28/100, Loss: 0.6895496845245361\n",
      "Epoch 29/100, Loss: 0.6893792152404785\n",
      "Epoch 30/100, Loss: 0.6892157196998596\n",
      "Epoch 31/100, Loss: 0.6890589594841003\n",
      "Epoch 32/100, Loss: 0.6889175176620483\n",
      "Epoch 33/100, Loss: 0.6887760162353516\n",
      "Epoch 34/100, Loss: 0.6886283755302429\n",
      "Epoch 35/100, Loss: 0.6884959936141968\n",
      "Epoch 36/100, Loss: 0.6883700489997864\n",
      "Epoch 37/100, Loss: 0.6882496476173401\n",
      "Epoch 38/100, Loss: 0.6881373524665833\n",
      "Epoch 39/100, Loss: 0.6880306601524353\n",
      "Epoch 40/100, Loss: 0.6879298090934753\n",
      "Epoch 41/100, Loss: 0.6878347992897034\n",
      "Epoch 42/100, Loss: 0.687745988368988\n",
      "Epoch 43/100, Loss: 0.6876633763313293\n",
      "Epoch 44/100, Loss: 0.6875873804092407\n",
      "Epoch 45/100, Loss: 0.6875176429748535\n",
      "Epoch 46/100, Loss: 0.6874545216560364\n",
      "Epoch 47/100, Loss: 0.6873977780342102\n",
      "Epoch 48/100, Loss: 0.6873476505279541\n",
      "Epoch 49/100, Loss: 0.6873036623001099\n",
      "Epoch 50/100, Loss: 0.6872657537460327\n",
      "Epoch 51/100, Loss: 0.6872336864471436\n",
      "Epoch 52/100, Loss: 0.687207043170929\n",
      "Epoch 53/100, Loss: 0.6871858835220337\n",
      "Epoch 54/100, Loss: 0.6871693134307861\n",
      "Epoch 55/100, Loss: 0.6871565580368042\n",
      "Epoch 56/100, Loss: 0.6871477961540222\n",
      "Epoch 57/100, Loss: 0.6871318817138672\n",
      "Epoch 58/100, Loss: 0.6870968341827393\n",
      "Epoch 59/100, Loss: 0.6870954036712646\n",
      "Epoch 60/100, Loss: 0.6870949864387512\n",
      "Epoch 61/100, Loss: 0.687095046043396\n",
      "Epoch 62/100, Loss: 0.6870956420898438\n",
      "Epoch 63/100, Loss: 0.6870962977409363\n",
      "Epoch 64/100, Loss: 0.687096893787384\n",
      "Epoch 65/100, Loss: 0.687097430229187\n",
      "Epoch 66/100, Loss: 0.6870976686477661\n",
      "Epoch 67/100, Loss: 0.687097430229187\n",
      "Epoch 68/100, Loss: 0.6870968341827393\n",
      "Epoch 69/100, Loss: 0.6870958209037781\n",
      "Epoch 70/100, Loss: 0.6870943307876587\n",
      "Epoch 71/100, Loss: 0.6870924234390259\n",
      "Epoch 72/100, Loss: 0.6870901584625244\n",
      "Epoch 73/100, Loss: 0.6870877146720886\n",
      "Epoch 74/100, Loss: 0.6870849132537842\n",
      "Epoch 75/100, Loss: 0.687082052230835\n",
      "Epoch 76/100, Loss: 0.6870791912078857\n",
      "Epoch 77/100, Loss: 0.6870762705802917\n",
      "Epoch 78/100, Loss: 0.6870733499526978\n",
      "Epoch 79/100, Loss: 0.6870706677436829\n",
      "Epoch 80/100, Loss: 0.687067985534668\n",
      "Epoch 81/100, Loss: 0.6870655417442322\n",
      "Epoch 82/100, Loss: 0.6870632171630859\n",
      "Epoch 83/100, Loss: 0.687061071395874\n",
      "Epoch 84/100, Loss: 0.687059223651886\n",
      "Epoch 85/100, Loss: 0.6870574951171875\n",
      "Epoch 86/100, Loss: 0.687055766582489\n",
      "Epoch 87/100, Loss: 0.6870543360710144\n",
      "Epoch 88/100, Loss: 0.687052845954895\n",
      "Epoch 89/100, Loss: 0.6870514154434204\n",
      "Epoch 90/100, Loss: 0.6870500445365906\n",
      "Epoch 91/100, Loss: 0.687048614025116\n",
      "Epoch 92/100, Loss: 0.6870471239089966\n",
      "Epoch 93/100, Loss: 0.687045693397522\n",
      "Epoch 94/100, Loss: 0.687044084072113\n",
      "Epoch 95/100, Loss: 0.6870425343513489\n",
      "Epoch 96/100, Loss: 0.6870408058166504\n",
      "Epoch 97/100, Loss: 0.6870390772819519\n",
      "Epoch 98/100, Loss: 0.6870372295379639\n",
      "Epoch 99/100, Loss: 0.687035322189331\n",
      "Epoch 100/100, Loss: 0.6870334148406982\n"
     ]
    }
   ],
   "source": [
    "# Train the meta-learner model\n",
    "meta_model = MetaLearner()\n",
    "meta_criterion = nn.BCELoss()\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=0.001)\n",
    "\n",
    "meta_X_train_tensor = torch.tensor(meta_X_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "meta_train_dataset = TensorDataset(meta_X_train_tensor, y_val_tensor)\n",
    "meta_train_loader = DataLoader(meta_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_model(meta_model, meta_optimizer, meta_criterion, meta_train_loader, n_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5263157894736842\n",
      "Recall: 19.0\n",
      "Precision: 10.0\n"
     ]
    }
   ],
   "source": [
    "#  the test dataset will be input into the sub-models again to produce intermediate test data for the meta-learner. Afterward, the meta-learner will use the intermediate test predictions from the sub-models to make the final predictions.\n",
    "\n",
    "lstm_test_predictions = lstm_model(torch.tensor(X_test, dtype=torch.float32)).detach().numpy()\n",
    "gru_test_predictions = gru_model(torch.tensor(X_test, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "meta_X_test = np.concatenate((lstm_test_predictions, gru_test_predictions), axis=1)\n",
    "meta_X_test_tensor = torch.tensor(meta_X_test, dtype=torch.float32)\n",
    "\n",
    "meta_test_predictions = meta_model(meta_X_test_tensor).detach().numpy()\n",
    "\n",
    "# Calculate the accuracy of the meta-learner\n",
    "meta_test_predictions = np.round(meta_test_predictions)\n",
    "accuracy = np.mean(meta_test_predictions == y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "# Recall\n",
    "recall = np.sum(np.logical_and(meta_test_predictions == 1, y_test == 1)) / np.sum(y_test)\n",
    "print(f'Recall: {recall}')\n",
    "# Precision\n",
    "precision = np.sum(np.logical_and(meta_test_predictions == 1, y_test == 1)) / np.sum(meta_test_predictions)\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, meta_test_predictions)\n",
    "\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
