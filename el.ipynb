{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Hyperparameters\n",
    "timesteps = 10  # Number of time steps\n",
    "num_features = 5  # Number of features\n",
    "n_epochs = 100  # Number of epochs\n",
    "learning_rate = 0.001\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(num_features, 50, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(50, 50, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm3 = nn.LSTM(50, 50, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.lstm4 = nn.LSTM(50, 50, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(50, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = x[:, -1, :]  # Take the last output, remove this?\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        # x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(num_features, 50, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.gru2 = nn.GRU(50, 50, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.gru3 = nn.GRU(50, 50, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.gru4 = nn.GRU(50, 50, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(50, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.gru3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.gru4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = x[:, -1, :]  # Take the last output, remove this?\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        # x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import input\n",
    "\n",
    "# df = input.load_data('data/original_dataset/source_price.csv')\n",
    "# # Partition data into training, validation and test sets. Training data should be from date 12/07/2017 to 04/09/2018, validation data (from 04/10/2018 to 05/04/2018), and test data (from 05/07/2018 to 06/01/2018)\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Hardcodidly extracting the exact dates for the partitioning\n",
    "# df_train = df.loc[0:82]\n",
    "# df_val = df.loc[83:101]\n",
    "# df_test = df.loc[102:]\n",
    "\n",
    "# print(df_test.head())\n",
    "\n",
    "# print(df_train.shape)\n",
    "# print(df_val.shape)\n",
    "# print(df_test.shape)\n",
    "\n",
    "# # print(df_val.head())\n",
    "# # print(df_val.tail())\n",
    "# # print(df_test.head())\n",
    "# # print(df_val)\n",
    "\n",
    "# df_train = df_train.drop(columns=['date'])\n",
    "# df_val = df_val.drop(columns=['date'])\n",
    "# df_test = df_test.drop(columns=['date'])\n",
    "\n",
    "\n",
    "# sc = MinMaxScaler(feature_range=(0,1))\n",
    "# df_train = sc.fit_transform(df_train)\n",
    "# df_val = sc.transform(df_val)\n",
    "# df_test = sc.transform(df_test)\n",
    "# print(df_train[0:5])\n",
    "# # print(df_train.shape)\n",
    "\n",
    "# def create_sequences_numpy_classification(data, n_days):\n",
    "#     X, y = [], []\n",
    "#     for i in range(n_days, len(data) - 1): \n",
    "#         X.append(data.to_numpy()[i-n_days:i])\n",
    "#         y.append(1 if data[['Adj Close']].to_numpy()[i, -1] - data[['Adj Close']].to_numpy()[i-1, -1] > 0 else 0) #Classification task\n",
    "#     # Delete the first column of X\n",
    "#     X = np.delete(X, 0, axis=2)\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# print(df[['Adj Close']].head())\n",
    "\n",
    "# # df = \n",
    "\n",
    "# X, y = create_sequences_numpy_classification(df, timesteps)\n",
    "\n",
    "# X_train = X[:83 - timesteps].astype(np.float32) \n",
    "# y_train = y[:83 - timesteps].astype(np.float32) \n",
    "# X_val = X[83 - timesteps:101 - timesteps].astype(np.float32) \n",
    "# y_val = y[83 - timesteps:101 - timesteps].astype(np.float32) \n",
    "# X_test = X[101 - timesteps:].astype(np.float32) \n",
    "# y_test = y[101 - timesteps:].astype(np.float32) \n",
    "# print(X.shape)\n",
    "# print(y.shape)\n",
    "# print(X_train.shape)\n",
    "# print(X_train[0])\n",
    "# print(X_train[0])\n",
    "# print(X_train.dtype)\n",
    "# print(y_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(y_val.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# split = (0.77);\n",
    "# sequence_length=10;\n",
    "# normalise= True\n",
    "# input_dim=5\n",
    "# input_timesteps=9\n",
    "# neurons=50\n",
    "# epochs=5\n",
    "# prediction_len=1\n",
    "# dense_output=1\n",
    "# drop_out=0\n",
    "\n",
    "# dataframe = pd.read_csv(\"data/original_dataset/source_price.csv\")\n",
    "# cols = ['Adj Close','wsj_mean_compound','cnbc_mean_compound','fortune_mean_compound',\n",
    "#           'reuters_mean_compound']\n",
    "\n",
    "# len_dataframe=dataframe.shape[0]\n",
    "\n",
    "# i_split = int(len(dataframe) * split)\n",
    "# data_train = dataframe.get(cols).values[:i_split]\n",
    "# data_test  = dataframe.get(cols).values[i_split:]\n",
    "# len_train  = len(data_train)\n",
    "# len_test   = len(data_test)\n",
    "# len_train_windows = None\n",
    "# print('data_train.shape',data_train.shape)\n",
    "# print('data_test.shape',data_test.shape)\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n",
    "# data_train[0:5]\n",
    "\n",
    "\n",
    "# # In[16]:\n",
    "\n",
    "\n",
    "# data_test[0:5]\n",
    "\n",
    "\n",
    "# # In[19]:\n",
    "\n",
    "\n",
    "# #get_test_data   \n",
    "# data_windows = []\n",
    "# for i in range(len_test - sequence_length):\n",
    "#     data_windows.append(data_test[i:i+sequence_length])\n",
    "# data_windows = np.array(data_windows).astype(float)\n",
    "#  # get original y_test\n",
    "# y_test_ori = data_windows[:, -1, [0]]\n",
    "# print('y_test_ori.shape',y_test_ori.shape)\n",
    "\n",
    "# window_data=data_windows\n",
    "# win_num=window_data.shape[0]\n",
    "# col_num=window_data.shape[2]\n",
    "# normalised_data = []\n",
    "# record_min=[]\n",
    "# record_max=[]\n",
    "\n",
    "# #normalize\n",
    "# for win_i in range(0,win_num):\n",
    "#     normalised_window = []\n",
    "#     for col_i in range(0,1):#col_num):\n",
    "#       temp_col=window_data[win_i,:,col_i]\n",
    "#       temp_min=min(temp_col)\n",
    "#       if col_i==0:\n",
    "#         record_min.append(temp_min)#record min\n",
    "#       temp_col=temp_col-temp_min\n",
    "#       temp_max=max(temp_col)\n",
    "#       if col_i==0:\n",
    "#         record_max.append(temp_max)#record max\n",
    "#       temp_col=temp_col/temp_max\n",
    "#       normalised_window.append(temp_col)\n",
    "#     for col_i in range(1,col_num):\n",
    "#       temp_col=window_data[win_i,:,col_i]\n",
    "#       normalised_window.append(temp_col)\n",
    "#     normalised_window = np.array(normalised_window).T\n",
    "#     normalised_data.append(normalised_window)\n",
    "# normalised_data=np.array(normalised_data)\n",
    "\n",
    "# # normalised_data=window_data\n",
    "# data_windows=normalised_data#get_test_data\n",
    "# x_test = data_windows[:, :-1]\n",
    "# y_test = data_windows[:, -1, [0]]\n",
    "\n",
    "\n",
    "# print('x_test.shape',x_test.shape)\n",
    "# print('y_test.shape',y_test.shape)\n",
    "\n",
    "# #get_train_data \n",
    "# data_windows = []\n",
    "# for i in range(len_train - sequence_length):\n",
    "#     data_windows.append(data_train[i:i+sequence_length])\n",
    "# data_windows = np.array(data_windows).astype(float)\n",
    "  \n",
    "# window_data=data_windows\n",
    "# win_num=window_data.shape[0]\n",
    "# col_num=window_data.shape[2]\n",
    "\n",
    "# normalised_data = []\n",
    "\n",
    "# for win_i in range(0,win_num):\n",
    "#     normalised_window = []\n",
    "#     for col_i in range(0,1):#col_num):\n",
    "#       temp_col=window_data[win_i,:,col_i]\n",
    "#       temp_min=min(temp_col)\n",
    "#       if col_i==0:\n",
    "#         record_min.append(temp_min)#record min\n",
    "#       temp_col=temp_col-temp_min\n",
    "#       temp_max=max(temp_col)\n",
    "#       if col_i==0:\n",
    "#         record_max.append(temp_max)#record max\n",
    "#       temp_col=temp_col/temp_max\n",
    "#       normalised_window.append(temp_col)\n",
    "#     for col_i in range(1,col_num):\n",
    "#       temp_col=window_data[win_i,:,col_i]\n",
    "#       normalised_window.append(temp_col)\n",
    "#     normalised_window = np.array(normalised_window).T\n",
    "#     normalised_data.append(normalised_window)\n",
    "# normalised_data=np.array(normalised_data)\n",
    "\n",
    "# # normalised_data=window_data\n",
    "# data_windows=normalised_data\n",
    "# x_train = data_windows[:, :-1]\n",
    "# y_train = data_windows[:, -1,[0]]\n",
    "# print('x_train.shape',x_train.shape)\n",
    "# print('y_train.shape',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train.shape (83, 5)\n",
      "data_valid.shape (19, 5)\n",
      "data_test.shape (19, 5)\n",
      "y_test_ori.shape (9, 1)\n",
      "x_test.shape (9, 9, 5)\n",
      "y_test.shape (9, 1)\n",
      "x_valid.shape (9, 9, 5)\n",
      "y_valid.shape (9, 1)\n",
      "x_train.shape (73, 9, 5)\n",
      "y_train.shape (73, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Parameters\n",
    "split = 0.85\n",
    "sequence_length = 10\n",
    "normalise = True\n",
    "input_dim = 5\n",
    "input_timesteps = 9\n",
    "neurons = 50\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Read data\n",
    "dataframe = pd.read_csv(\"data/original_dataset/source_price.csv\")\n",
    "cols = ['Adj Close', 'wsj_mean_compound', 'cnbc_mean_compound', 'fortune_mean_compound', 'reuters_mean_compound']\n",
    "\n",
    "len_dataframe = dataframe.shape[0]\n",
    "i_split = int(len(dataframe) * split)\n",
    "\n",
    "data_train = dataframe.get(cols).values[:i_split]\n",
    "data_test = dataframe.get(cols).values[i_split:]\n",
    "len_train = len(data_train)\n",
    "len_test = len(data_test)\n",
    "\n",
    "# Ensure validation data has the same length as test data\n",
    "len_valid = len_test\n",
    "data_valid = data_train[-len_valid:]\n",
    "data_train = data_train[:-len_valid]\n",
    "\n",
    "print('data_train.shape', data_train.shape)\n",
    "print('data_valid.shape', data_valid.shape)\n",
    "print('data_test.shape', data_test.shape)\n",
    "\n",
    "# Helper function to create data windows\n",
    "def create_data_windows(data, sequence_length):\n",
    "    data_windows = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        data_windows.append(data[i:i + sequence_length])\n",
    "    return np.array(data_windows).astype(float)\n",
    "\n",
    "# Normalize function\n",
    "def normalize_windows(data_windows):\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "    win_num = data_windows.shape[0]\n",
    "    col_num = data_windows.shape[2]\n",
    "\n",
    "    for win_i in range(win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(col_num):\n",
    "            temp_col = data_windows[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min)\n",
    "            temp_col -= temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if temp_max == 0:\n",
    "                temp_max = 1  # Avoid division by zero\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max)\n",
    "            temp_col /= temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "\n",
    "    return np.array(normalised_data), record_min, record_max\n",
    "\n",
    "# Process test data\n",
    "data_windows = create_data_windows(data_test, sequence_length)\n",
    "y_test_ori = data_windows[:, -1, [0]]\n",
    "print('y_test_ori.shape', y_test_ori.shape)\n",
    "\n",
    "if normalise:\n",
    "    data_windows, _, _ = normalize_windows(data_windows)\n",
    "x_test = data_windows[:, :-1]\n",
    "y_test = data_windows[:, -1, [0]]\n",
    "print('x_test.shape', x_test.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "\n",
    "# Process validation data\n",
    "data_windows = create_data_windows(data_valid, sequence_length)\n",
    "\n",
    "if normalise:\n",
    "    data_windows, _, _ = normalize_windows(data_windows)\n",
    "x_valid = data_windows[:, :-1]\n",
    "y_valid = data_windows[:, -1, [0]]\n",
    "print('x_valid.shape', x_valid.shape)\n",
    "print('y_valid.shape', y_valid.shape)\n",
    "\n",
    "# Process training data\n",
    "data_windows = create_data_windows(data_train, sequence_length)\n",
    "\n",
    "if normalise:\n",
    "    data_windows, record_min, record_max = normalize_windows(data_windows)\n",
    "x_train = data_windows[:, :-1]\n",
    "y_train = data_windows[:, -1, [0]]\n",
    "print('x_train.shape', x_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "\n",
    "# Check for NaN values in the dataset\n",
    "assert not np.any(np.isnan(data_train))\n",
    "assert not np.any(np.isnan(data_valid))\n",
    "assert not np.any(np.isnan(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 9, 5])\n",
      "torch.Size([73, 1])\n",
      "Training LSTM Model\n",
      "Epoch 1/100, Loss: 0.28488999605178833\n",
      "Epoch 2/100, Loss: 0.28308379650115967\n",
      "Epoch 3/100, Loss: 0.22371690471967062\n",
      "Epoch 4/100, Loss: 0.1590967377026876\n",
      "Epoch 5/100, Loss: 0.16827096541722616\n",
      "Epoch 6/100, Loss: 0.13770746191342673\n",
      "Epoch 7/100, Loss: 0.16349068780740103\n",
      "Epoch 8/100, Loss: 0.15105808277924856\n",
      "Epoch 9/100, Loss: 0.16418969134489694\n",
      "Epoch 10/100, Loss: 0.14396325995524725\n",
      "Epoch 11/100, Loss: 0.14853281527757645\n",
      "Epoch 12/100, Loss: 0.12806299577156702\n",
      "Epoch 13/100, Loss: 0.1459327737490336\n",
      "Epoch 14/100, Loss: 0.1346990242600441\n",
      "Epoch 15/100, Loss: 0.13310088713963827\n",
      "Epoch 16/100, Loss: 0.1656397581100464\n",
      "Epoch 17/100, Loss: 0.13339669754107794\n",
      "Epoch 18/100, Loss: 0.1328387757142385\n",
      "Epoch 19/100, Loss: 0.13774819175402322\n",
      "Epoch 20/100, Loss: 0.14094843218723932\n",
      "Epoch 21/100, Loss: 0.1275172879298528\n",
      "Epoch 22/100, Loss: 0.13418375452359518\n",
      "Epoch 23/100, Loss: 0.11576136946678162\n",
      "Epoch 24/100, Loss: 0.12683634956677756\n",
      "Epoch 25/100, Loss: 0.1320487062136332\n",
      "Epoch 26/100, Loss: 0.0900326557457447\n",
      "Epoch 27/100, Loss: 0.07990511258443196\n",
      "Epoch 28/100, Loss: 0.09730063875516255\n",
      "Epoch 29/100, Loss: 0.11031983296076457\n",
      "Epoch 30/100, Loss: 0.09686404218276341\n",
      "Epoch 31/100, Loss: 0.07110349833965302\n",
      "Epoch 32/100, Loss: 0.09093266725540161\n",
      "Epoch 33/100, Loss: 0.09950723499059677\n",
      "Epoch 34/100, Loss: 0.08787850538889568\n",
      "Epoch 35/100, Loss: 0.07723844672242801\n",
      "Epoch 36/100, Loss: 0.06669904539982478\n",
      "Epoch 37/100, Loss: 0.08930981407562892\n",
      "Epoch 38/100, Loss: 0.0728896086414655\n",
      "Epoch 39/100, Loss: 0.09195222705602646\n",
      "Epoch 40/100, Loss: 0.07429270943005879\n",
      "Epoch 41/100, Loss: 0.07971143970886867\n",
      "Epoch 42/100, Loss: 0.07009422034025192\n",
      "Epoch 43/100, Loss: 0.06786731878916423\n",
      "Epoch 44/100, Loss: 0.07520631949106853\n",
      "Epoch 45/100, Loss: 0.06455688551068306\n",
      "Epoch 46/100, Loss: 0.08185250063737233\n",
      "Epoch 47/100, Loss: 0.07627742985884349\n",
      "Epoch 48/100, Loss: 0.06941250214974086\n",
      "Epoch 49/100, Loss: 0.05710247221092383\n",
      "Epoch 50/100, Loss: 0.06972017263372739\n",
      "Epoch 51/100, Loss: 0.07183895880977313\n",
      "Epoch 52/100, Loss: 0.0680563673377037\n",
      "Epoch 53/100, Loss: 0.09359743570288022\n",
      "Epoch 54/100, Loss: 0.0652414175371329\n",
      "Epoch 55/100, Loss: 0.07322728509704272\n",
      "Epoch 56/100, Loss: 0.06629386047522227\n",
      "Epoch 57/100, Loss: 0.06159087394674619\n",
      "Epoch 58/100, Loss: 0.053540914629896484\n",
      "Epoch 59/100, Loss: 0.07329874734083812\n",
      "Epoch 60/100, Loss: 0.06938796242078145\n",
      "Epoch 61/100, Loss: 0.07992841179172198\n",
      "Epoch 62/100, Loss: 0.06431624790032704\n",
      "Epoch 63/100, Loss: 0.05803955222169558\n",
      "Epoch 64/100, Loss: 0.08393065258860588\n",
      "Epoch 65/100, Loss: 0.04746577826639017\n",
      "Epoch 66/100, Loss: 0.05867871269583702\n",
      "Epoch 67/100, Loss: 0.077753031005462\n",
      "Epoch 68/100, Loss: 0.05914087096850077\n",
      "Epoch 69/100, Loss: 0.06836561858654022\n",
      "Epoch 70/100, Loss: 0.06484087804953258\n",
      "Epoch 71/100, Loss: 0.05662966147065163\n",
      "Epoch 72/100, Loss: 0.07214699188868205\n",
      "Epoch 73/100, Loss: 0.06735951205094655\n",
      "Epoch 74/100, Loss: 0.06952195862929027\n",
      "Epoch 75/100, Loss: 0.057113283003369965\n",
      "Epoch 76/100, Loss: 0.06802478556831677\n",
      "Epoch 77/100, Loss: 0.06686742727955182\n",
      "Epoch 78/100, Loss: 0.05878583466013273\n",
      "Epoch 79/100, Loss: 0.05777775247891744\n",
      "Epoch 80/100, Loss: 0.055747561156749725\n",
      "Epoch 81/100, Loss: 0.0726282832523187\n",
      "Epoch 82/100, Loss: 0.057421875496705375\n",
      "Epoch 83/100, Loss: 0.07641978810230891\n",
      "Epoch 84/100, Loss: 0.06963824853301048\n",
      "Epoch 85/100, Loss: 0.08283534646034241\n",
      "Epoch 86/100, Loss: 0.08188187703490257\n",
      "Epoch 87/100, Loss: 0.06771955887476604\n",
      "Epoch 88/100, Loss: 0.06400674829880397\n",
      "Epoch 89/100, Loss: 0.0709531307220459\n",
      "Epoch 90/100, Loss: 0.06428060804804166\n",
      "Epoch 91/100, Loss: 0.05282654116551081\n",
      "Epoch 92/100, Loss: 0.06019820893804232\n",
      "Epoch 93/100, Loss: 0.06089714293678602\n",
      "Epoch 94/100, Loss: 0.058669280260801315\n",
      "Epoch 95/100, Loss: 0.07161206503709157\n",
      "Epoch 96/100, Loss: 0.07562599827845891\n",
      "Epoch 97/100, Loss: 0.06883174180984497\n",
      "Epoch 98/100, Loss: 0.06866099933783214\n",
      "Epoch 99/100, Loss: 0.07289013390739758\n",
      "Epoch 100/100, Loss: 0.06424630433320999\n",
      "Training GRU Model\n",
      "Epoch 1/100, Loss: 0.44636473059654236\n",
      "Epoch 2/100, Loss: 0.24787977834542593\n",
      "Epoch 3/100, Loss: 0.16504201292991638\n",
      "Epoch 4/100, Loss: 0.2116017540295919\n",
      "Epoch 5/100, Loss: 0.18680200477441153\n",
      "Epoch 6/100, Loss: 0.13047264764706293\n",
      "Epoch 7/100, Loss: 0.14646479984124502\n",
      "Epoch 8/100, Loss: 0.16778894265492758\n",
      "Epoch 9/100, Loss: 0.15705145398775736\n",
      "Epoch 10/100, Loss: 0.1489304999510447\n",
      "Epoch 11/100, Loss: 0.15517396728197733\n",
      "Epoch 12/100, Loss: 0.12475272019704182\n",
      "Epoch 13/100, Loss: 0.14822095135847727\n",
      "Epoch 14/100, Loss: 0.14539799590905508\n",
      "Epoch 15/100, Loss: 0.13246702899535498\n",
      "Epoch 16/100, Loss: 0.13334511717160544\n",
      "Epoch 17/100, Loss: 0.13246200482050577\n",
      "Epoch 18/100, Loss: 0.13258876403172812\n",
      "Epoch 19/100, Loss: 0.1320522278547287\n",
      "Epoch 20/100, Loss: 0.12112737198670705\n",
      "Epoch 21/100, Loss: 0.11064469069242477\n",
      "Epoch 22/100, Loss: 0.09036032855510712\n",
      "Epoch 23/100, Loss: 0.09517772495746613\n",
      "Epoch 24/100, Loss: 0.06631096328298251\n",
      "Epoch 25/100, Loss: 0.0777556945880254\n",
      "Epoch 26/100, Loss: 0.1174473837018013\n",
      "Epoch 27/100, Loss: 0.06549809252222379\n",
      "Epoch 28/100, Loss: 0.06543910255034764\n",
      "Epoch 29/100, Loss: 0.07450270652770996\n",
      "Epoch 30/100, Loss: 0.09800867487986882\n",
      "Epoch 31/100, Loss: 0.06893577550848325\n",
      "Epoch 32/100, Loss: 0.1025085411965847\n",
      "Epoch 33/100, Loss: 0.06121271103620529\n",
      "Epoch 34/100, Loss: 0.09144851565361023\n",
      "Epoch 35/100, Loss: 0.09409154951572418\n",
      "Epoch 36/100, Loss: 0.07768904666105907\n",
      "Epoch 37/100, Loss: 0.07813465471069019\n",
      "Epoch 38/100, Loss: 0.06614294399817784\n",
      "Epoch 39/100, Loss: 0.0705556149284045\n",
      "Epoch 40/100, Loss: 0.07871780420343082\n",
      "Epoch 41/100, Loss: 0.0586311419804891\n",
      "Epoch 42/100, Loss: 0.07374610751867294\n",
      "Epoch 43/100, Loss: 0.06016787514090538\n",
      "Epoch 44/100, Loss: 0.06728458156188329\n",
      "Epoch 45/100, Loss: 0.06683214381337166\n",
      "Epoch 46/100, Loss: 0.05071757920086384\n",
      "Epoch 47/100, Loss: 0.06373554343978564\n",
      "Epoch 48/100, Loss: 0.06198979044953982\n",
      "Epoch 49/100, Loss: 0.054560884833335876\n",
      "Epoch 50/100, Loss: 0.07095250363151233\n",
      "Epoch 51/100, Loss: 0.06246967862049738\n",
      "Epoch 52/100, Loss: 0.06704060981671016\n",
      "Epoch 53/100, Loss: 0.08488294233878453\n",
      "Epoch 54/100, Loss: 0.05414306869109472\n",
      "Epoch 55/100, Loss: 0.05475647499163946\n",
      "Epoch 56/100, Loss: 0.06134936213493347\n",
      "Epoch 57/100, Loss: 0.06312516704201698\n",
      "Epoch 58/100, Loss: 0.06263651077946027\n",
      "Epoch 59/100, Loss: 0.06288053840398788\n",
      "Epoch 60/100, Loss: 0.0675562396645546\n",
      "Epoch 61/100, Loss: 0.07131858418385188\n",
      "Epoch 62/100, Loss: 0.07378995046019554\n",
      "Epoch 63/100, Loss: 0.05273060624798139\n",
      "Epoch 64/100, Loss: 0.06020676096280416\n",
      "Epoch 65/100, Loss: 0.0553588488449653\n",
      "Epoch 66/100, Loss: 0.057719928522904716\n",
      "Epoch 67/100, Loss: 0.05327915896972021\n",
      "Epoch 68/100, Loss: 0.06511944284041722\n",
      "Epoch 69/100, Loss: 0.05343379266560078\n",
      "Epoch 70/100, Loss: 0.06744284555315971\n",
      "Epoch 71/100, Loss: 0.06468639274438222\n",
      "Epoch 72/100, Loss: 0.07666494076450665\n",
      "Epoch 73/100, Loss: 0.06224673489729563\n",
      "Epoch 74/100, Loss: 0.05633106827735901\n",
      "Epoch 75/100, Loss: 0.05338565011819204\n",
      "Epoch 76/100, Loss: 0.06155298401912054\n",
      "Epoch 77/100, Loss: 0.07297177240252495\n",
      "Epoch 78/100, Loss: 0.07028267780939738\n",
      "Epoch 79/100, Loss: 0.059286644061406456\n",
      "Epoch 80/100, Loss: 0.061520123233397804\n",
      "Epoch 81/100, Loss: 0.0582142968972524\n",
      "Epoch 82/100, Loss: 0.054332892720897995\n",
      "Epoch 83/100, Loss: 0.06644675011436145\n",
      "Epoch 84/100, Loss: 0.047161352510253586\n",
      "Epoch 85/100, Loss: 0.06235250582297643\n",
      "Epoch 86/100, Loss: 0.04908665890494982\n",
      "Epoch 87/100, Loss: 0.04485491787393888\n",
      "Epoch 88/100, Loss: 0.05855838333566984\n",
      "Epoch 89/100, Loss: 0.06647118677695592\n",
      "Epoch 90/100, Loss: 0.05910452579458555\n",
      "Epoch 91/100, Loss: 0.04928605010112127\n",
      "Epoch 92/100, Loss: 0.05486135805646578\n",
      "Epoch 93/100, Loss: 0.06213809549808502\n",
      "Epoch 94/100, Loss: 0.048920225352048874\n",
      "Epoch 95/100, Loss: 0.06428194046020508\n",
      "Epoch 96/100, Loss: 0.08796531955401103\n",
      "Epoch 97/100, Loss: 0.07170204445719719\n",
      "Epoch 98/100, Loss: 0.07290735840797424\n",
      "Epoch 99/100, Loss: 0.0655929980178674\n",
      "Epoch 100/100, Loss: 0.057549470414717994\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Instantiate models\n",
    "lstm_model = LSTMModel()\n",
    "gru_model = GRUModel()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "gru_optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch tensors and create DataLoader\n",
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, optimizer, criterion, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "# Train the LSTM model\n",
    "print(\"Training LSTM Model\")\n",
    "train_model(lstm_model, lstm_optimizer, criterion, train_loader, n_epochs)\n",
    "\n",
    "# Train the GRU model\n",
    "print(\"Training GRU Model\")\n",
    "train_model(gru_model, gru_optimizer, criterion, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 2)\n"
     ]
    }
   ],
   "source": [
    "lstm_val_predictions = lstm_model(torch.tensor(x_valid, dtype=torch.float32)).detach().numpy()\n",
    "gru_val_predictions = gru_model(torch.tensor(x_valid, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "# Combine predictions to form new training data for the meta-learner\n",
    "meta_X_train = np.concatenate((lstm_val_predictions, gru_val_predictions), axis=1)\n",
    "\n",
    "print(meta_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the meta-learner model\n",
    "# it's a fully-connect neuralnetwork with three layers; the activation function for this model is the Rectified Linear Unit (ReLu).\n",
    "# NOTE: The paper doesn't specify the number of neurons in the hidden layers, so I'm basing on the stanford paper\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 30)\n",
    "        self.fc2 = nn.Linear(30, 25)\n",
    "        self.fc3 = nn.Linear(25, 1)\n",
    "        # self.fc4 = nn.Linear(20, 1)\n",
    "        # self.sigmoid = nn.Sigmoid() \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        # x = self.fc4(x)\n",
    "        # x = self.sigmoid(x) #We also asume a sigmoid activation function for the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.03621266782283783\n",
      "Epoch 2/100, Loss: 0.03588078171014786\n",
      "Epoch 3/100, Loss: 0.03559090197086334\n",
      "Epoch 4/100, Loss: 0.03540235012769699\n",
      "Epoch 5/100, Loss: 0.03522429242730141\n",
      "Epoch 6/100, Loss: 0.03504683077335358\n",
      "Epoch 7/100, Loss: 0.034876417368650436\n",
      "Epoch 8/100, Loss: 0.03469151630997658\n",
      "Epoch 9/100, Loss: 0.034495387226343155\n",
      "Epoch 10/100, Loss: 0.03429414704442024\n",
      "Epoch 11/100, Loss: 0.0341060534119606\n",
      "Epoch 12/100, Loss: 0.033928778022527695\n",
      "Epoch 13/100, Loss: 0.033760130405426025\n",
      "Epoch 14/100, Loss: 0.03360319882631302\n",
      "Epoch 15/100, Loss: 0.03349326178431511\n",
      "Epoch 16/100, Loss: 0.0333959199488163\n",
      "Epoch 17/100, Loss: 0.033308982849121094\n",
      "Epoch 18/100, Loss: 0.033235810697078705\n",
      "Epoch 19/100, Loss: 0.033175621181726456\n",
      "Epoch 20/100, Loss: 0.03311867266893387\n",
      "Epoch 21/100, Loss: 0.033065315335989\n",
      "Epoch 22/100, Loss: 0.03301985561847687\n",
      "Epoch 23/100, Loss: 0.032979194074869156\n",
      "Epoch 24/100, Loss: 0.032920487225055695\n",
      "Epoch 25/100, Loss: 0.03284447640180588\n",
      "Epoch 26/100, Loss: 0.03276829794049263\n",
      "Epoch 27/100, Loss: 0.03269921988248825\n",
      "Epoch 28/100, Loss: 0.03263719379901886\n",
      "Epoch 29/100, Loss: 0.032598331570625305\n",
      "Epoch 30/100, Loss: 0.03257662430405617\n",
      "Epoch 31/100, Loss: 0.032553959637880325\n",
      "Epoch 32/100, Loss: 0.03252987563610077\n",
      "Epoch 33/100, Loss: 0.03250892832875252\n",
      "Epoch 34/100, Loss: 0.03248826786875725\n",
      "Epoch 35/100, Loss: 0.032464757561683655\n",
      "Epoch 36/100, Loss: 0.03244072571396828\n",
      "Epoch 37/100, Loss: 0.03242559731006622\n",
      "Epoch 38/100, Loss: 0.032406147569417953\n",
      "Epoch 39/100, Loss: 0.032381005585193634\n",
      "Epoch 40/100, Loss: 0.032350800931453705\n",
      "Epoch 41/100, Loss: 0.03232845664024353\n",
      "Epoch 42/100, Loss: 0.03230885788798332\n",
      "Epoch 43/100, Loss: 0.032286785542964935\n",
      "Epoch 44/100, Loss: 0.03226199373602867\n",
      "Epoch 45/100, Loss: 0.03223550692200661\n",
      "Epoch 46/100, Loss: 0.032207269221544266\n",
      "Epoch 47/100, Loss: 0.032177750021219254\n",
      "Epoch 48/100, Loss: 0.03214840963482857\n",
      "Epoch 49/100, Loss: 0.032126668840646744\n",
      "Epoch 50/100, Loss: 0.03210228681564331\n",
      "Epoch 51/100, Loss: 0.0320717953145504\n",
      "Epoch 52/100, Loss: 0.03204094246029854\n",
      "Epoch 53/100, Loss: 0.032015979290008545\n",
      "Epoch 54/100, Loss: 0.03198878467082977\n",
      "Epoch 55/100, Loss: 0.03195953741669655\n",
      "Epoch 56/100, Loss: 0.031931065022945404\n",
      "Epoch 57/100, Loss: 0.031900618225336075\n",
      "Epoch 58/100, Loss: 0.0318719856441021\n",
      "Epoch 59/100, Loss: 0.0318436473608017\n",
      "Epoch 60/100, Loss: 0.031815264374017715\n",
      "Epoch 61/100, Loss: 0.03178774565458298\n",
      "Epoch 62/100, Loss: 0.03175996243953705\n",
      "Epoch 63/100, Loss: 0.03172721341252327\n",
      "Epoch 64/100, Loss: 0.0316937230527401\n",
      "Epoch 65/100, Loss: 0.03166264295578003\n",
      "Epoch 66/100, Loss: 0.031634844839572906\n",
      "Epoch 67/100, Loss: 0.03160001337528229\n",
      "Epoch 68/100, Loss: 0.031563594937324524\n",
      "Epoch 69/100, Loss: 0.031533822417259216\n",
      "Epoch 70/100, Loss: 0.0315014086663723\n",
      "Epoch 71/100, Loss: 0.03146734461188316\n",
      "Epoch 72/100, Loss: 0.03143136203289032\n",
      "Epoch 73/100, Loss: 0.03139476478099823\n",
      "Epoch 74/100, Loss: 0.03135809674859047\n",
      "Epoch 75/100, Loss: 0.03133130446076393\n",
      "Epoch 76/100, Loss: 0.03129350021481514\n",
      "Epoch 77/100, Loss: 0.031249871477484703\n",
      "Epoch 78/100, Loss: 0.031215276569128036\n",
      "Epoch 79/100, Loss: 0.031178781762719154\n",
      "Epoch 80/100, Loss: 0.031141124665737152\n",
      "Epoch 81/100, Loss: 0.031102929264307022\n",
      "Epoch 82/100, Loss: 0.031067438423633575\n",
      "Epoch 83/100, Loss: 0.031038522720336914\n",
      "Epoch 84/100, Loss: 0.031001485884189606\n",
      "Epoch 85/100, Loss: 0.03096579760313034\n",
      "Epoch 86/100, Loss: 0.03093119151890278\n",
      "Epoch 87/100, Loss: 0.03089665062725544\n",
      "Epoch 88/100, Loss: 0.030858414247632027\n",
      "Epoch 89/100, Loss: 0.030818022787570953\n",
      "Epoch 90/100, Loss: 0.030775269493460655\n",
      "Epoch 91/100, Loss: 0.03074011579155922\n",
      "Epoch 92/100, Loss: 0.03070363774895668\n",
      "Epoch 93/100, Loss: 0.03066161647439003\n",
      "Epoch 94/100, Loss: 0.030624184757471085\n",
      "Epoch 95/100, Loss: 0.030586889013648033\n",
      "Epoch 96/100, Loss: 0.03054678812623024\n",
      "Epoch 97/100, Loss: 0.03050803765654564\n",
      "Epoch 98/100, Loss: 0.030465206131339073\n",
      "Epoch 99/100, Loss: 0.03042762167751789\n",
      "Epoch 100/100, Loss: 0.030384473502635956\n"
     ]
    }
   ],
   "source": [
    "# Train the meta-learner model\n",
    "meta_model = MetaLearner()\n",
    "meta_criterion = nn.MSELoss()\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=0.001)\n",
    "\n",
    "meta_X_train_tensor = torch.tensor(meta_X_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_valid, dtype=torch.float32)\n",
    "\n",
    "meta_train_dataset = TensorDataset(meta_X_train_tensor, y_val_tensor)\n",
    "meta_train_loader = DataLoader(meta_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_model(meta_model, meta_optimizer, meta_criterion, meta_train_loader, n_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24321267]\n",
      " [0.20371778]\n",
      " [0.23999418]\n",
      " [0.24364202]\n",
      " [0.19661929]\n",
      " [0.10280016]\n",
      " [0.24222371]\n",
      " [0.2333462 ]\n",
      " [0.2698598 ]]\n",
      "[[0.70520661]\n",
      " [1.        ]\n",
      " [0.75667019]\n",
      " [1.        ]\n",
      " [0.74679456]\n",
      " [0.45238494]\n",
      " [0.        ]\n",
      " [0.78632182]\n",
      " [0.35482241]]\n",
      "Accuracy: 0.1111111111111111\n",
      "Recall: 0.0\n",
      "Precision: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karlo\\AppData\\Local\\Temp\\ipykernel_21800\\1223505021.py:22: RuntimeWarning: invalid value encountered in divide\n",
      "  precision = np.sum(np.logical_and(meta_test_predictions == 1, y_test == 1)) / np.sum(meta_test_predictions)\n"
     ]
    }
   ],
   "source": [
    "#  the test dataset will be input into the sub-models again to produce intermediate test data for the meta-learner. Afterward, the meta-learner will use the intermediate test predictions from the sub-models to make the final predictions.\n",
    "\n",
    "lstm_test_predictions = lstm_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "gru_test_predictions = gru_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "meta_X_test = np.concatenate((lstm_test_predictions, gru_test_predictions), axis=1)\n",
    "meta_X_test_tensor = torch.tensor(meta_X_test, dtype=torch.float32)\n",
    "\n",
    "meta_test_predictions = meta_model(meta_X_test_tensor).detach().numpy()\n",
    "\n",
    "print(meta_test_predictions)\n",
    "print(y_test)\n",
    "\n",
    "# Calculate the accuracy of the meta-learner\n",
    "meta_test_predictions = np.round(meta_test_predictions)\n",
    "accuracy = np.mean(meta_test_predictions == y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "# Recall\n",
    "recall = np.sum(np.logical_and(meta_test_predictions == 1, y_test == 1)) / np.sum(y_test)\n",
    "print(f'Recall: {recall}')\n",
    "# Precision\n",
    "precision = np.sum(np.logical_and(meta_test_predictions == 1, y_test == 1)) / np.sum(meta_test_predictions)\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_test_predictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(cm, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\karlo\\scoop\\apps\\python\\current\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\karlo\\scoop\\apps\\python\\current\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:319\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    225\u001b[0m     {\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    236\u001b[0m ):\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32mc:\\Users\\karlo\\scoop\\apps\\python\\current\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:94\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     96\u001b[0m             type_true, type_pred\n\u001b[0;32m     97\u001b[0m         )\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    101\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, meta_test_predictions)\n",
    "\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
