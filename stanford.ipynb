{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the level 1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_features': 9, 'use_time_horizon': False, 'horizon': 10, 'days_forward': 1, 'end_split': 30, 'return_lowest_val_loss': False, 'models': [{'model': 'lstm', 'hidden_size': 50, 'dropout': 0.4, 'learning_rate': 0.0016, 'batch_size': 16, 'num_epochs': 100, 'shuffle': True}, {'model': 'gru', 'hidden_size': 50, 'dropout': 0.4, 'learning_rate': 0.0008, 'batch_size': 16, 'num_epochs': 100, 'shuffle': True}, {'model': 'mlp', 'hidden_size': 4, 'learning_rate': 0.001, 'batch_size': 8, 'num_epochs': 100, 'shuffle': True}]}\n",
      "[{'model': 'lstm', 'hidden_size': 50, 'dropout': 0.4, 'learning_rate': 0.0016, 'batch_size': 16, 'num_epochs': 100, 'shuffle': True}, {'model': 'gru', 'hidden_size': 50, 'dropout': 0.4, 'learning_rate': 0.0008, 'batch_size': 16, 'num_epochs': 100, 'shuffle': True}, {'model': 'mlp', 'hidden_size': 4, 'learning_rate': 0.001, 'batch_size': 8, 'num_epochs': 100, 'shuffle': True}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import yaml\n",
    "from os.path import join\n",
    "\n",
    "import data_prep\n",
    "\n",
    "# Read in config file\n",
    "hyperparams = yaml.safe_load(open('hyperparams.yaml'))\n",
    "print(hyperparams)\n",
    "\n",
    "# Hyperparameters\n",
    "num_features = hyperparams['num_features']  # Number of features\n",
    "use_time_horizon = hyperparams['use_time_horizon']  # Use time horizon\n",
    "HORIZON = hyperparams['horizon']  # Number of days into the future to predict\n",
    "DAYS_FORWARD = hyperparams['days_forward']  # Number of days into the future to predict\n",
    "END_SPLIT = hyperparams['end_split']  # End of the split\n",
    "DATA_PATH = join('data', 'original_dataset', 'Finalised_datasets', 'amzn_all_sources_WITH_TH_2017-2020.csv') #'Finalised_datasets',\n",
    "models = hyperparams['models']  # Models to train\n",
    "\n",
    "print(models)\n",
    "lstm_params = models[0]\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(num_features, lstm_params['hidden_size'], batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(lstm_params['dropout'])\n",
    "        self.lstm2 = nn.LSTM(lstm_params['hidden_size'], lstm_params['hidden_size'], batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(lstm_params['dropout'])\n",
    "        self.lstm3 = nn.LSTM(lstm_params['hidden_size'],lstm_params['hidden_size'], batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(lstm_params['dropout'])\n",
    "        self.lstm4 = nn.LSTM(lstm_params['hidden_size'],lstm_params['hidden_size'], batch_first=True)  \n",
    "        self.dropout4 = nn.Dropout(lstm_params['dropout'])\n",
    "        self.fc = nn.Linear(lstm_params['hidden_size'], 1) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if use_time_horizon:\n",
    "            outputs = []\n",
    "            # For each sequence in the batch\n",
    "            for i in range(x.shape[0]):\n",
    "                x_ele = x[i]\n",
    "                # remove padding\n",
    "                x_ele = x_ele[x_ele[:, 0] != -1]\n",
    "                # print(\"before\", x_ele.shape)\n",
    "                x_ele = x_ele.unsqueeze(0) # Add a batch dimension\n",
    "                # print(\"after\", x_ele.shape)\n",
    "\n",
    "                out, _ = self.lstm1(x_ele)  # process single sequence\n",
    "                out = self.dropout1(out)\n",
    "                out, _ = self.lstm2(out)\n",
    "                out = self.dropout2(out)\n",
    "                out, _ = self.lstm3(out)\n",
    "                out = self.dropout3(out)\n",
    "                out, _ = self.lstm4(out)\n",
    "                out = out[:, -1, :]  # take the last output from the last LSTM layer\n",
    "                out = self.dropout4(out)\n",
    "                out = self.fc(out)\n",
    "                out = self.sigmoid(out)\n",
    "                outputs.append(out)\n",
    "\n",
    "            outputs = torch.cat(outputs, dim=0)  # recombine into a single batch tensor\n",
    "            return outputs\n",
    "        else:\n",
    "            # print(\"x shape\", x.shape)\n",
    "            x, _ = self.lstm1(x)\n",
    "            x = self.dropout1(x)\n",
    "            x, _ = self.lstm2(x)\n",
    "            x = self.dropout2(x)\n",
    "            x, _ = self.lstm3(x)\n",
    "            x = self.dropout3(x)\n",
    "            x, _ = self.lstm4(x)\n",
    "            x = x[:, -1, :]  # Take the last output from the last LSTM layer\n",
    "            x = self.dropout4(x)\n",
    "            x = self.fc(x)   # Linear layer to map to 1 output\n",
    "            x = self.sigmoid(x)\n",
    "            return x\n",
    "\n",
    "gru_params = models[1]\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(num_features, gru_params['hidden_size'], batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(gru_params['dropout'])\n",
    "        self.gru2 = nn.GRU(gru_params['hidden_size'], gru_params['hidden_size'], batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(gru_params['dropout'])\n",
    "        self.gru3 = nn.GRU(gru_params['hidden_size'], gru_params['hidden_size'], batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(gru_params['dropout'])\n",
    "        self.gru4 = nn.GRU(gru_params['hidden_size'], gru_params['hidden_size'], batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(gru_params['dropout'])\n",
    "        self.fc = nn.Linear(gru_params['hidden_size'], 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        if use_time_horizon:\n",
    "\n",
    "            outputs = []\n",
    "            # For each sequence in the batch\n",
    "            for i in range(x.shape[0]):\n",
    "                x_ele = x[i]\n",
    "                # remove padding\n",
    "                x_ele = x_ele[x_ele[:, 0] != -1]\n",
    "                x_ele = x_ele.unsqueeze(0)\n",
    "                # print(x_ele.shape)\n",
    "\n",
    "                # Pass through the GRU layers\n",
    "                x_ele, _ = self.gru1(x_ele)\n",
    "                x_ele = self.dropout1(x_ele)\n",
    "                x_ele, _ = self.gru2(x_ele)\n",
    "                x_ele = self.dropout2(x_ele)\n",
    "                x_ele, _ = self.gru3(x_ele)\n",
    "                x_ele = self.dropout3(x_ele)\n",
    "                x_ele, _ = self.gru4(x_ele)\n",
    "                x_ele = x_ele[:, -1, :]  # Take the last output\n",
    "                x_ele = self.dropout4(x_ele)\n",
    "                x_ele = self.fc(x_ele)   # Linear layer to map to 1 output\n",
    "                x_ele = self.sigmoid(x_ele)\n",
    "                outputs.append(x_ele)\n",
    "\n",
    "            outputs = torch.cat(outputs, dim=0)  # recombine into a single batch tensor\n",
    "            return outputs\n",
    "        else:\n",
    "            x, _ = self.gru1(x)\n",
    "            x = self.dropout1(x)\n",
    "            x, _ = self.gru2(x)\n",
    "            x = self.dropout2(x)\n",
    "            x, _ = self.gru3(x)\n",
    "            x = self.dropout3(x)\n",
    "            x, _ = self.gru4(x)\n",
    "            x = x[:, -1, :]  # Take the last output\n",
    "            x = self.dropout4(x)\n",
    "            x = self.fc(x)   # Linear layer to map to 1 output\n",
    "            x = self.sigmoid(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a fully-connect neuralnetwork with three layers; the activation function for this model is the Rectified Linear Unit (ReLu).\n",
    "# NOTE: The paper doesn't specify the number of neurons in the hidden layers, so I'm basing on the stanford paper\n",
    "meta_params = models[2]\n",
    "\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, meta_params['hidden_size'], bias=True)\n",
    "        self.fc12 = nn.Linear(meta_params['hidden_size'], 1, bias=False)\n",
    "        # self.fc2 = nn.Linear(30, 25)\n",
    "        # self.fc3 = nn.Linear(25, 20)\n",
    "        # self.fc4 = nn.Linear(20, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc12(x)\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        # x = self.relu(self.fc3(x))\n",
    "        # x = self.fc4(x)\n",
    "        x = self.sigmoid(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  mean_compound_reuters  mean_compound_guardian  \\\n",
      "29  2018/01/29                    0.0                  0.0000   \n",
      "30  2018/01/30                    0.0                  0.4215   \n",
      "31  2018/01/31                    0.0                  0.0000   \n",
      "32  2018/02/01                    0.0                  0.2074   \n",
      "33  2018/02/02                    0.0                  0.0000   \n",
      "\n",
      "    mean_compound_cnbc  mean_compound_other   mean TH      Close       Volume  \\\n",
      "29              0.0000             0.526700  3.500000  70.884003  114038000.0   \n",
      "30              0.3854             0.117916  4.153333  71.890999  117438000.0   \n",
      "31              0.0000             0.088438  4.125000  72.544502  128494000.0   \n",
      "32              0.1779             0.177900  3.666667  69.500000  182276000.0   \n",
      "33              0.1779             0.580220  3.600000  71.497498  222514000.0   \n",
      "\n",
      "         Open       High  ...  Adj Close     SMA_15     SMA_30  SMA_Indicator  \\\n",
      "29  70.459000  71.569504  ...  70.884003  65.895266  62.612017              1   \n",
      "30  70.158501  71.962502  ...  71.890999  66.531767  63.051283              1   \n",
      "31  72.565002  73.628998  ...  72.544502  67.192400  63.504200              1   \n",
      "32  72.250000  72.994003  ...  69.500000  67.644633  63.836567              1   \n",
      "33  73.869499  74.900002  ...  71.497498  68.155533  64.240850              1   \n",
      "\n",
      "          MBB        UBB        LBB  Bollinger_Indicator  Close_diff_UBB  \\\n",
      "29  64.424975  71.504493  57.345456                    2       -0.620490   \n",
      "30  65.095850  72.341727  57.849973                    2       -0.450728   \n",
      "31  65.750550  73.211717  58.289383                    2       -0.667215   \n",
      "32  66.215050  73.374486  59.055613                    2       -3.874486   \n",
      "33  66.765950  73.760853  59.771047                    2       -2.263355   \n",
      "\n",
      "    Close_diff_LBB  \n",
      "29       13.538546  \n",
      "30       14.041026  \n",
      "31       14.255119  \n",
      "32       10.444387  \n",
      "33       11.726451  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "475\n",
      "x_train.shape (415, 10, 9)\n",
      "y_train.shape (415,)\n",
      "x_val.shape (30, 10, 9)\n",
      "y_val.shape (30,)\n",
      "x_test.shape (30, 10, 9)\n",
      "y_test.shape (30,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def get_data_stanford():\n",
    "\n",
    "    # To drop columns, change the data_prep.py file\n",
    "\n",
    "    # split_y: tuple\n",
    "    #     (train_y, validate_y, test_y)\n",
    "    # split_X: tuple\n",
    "    #     (train_X, validate_X, test_X)\n",
    "    split_y, split_X = data_prep.data_prep(DATA_PATH, HORIZON, DAYS_FORWARD, END_SPLIT, use_time_horizon)\n",
    "\n",
    "\n",
    "    return split_X, split_y\n",
    "\n",
    "\n",
    "\n",
    "(x_train, x_val, x_test), (y_train, y_val, y_test) = get_data_stanford()\n",
    "\n",
    "print('x_train.shape', x_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('x_val.shape', x_val.shape)\n",
    "print('y_val.shape', y_val.shape)\n",
    "print('x_test.shape', x_test.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instatiate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Instantiate models\n",
    "lstm_model = LSTMModel()\n",
    "gru_model = GRUModel()\n",
    "meta_model = MetaLearner()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "lstm_optimizer = optim.RMSprop(lstm_model.parameters(), lr=lstm_params['learning_rate'], weight_decay=1e-5) # 16 batch size, 150 epochs\n",
    "gru_optimizer = optim.RMSprop(gru_model.parameters(), lr=gru_params['learning_rate'], weight_decay=1e-5) # 16 batch size, 200 epochs\n",
    "meta_criterion = nn.BCELoss()\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), eps=1e-7, lr=meta_params['learning_rate'])\n",
    "base_models_batch_size = lstm_params['batch_size'] # same batch size for both models\n",
    "meta_learner_batch_size = meta_params['batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([415, 10, 9])\n",
      "torch.Size([415])\n",
      "Training LSTM Model\n",
      "Epoch 1/100, Training Loss: 0.6945, Training Accuracy: 0.56, Validation Loss: 0.7033, Validation Accuracy: 0.47\n",
      "Epoch 2/100, Training Loss: 0.6895, Training Accuracy: 0.56, Validation Loss: 0.7070, Validation Accuracy: 0.47\n",
      "Epoch 3/100, Training Loss: 0.6885, Training Accuracy: 0.56, Validation Loss: 0.7048, Validation Accuracy: 0.47\n",
      "Epoch 4/100, Training Loss: 0.6865, Training Accuracy: 0.56, Validation Loss: 0.7081, Validation Accuracy: 0.47\n",
      "Epoch 5/100, Training Loss: 0.6862, Training Accuracy: 0.56, Validation Loss: 0.6978, Validation Accuracy: 0.47\n",
      "Epoch 6/100, Training Loss: 0.6861, Training Accuracy: 0.56, Validation Loss: 0.7016, Validation Accuracy: 0.47\n",
      "Epoch 7/100, Training Loss: 0.6860, Training Accuracy: 0.55, Validation Loss: 0.6976, Validation Accuracy: 0.47\n",
      "Epoch 8/100, Training Loss: 0.6853, Training Accuracy: 0.56, Validation Loss: 0.6944, Validation Accuracy: 0.47\n",
      "Epoch 9/100, Training Loss: 0.6864, Training Accuracy: 0.56, Validation Loss: 0.7045, Validation Accuracy: 0.47\n",
      "Epoch 10/100, Training Loss: 0.6839, Training Accuracy: 0.56, Validation Loss: 0.6970, Validation Accuracy: 0.47\n",
      "Epoch 11/100, Training Loss: 0.6859, Training Accuracy: 0.54, Validation Loss: 0.7096, Validation Accuracy: 0.47\n",
      "Epoch 12/100, Training Loss: 0.6865, Training Accuracy: 0.56, Validation Loss: 0.7030, Validation Accuracy: 0.47\n",
      "Epoch 13/100, Training Loss: 0.6831, Training Accuracy: 0.57, Validation Loss: 0.6939, Validation Accuracy: 0.47\n",
      "Epoch 14/100, Training Loss: 0.6832, Training Accuracy: 0.58, Validation Loss: 0.7028, Validation Accuracy: 0.47\n",
      "Epoch 15/100, Training Loss: 0.6821, Training Accuracy: 0.58, Validation Loss: 0.6988, Validation Accuracy: 0.47\n",
      "Epoch 16/100, Training Loss: 0.6851, Training Accuracy: 0.56, Validation Loss: 0.7062, Validation Accuracy: 0.47\n",
      "Epoch 17/100, Training Loss: 0.6847, Training Accuracy: 0.57, Validation Loss: 0.7057, Validation Accuracy: 0.47\n",
      "Epoch 18/100, Training Loss: 0.6822, Training Accuracy: 0.56, Validation Loss: 0.6960, Validation Accuracy: 0.47\n",
      "Epoch 19/100, Training Loss: 0.6835, Training Accuracy: 0.55, Validation Loss: 0.7040, Validation Accuracy: 0.47\n",
      "Epoch 20/100, Training Loss: 0.6824, Training Accuracy: 0.55, Validation Loss: 0.6965, Validation Accuracy: 0.47\n",
      "Epoch 21/100, Training Loss: 0.6825, Training Accuracy: 0.57, Validation Loss: 0.7032, Validation Accuracy: 0.47\n",
      "Epoch 22/100, Training Loss: 0.6802, Training Accuracy: 0.56, Validation Loss: 0.6991, Validation Accuracy: 0.47\n",
      "Epoch 23/100, Training Loss: 0.6819, Training Accuracy: 0.56, Validation Loss: 0.7019, Validation Accuracy: 0.47\n",
      "Epoch 24/100, Training Loss: 0.6822, Training Accuracy: 0.56, Validation Loss: 0.7026, Validation Accuracy: 0.47\n",
      "Epoch 25/100, Training Loss: 0.6826, Training Accuracy: 0.55, Validation Loss: 0.7000, Validation Accuracy: 0.47\n",
      "Epoch 26/100, Training Loss: 0.6807, Training Accuracy: 0.57, Validation Loss: 0.6981, Validation Accuracy: 0.47\n",
      "Epoch 27/100, Training Loss: 0.6802, Training Accuracy: 0.54, Validation Loss: 0.7040, Validation Accuracy: 0.47\n",
      "Epoch 28/100, Training Loss: 0.6824, Training Accuracy: 0.56, Validation Loss: 0.7046, Validation Accuracy: 0.47\n",
      "Epoch 29/100, Training Loss: 0.6823, Training Accuracy: 0.56, Validation Loss: 0.7053, Validation Accuracy: 0.47\n",
      "Epoch 30/100, Training Loss: 0.6820, Training Accuracy: 0.55, Validation Loss: 0.7014, Validation Accuracy: 0.47\n",
      "Epoch 31/100, Training Loss: 0.6797, Training Accuracy: 0.56, Validation Loss: 0.6968, Validation Accuracy: 0.47\n",
      "Epoch 32/100, Training Loss: 0.6774, Training Accuracy: 0.57, Validation Loss: 0.6997, Validation Accuracy: 0.47\n",
      "Epoch 33/100, Training Loss: 0.6774, Training Accuracy: 0.59, Validation Loss: 0.7090, Validation Accuracy: 0.47\n",
      "Epoch 34/100, Training Loss: 0.6829, Training Accuracy: 0.57, Validation Loss: 0.6926, Validation Accuracy: 0.47\n",
      "Epoch 35/100, Training Loss: 0.6802, Training Accuracy: 0.57, Validation Loss: 0.7027, Validation Accuracy: 0.47\n",
      "Epoch 36/100, Training Loss: 0.6822, Training Accuracy: 0.59, Validation Loss: 0.7060, Validation Accuracy: 0.47\n",
      "Epoch 37/100, Training Loss: 0.6801, Training Accuracy: 0.56, Validation Loss: 0.6998, Validation Accuracy: 0.47\n",
      "Epoch 38/100, Training Loss: 0.6783, Training Accuracy: 0.58, Validation Loss: 0.7068, Validation Accuracy: 0.47\n",
      "Epoch 39/100, Training Loss: 0.6797, Training Accuracy: 0.56, Validation Loss: 0.7029, Validation Accuracy: 0.47\n",
      "Epoch 40/100, Training Loss: 0.6787, Training Accuracy: 0.57, Validation Loss: 0.7079, Validation Accuracy: 0.47\n",
      "Epoch 41/100, Training Loss: 0.6803, Training Accuracy: 0.58, Validation Loss: 0.7100, Validation Accuracy: 0.47\n",
      "Epoch 42/100, Training Loss: 0.6813, Training Accuracy: 0.56, Validation Loss: 0.7076, Validation Accuracy: 0.47\n",
      "Epoch 43/100, Training Loss: 0.6761, Training Accuracy: 0.56, Validation Loss: 0.7085, Validation Accuracy: 0.47\n",
      "Epoch 44/100, Training Loss: 0.6750, Training Accuracy: 0.58, Validation Loss: 0.7047, Validation Accuracy: 0.47\n",
      "Epoch 45/100, Training Loss: 0.6788, Training Accuracy: 0.58, Validation Loss: 0.7112, Validation Accuracy: 0.47\n",
      "Epoch 46/100, Training Loss: 0.6759, Training Accuracy: 0.58, Validation Loss: 0.7087, Validation Accuracy: 0.47\n",
      "Epoch 47/100, Training Loss: 0.6796, Training Accuracy: 0.56, Validation Loss: 0.7085, Validation Accuracy: 0.47\n",
      "Epoch 48/100, Training Loss: 0.6707, Training Accuracy: 0.57, Validation Loss: 0.7082, Validation Accuracy: 0.47\n",
      "Epoch 49/100, Training Loss: 0.6864, Training Accuracy: 0.55, Validation Loss: 0.7071, Validation Accuracy: 0.47\n",
      "Epoch 50/100, Training Loss: 0.6835, Training Accuracy: 0.58, Validation Loss: 0.7084, Validation Accuracy: 0.47\n",
      "Epoch 51/100, Training Loss: 0.6752, Training Accuracy: 0.58, Validation Loss: 0.7000, Validation Accuracy: 0.47\n",
      "Epoch 52/100, Training Loss: 0.6720, Training Accuracy: 0.59, Validation Loss: 0.7078, Validation Accuracy: 0.47\n",
      "Epoch 53/100, Training Loss: 0.6753, Training Accuracy: 0.58, Validation Loss: 0.7140, Validation Accuracy: 0.47\n",
      "Epoch 54/100, Training Loss: 0.6766, Training Accuracy: 0.57, Validation Loss: 0.7100, Validation Accuracy: 0.47\n",
      "Epoch 55/100, Training Loss: 0.6772, Training Accuracy: 0.59, Validation Loss: 0.7105, Validation Accuracy: 0.47\n",
      "Epoch 56/100, Training Loss: 0.6802, Training Accuracy: 0.56, Validation Loss: 0.7085, Validation Accuracy: 0.47\n",
      "Epoch 57/100, Training Loss: 0.6753, Training Accuracy: 0.58, Validation Loss: 0.7035, Validation Accuracy: 0.47\n",
      "Epoch 58/100, Training Loss: 0.6777, Training Accuracy: 0.58, Validation Loss: 0.7092, Validation Accuracy: 0.47\n",
      "Epoch 59/100, Training Loss: 0.6776, Training Accuracy: 0.56, Validation Loss: 0.7040, Validation Accuracy: 0.47\n",
      "Epoch 60/100, Training Loss: 0.6810, Training Accuracy: 0.58, Validation Loss: 0.7090, Validation Accuracy: 0.47\n",
      "Epoch 61/100, Training Loss: 0.6790, Training Accuracy: 0.57, Validation Loss: 0.7073, Validation Accuracy: 0.47\n",
      "Epoch 62/100, Training Loss: 0.6726, Training Accuracy: 0.59, Validation Loss: 0.7069, Validation Accuracy: 0.47\n",
      "Epoch 63/100, Training Loss: 0.6784, Training Accuracy: 0.57, Validation Loss: 0.7075, Validation Accuracy: 0.47\n",
      "Epoch 64/100, Training Loss: 0.6754, Training Accuracy: 0.58, Validation Loss: 0.7099, Validation Accuracy: 0.47\n",
      "Epoch 65/100, Training Loss: 0.6753, Training Accuracy: 0.58, Validation Loss: 0.7082, Validation Accuracy: 0.47\n",
      "Epoch 66/100, Training Loss: 0.6764, Training Accuracy: 0.57, Validation Loss: 0.7088, Validation Accuracy: 0.47\n",
      "Epoch 67/100, Training Loss: 0.6773, Training Accuracy: 0.58, Validation Loss: 0.7063, Validation Accuracy: 0.47\n",
      "Epoch 68/100, Training Loss: 0.6768, Training Accuracy: 0.58, Validation Loss: 0.7096, Validation Accuracy: 0.47\n",
      "Epoch 69/100, Training Loss: 0.6752, Training Accuracy: 0.59, Validation Loss: 0.7105, Validation Accuracy: 0.47\n",
      "Epoch 70/100, Training Loss: 0.6721, Training Accuracy: 0.59, Validation Loss: 0.7124, Validation Accuracy: 0.47\n",
      "Epoch 71/100, Training Loss: 0.6757, Training Accuracy: 0.58, Validation Loss: 0.7106, Validation Accuracy: 0.47\n",
      "Epoch 72/100, Training Loss: 0.6720, Training Accuracy: 0.60, Validation Loss: 0.7114, Validation Accuracy: 0.47\n",
      "Epoch 73/100, Training Loss: 0.6749, Training Accuracy: 0.58, Validation Loss: 0.7141, Validation Accuracy: 0.47\n",
      "Epoch 74/100, Training Loss: 0.6718, Training Accuracy: 0.59, Validation Loss: 0.7131, Validation Accuracy: 0.47\n",
      "Epoch 75/100, Training Loss: 0.6718, Training Accuracy: 0.59, Validation Loss: 0.7118, Validation Accuracy: 0.47\n",
      "Epoch 76/100, Training Loss: 0.6692, Training Accuracy: 0.59, Validation Loss: 0.7139, Validation Accuracy: 0.47\n",
      "Epoch 77/100, Training Loss: 0.6792, Training Accuracy: 0.57, Validation Loss: 0.7138, Validation Accuracy: 0.47\n",
      "Epoch 78/100, Training Loss: 0.6739, Training Accuracy: 0.57, Validation Loss: 0.7140, Validation Accuracy: 0.47\n",
      "Epoch 79/100, Training Loss: 0.6679, Training Accuracy: 0.59, Validation Loss: 0.7173, Validation Accuracy: 0.47\n",
      "Epoch 80/100, Training Loss: 0.6729, Training Accuracy: 0.59, Validation Loss: 0.7159, Validation Accuracy: 0.47\n",
      "Epoch 81/100, Training Loss: 0.6759, Training Accuracy: 0.58, Validation Loss: 0.7191, Validation Accuracy: 0.47\n",
      "Epoch 82/100, Training Loss: 0.6756, Training Accuracy: 0.60, Validation Loss: 0.7267, Validation Accuracy: 0.47\n",
      "Epoch 83/100, Training Loss: 0.6780, Training Accuracy: 0.58, Validation Loss: 0.7228, Validation Accuracy: 0.47\n",
      "Epoch 84/100, Training Loss: 0.6777, Training Accuracy: 0.60, Validation Loss: 0.7198, Validation Accuracy: 0.47\n",
      "Epoch 85/100, Training Loss: 0.6769, Training Accuracy: 0.59, Validation Loss: 0.7187, Validation Accuracy: 0.47\n",
      "Epoch 86/100, Training Loss: 0.6786, Training Accuracy: 0.57, Validation Loss: 0.7178, Validation Accuracy: 0.47\n",
      "Epoch 87/100, Training Loss: 0.6694, Training Accuracy: 0.59, Validation Loss: 0.7129, Validation Accuracy: 0.47\n",
      "Epoch 88/100, Training Loss: 0.6677, Training Accuracy: 0.59, Validation Loss: 0.7155, Validation Accuracy: 0.47\n",
      "Epoch 89/100, Training Loss: 0.6714, Training Accuracy: 0.60, Validation Loss: 0.7108, Validation Accuracy: 0.47\n",
      "Epoch 90/100, Training Loss: 0.6692, Training Accuracy: 0.60, Validation Loss: 0.7169, Validation Accuracy: 0.47\n",
      "Epoch 91/100, Training Loss: 0.6665, Training Accuracy: 0.60, Validation Loss: 0.7146, Validation Accuracy: 0.47\n",
      "Epoch 92/100, Training Loss: 0.6671, Training Accuracy: 0.60, Validation Loss: 0.7200, Validation Accuracy: 0.47\n",
      "Epoch 93/100, Training Loss: 0.6684, Training Accuracy: 0.58, Validation Loss: 0.7215, Validation Accuracy: 0.47\n",
      "Epoch 94/100, Training Loss: 0.6636, Training Accuracy: 0.59, Validation Loss: 0.7204, Validation Accuracy: 0.47\n",
      "Epoch 95/100, Training Loss: 0.6698, Training Accuracy: 0.58, Validation Loss: 0.7131, Validation Accuracy: 0.47\n",
      "Epoch 96/100, Training Loss: 0.6662, Training Accuracy: 0.61, Validation Loss: 0.7215, Validation Accuracy: 0.47\n",
      "Epoch 97/100, Training Loss: 0.6674, Training Accuracy: 0.58, Validation Loss: 0.7319, Validation Accuracy: 0.47\n",
      "Epoch 98/100, Training Loss: 0.6718, Training Accuracy: 0.60, Validation Loss: 0.7217, Validation Accuracy: 0.47\n",
      "Epoch 99/100, Training Loss: 0.6658, Training Accuracy: 0.60, Validation Loss: 0.7273, Validation Accuracy: 0.47\n",
      "Epoch 100/100, Training Loss: 0.6552, Training Accuracy: 0.62, Validation Loss: 0.7101, Validation Accuracy: 0.50\n",
      "Training GRU Model\n",
      "Epoch 1/100, Training Loss: 0.6964, Training Accuracy: 0.55, Validation Loss: 0.7196, Validation Accuracy: 0.47\n",
      "Epoch 2/100, Training Loss: 0.6909, Training Accuracy: 0.55, Validation Loss: 0.7101, Validation Accuracy: 0.47\n",
      "Epoch 3/100, Training Loss: 0.6900, Training Accuracy: 0.56, Validation Loss: 0.7056, Validation Accuracy: 0.47\n",
      "Epoch 4/100, Training Loss: 0.6883, Training Accuracy: 0.56, Validation Loss: 0.6988, Validation Accuracy: 0.47\n",
      "Epoch 5/100, Training Loss: 0.6858, Training Accuracy: 0.56, Validation Loss: 0.6981, Validation Accuracy: 0.47\n",
      "Epoch 6/100, Training Loss: 0.6832, Training Accuracy: 0.55, Validation Loss: 0.7023, Validation Accuracy: 0.47\n",
      "Epoch 7/100, Training Loss: 0.6851, Training Accuracy: 0.56, Validation Loss: 0.7061, Validation Accuracy: 0.47\n",
      "Epoch 8/100, Training Loss: 0.6832, Training Accuracy: 0.57, Validation Loss: 0.7123, Validation Accuracy: 0.47\n",
      "Epoch 9/100, Training Loss: 0.6829, Training Accuracy: 0.55, Validation Loss: 0.7007, Validation Accuracy: 0.47\n",
      "Epoch 10/100, Training Loss: 0.6824, Training Accuracy: 0.58, Validation Loss: 0.7026, Validation Accuracy: 0.47\n",
      "Epoch 11/100, Training Loss: 0.6795, Training Accuracy: 0.57, Validation Loss: 0.7057, Validation Accuracy: 0.47\n",
      "Epoch 12/100, Training Loss: 0.6787, Training Accuracy: 0.59, Validation Loss: 0.6977, Validation Accuracy: 0.47\n",
      "Epoch 13/100, Training Loss: 0.6776, Training Accuracy: 0.56, Validation Loss: 0.7055, Validation Accuracy: 0.47\n",
      "Epoch 14/100, Training Loss: 0.6819, Training Accuracy: 0.57, Validation Loss: 0.6993, Validation Accuracy: 0.47\n",
      "Epoch 15/100, Training Loss: 0.6832, Training Accuracy: 0.56, Validation Loss: 0.7020, Validation Accuracy: 0.47\n",
      "Epoch 16/100, Training Loss: 0.6816, Training Accuracy: 0.59, Validation Loss: 0.7013, Validation Accuracy: 0.47\n",
      "Epoch 17/100, Training Loss: 0.6820, Training Accuracy: 0.59, Validation Loss: 0.7117, Validation Accuracy: 0.47\n",
      "Epoch 18/100, Training Loss: 0.6818, Training Accuracy: 0.57, Validation Loss: 0.7080, Validation Accuracy: 0.47\n",
      "Epoch 19/100, Training Loss: 0.6793, Training Accuracy: 0.57, Validation Loss: 0.6967, Validation Accuracy: 0.47\n",
      "Epoch 20/100, Training Loss: 0.6841, Training Accuracy: 0.54, Validation Loss: 0.7026, Validation Accuracy: 0.47\n",
      "Epoch 21/100, Training Loss: 0.6788, Training Accuracy: 0.58, Validation Loss: 0.7027, Validation Accuracy: 0.47\n",
      "Epoch 22/100, Training Loss: 0.6827, Training Accuracy: 0.56, Validation Loss: 0.7008, Validation Accuracy: 0.47\n",
      "Epoch 23/100, Training Loss: 0.6793, Training Accuracy: 0.56, Validation Loss: 0.7158, Validation Accuracy: 0.47\n",
      "Epoch 24/100, Training Loss: 0.6814, Training Accuracy: 0.57, Validation Loss: 0.6918, Validation Accuracy: 0.47\n",
      "Epoch 25/100, Training Loss: 0.6815, Training Accuracy: 0.56, Validation Loss: 0.6968, Validation Accuracy: 0.47\n",
      "Epoch 26/100, Training Loss: 0.6823, Training Accuracy: 0.55, Validation Loss: 0.7053, Validation Accuracy: 0.47\n",
      "Epoch 27/100, Training Loss: 0.6837, Training Accuracy: 0.57, Validation Loss: 0.7023, Validation Accuracy: 0.47\n",
      "Epoch 28/100, Training Loss: 0.6812, Training Accuracy: 0.57, Validation Loss: 0.6977, Validation Accuracy: 0.47\n",
      "Epoch 29/100, Training Loss: 0.6807, Training Accuracy: 0.59, Validation Loss: 0.6911, Validation Accuracy: 0.47\n",
      "Epoch 30/100, Training Loss: 0.6781, Training Accuracy: 0.56, Validation Loss: 0.7025, Validation Accuracy: 0.47\n",
      "Epoch 31/100, Training Loss: 0.6821, Training Accuracy: 0.56, Validation Loss: 0.7083, Validation Accuracy: 0.47\n",
      "Epoch 32/100, Training Loss: 0.6820, Training Accuracy: 0.58, Validation Loss: 0.6951, Validation Accuracy: 0.47\n",
      "Epoch 33/100, Training Loss: 0.6823, Training Accuracy: 0.56, Validation Loss: 0.7000, Validation Accuracy: 0.47\n",
      "Epoch 34/100, Training Loss: 0.6796, Training Accuracy: 0.57, Validation Loss: 0.7114, Validation Accuracy: 0.47\n",
      "Epoch 35/100, Training Loss: 0.6843, Training Accuracy: 0.56, Validation Loss: 0.7010, Validation Accuracy: 0.47\n",
      "Epoch 36/100, Training Loss: 0.6818, Training Accuracy: 0.56, Validation Loss: 0.7035, Validation Accuracy: 0.47\n",
      "Epoch 37/100, Training Loss: 0.6799, Training Accuracy: 0.56, Validation Loss: 0.7086, Validation Accuracy: 0.47\n",
      "Epoch 38/100, Training Loss: 0.6826, Training Accuracy: 0.56, Validation Loss: 0.7016, Validation Accuracy: 0.47\n",
      "Epoch 39/100, Training Loss: 0.6790, Training Accuracy: 0.57, Validation Loss: 0.6957, Validation Accuracy: 0.47\n",
      "Epoch 40/100, Training Loss: 0.6792, Training Accuracy: 0.57, Validation Loss: 0.7096, Validation Accuracy: 0.47\n",
      "Epoch 41/100, Training Loss: 0.6792, Training Accuracy: 0.58, Validation Loss: 0.7042, Validation Accuracy: 0.47\n",
      "Epoch 42/100, Training Loss: 0.6788, Training Accuracy: 0.58, Validation Loss: 0.7071, Validation Accuracy: 0.47\n",
      "Epoch 43/100, Training Loss: 0.6825, Training Accuracy: 0.57, Validation Loss: 0.7111, Validation Accuracy: 0.47\n",
      "Epoch 44/100, Training Loss: 0.6794, Training Accuracy: 0.57, Validation Loss: 0.7004, Validation Accuracy: 0.47\n",
      "Epoch 45/100, Training Loss: 0.6794, Training Accuracy: 0.58, Validation Loss: 0.6975, Validation Accuracy: 0.47\n",
      "Epoch 46/100, Training Loss: 0.6773, Training Accuracy: 0.59, Validation Loss: 0.7060, Validation Accuracy: 0.47\n",
      "Epoch 47/100, Training Loss: 0.6800, Training Accuracy: 0.56, Validation Loss: 0.7017, Validation Accuracy: 0.47\n",
      "Epoch 48/100, Training Loss: 0.6757, Training Accuracy: 0.56, Validation Loss: 0.7040, Validation Accuracy: 0.47\n",
      "Epoch 49/100, Training Loss: 0.6796, Training Accuracy: 0.58, Validation Loss: 0.7100, Validation Accuracy: 0.47\n",
      "Epoch 50/100, Training Loss: 0.6825, Training Accuracy: 0.57, Validation Loss: 0.6943, Validation Accuracy: 0.47\n",
      "Epoch 51/100, Training Loss: 0.6775, Training Accuracy: 0.57, Validation Loss: 0.7099, Validation Accuracy: 0.47\n",
      "Epoch 52/100, Training Loss: 0.6826, Training Accuracy: 0.60, Validation Loss: 0.7035, Validation Accuracy: 0.47\n",
      "Epoch 53/100, Training Loss: 0.6799, Training Accuracy: 0.57, Validation Loss: 0.7063, Validation Accuracy: 0.47\n",
      "Epoch 54/100, Training Loss: 0.6791, Training Accuracy: 0.57, Validation Loss: 0.6920, Validation Accuracy: 0.47\n",
      "Epoch 55/100, Training Loss: 0.6788, Training Accuracy: 0.59, Validation Loss: 0.7090, Validation Accuracy: 0.47\n",
      "Epoch 56/100, Training Loss: 0.6787, Training Accuracy: 0.56, Validation Loss: 0.7113, Validation Accuracy: 0.47\n",
      "Epoch 57/100, Training Loss: 0.6775, Training Accuracy: 0.58, Validation Loss: 0.7162, Validation Accuracy: 0.47\n",
      "Epoch 58/100, Training Loss: 0.6818, Training Accuracy: 0.58, Validation Loss: 0.7025, Validation Accuracy: 0.47\n",
      "Epoch 59/100, Training Loss: 0.6772, Training Accuracy: 0.57, Validation Loss: 0.6935, Validation Accuracy: 0.47\n",
      "Epoch 60/100, Training Loss: 0.6769, Training Accuracy: 0.59, Validation Loss: 0.6988, Validation Accuracy: 0.47\n",
      "Epoch 61/100, Training Loss: 0.6775, Training Accuracy: 0.57, Validation Loss: 0.7024, Validation Accuracy: 0.47\n",
      "Epoch 62/100, Training Loss: 0.6815, Training Accuracy: 0.59, Validation Loss: 0.6966, Validation Accuracy: 0.47\n",
      "Epoch 63/100, Training Loss: 0.6806, Training Accuracy: 0.57, Validation Loss: 0.7051, Validation Accuracy: 0.47\n",
      "Epoch 64/100, Training Loss: 0.6789, Training Accuracy: 0.56, Validation Loss: 0.7256, Validation Accuracy: 0.47\n",
      "Epoch 65/100, Training Loss: 0.6800, Training Accuracy: 0.56, Validation Loss: 0.7176, Validation Accuracy: 0.47\n",
      "Epoch 66/100, Training Loss: 0.6714, Training Accuracy: 0.60, Validation Loss: 0.6913, Validation Accuracy: 0.47\n",
      "Epoch 67/100, Training Loss: 0.6771, Training Accuracy: 0.58, Validation Loss: 0.7096, Validation Accuracy: 0.47\n",
      "Epoch 68/100, Training Loss: 0.6816, Training Accuracy: 0.56, Validation Loss: 0.7109, Validation Accuracy: 0.47\n",
      "Epoch 69/100, Training Loss: 0.6771, Training Accuracy: 0.57, Validation Loss: 0.7259, Validation Accuracy: 0.47\n",
      "Epoch 70/100, Training Loss: 0.6788, Training Accuracy: 0.58, Validation Loss: 0.7203, Validation Accuracy: 0.47\n",
      "Epoch 71/100, Training Loss: 0.6770, Training Accuracy: 0.56, Validation Loss: 0.7418, Validation Accuracy: 0.47\n",
      "Epoch 72/100, Training Loss: 0.6797, Training Accuracy: 0.58, Validation Loss: 0.7042, Validation Accuracy: 0.47\n",
      "Epoch 73/100, Training Loss: 0.6790, Training Accuracy: 0.58, Validation Loss: 0.7181, Validation Accuracy: 0.47\n",
      "Epoch 74/100, Training Loss: 0.6775, Training Accuracy: 0.60, Validation Loss: 0.7244, Validation Accuracy: 0.47\n",
      "Epoch 75/100, Training Loss: 0.6748, Training Accuracy: 0.60, Validation Loss: 0.6987, Validation Accuracy: 0.47\n",
      "Epoch 76/100, Training Loss: 0.6745, Training Accuracy: 0.57, Validation Loss: 0.7196, Validation Accuracy: 0.47\n",
      "Epoch 77/100, Training Loss: 0.6809, Training Accuracy: 0.58, Validation Loss: 0.7231, Validation Accuracy: 0.47\n",
      "Epoch 78/100, Training Loss: 0.6733, Training Accuracy: 0.60, Validation Loss: 0.7274, Validation Accuracy: 0.47\n",
      "Epoch 79/100, Training Loss: 0.6764, Training Accuracy: 0.58, Validation Loss: 0.7272, Validation Accuracy: 0.47\n",
      "Epoch 80/100, Training Loss: 0.6733, Training Accuracy: 0.58, Validation Loss: 0.6988, Validation Accuracy: 0.47\n",
      "Epoch 81/100, Training Loss: 0.6774, Training Accuracy: 0.59, Validation Loss: 0.7088, Validation Accuracy: 0.47\n",
      "Epoch 82/100, Training Loss: 0.6759, Training Accuracy: 0.58, Validation Loss: 0.7016, Validation Accuracy: 0.47\n",
      "Epoch 83/100, Training Loss: 0.6767, Training Accuracy: 0.56, Validation Loss: 0.7298, Validation Accuracy: 0.47\n",
      "Epoch 84/100, Training Loss: 0.6766, Training Accuracy: 0.57, Validation Loss: 0.7140, Validation Accuracy: 0.47\n",
      "Epoch 85/100, Training Loss: 0.6717, Training Accuracy: 0.60, Validation Loss: 0.6959, Validation Accuracy: 0.47\n",
      "Epoch 86/100, Training Loss: 0.6737, Training Accuracy: 0.57, Validation Loss: 0.7160, Validation Accuracy: 0.47\n",
      "Epoch 87/100, Training Loss: 0.6738, Training Accuracy: 0.59, Validation Loss: 0.7238, Validation Accuracy: 0.47\n",
      "Epoch 88/100, Training Loss: 0.6736, Training Accuracy: 0.60, Validation Loss: 0.7372, Validation Accuracy: 0.47\n",
      "Epoch 89/100, Training Loss: 0.6709, Training Accuracy: 0.60, Validation Loss: 0.7337, Validation Accuracy: 0.47\n",
      "Epoch 90/100, Training Loss: 0.6761, Training Accuracy: 0.58, Validation Loss: 0.7284, Validation Accuracy: 0.47\n",
      "Epoch 91/100, Training Loss: 0.6682, Training Accuracy: 0.60, Validation Loss: 0.7266, Validation Accuracy: 0.47\n",
      "Epoch 92/100, Training Loss: 0.6747, Training Accuracy: 0.60, Validation Loss: 0.7206, Validation Accuracy: 0.47\n",
      "Epoch 93/100, Training Loss: 0.6722, Training Accuracy: 0.59, Validation Loss: 0.7506, Validation Accuracy: 0.47\n",
      "Epoch 94/100, Training Loss: 0.6787, Training Accuracy: 0.57, Validation Loss: 0.7171, Validation Accuracy: 0.47\n",
      "Epoch 95/100, Training Loss: 0.6705, Training Accuracy: 0.58, Validation Loss: 0.7119, Validation Accuracy: 0.50\n",
      "Epoch 96/100, Training Loss: 0.6684, Training Accuracy: 0.59, Validation Loss: 0.7136, Validation Accuracy: 0.47\n",
      "Epoch 97/100, Training Loss: 0.6634, Training Accuracy: 0.57, Validation Loss: 0.7129, Validation Accuracy: 0.47\n",
      "Epoch 98/100, Training Loss: 0.6674, Training Accuracy: 0.58, Validation Loss: 0.7062, Validation Accuracy: 0.50\n",
      "Epoch 99/100, Training Loss: 0.6716, Training Accuracy: 0.59, Validation Loss: 0.7306, Validation Accuracy: 0.50\n",
      "Epoch 100/100, Training Loss: 0.6660, Training Accuracy: 0.60, Validation Loss: 0.7217, Validation Accuracy: 0.47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GRUModel(\n",
       "  (gru1): GRU(9, 50, batch_first=True)\n",
       "  (dropout1): Dropout(p=0.4, inplace=False)\n",
       "  (gru2): GRU(50, 50, batch_first=True)\n",
       "  (dropout2): Dropout(p=0.4, inplace=False)\n",
       "  (gru3): GRU(50, 50, batch_first=True)\n",
       "  (dropout3): Dropout(p=0.4, inplace=False)\n",
       "  (gru4): GRU(50, 50, batch_first=True)\n",
       "  (dropout4): Dropout(p=0.4, inplace=False)\n",
       "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Convert data to PyTorch tensors and create DataLoader\n",
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=base_models_batch_size, shuffle=lstm_params['shuffle']) #Stanford had shuffle true\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=base_models_batch_size) \n",
    "\n",
    "import torch\n",
    "\n",
    "def train_model(model, optimizer, criterion, train_loader, n_epochs=150, val_loader=None, return_lowest_val_loss=False):\n",
    "    model.train()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            predicted_train = output.round()\n",
    "            total_train += y_batch.size(0)\n",
    "            correct_train += (predicted_train == y_batch.view(-1, 1)).sum().item()\n",
    "\n",
    "        # Output training loss and accuracy\n",
    "        train_loss_avg = epoch_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss_avg:.4f}, Training Accuracy: {train_accuracy:.2f}', end='')\n",
    "\n",
    "        # Validation phase (if val_loader is provided)\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad():\n",
    "                for X_val, y_val in val_loader:\n",
    "                    output_val = model(X_val)\n",
    "                    val_loss += criterion(output_val, y_val.view(-1, 1)).item()\n",
    "\n",
    "                    predicted_val = output_val.round()\n",
    "                    total_val += y_val.size(0)\n",
    "                    correct_val += (predicted_val == y_val.view(-1, 1)).sum().item()\n",
    "\n",
    "            val_loss_avg = val_loss / len(val_loader)\n",
    "            val_accuracy = correct_val / total_val\n",
    "            print(f', Validation Loss: {val_loss_avg:.4f}, Validation Accuracy: {val_accuracy:.2f}')\n",
    "            \n",
    "            # if val_accuracy > best_val_accuracy - 1e-2:\n",
    "            #     best_val_loss = val_loss_avg\n",
    "            #     best_val_accuracy = val_accuracy\n",
    "            #     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            # # Check if this is the best model so far\n",
    "            if val_loss_avg < best_val_loss + 1e-2 : # Add 1e-2 as a \"buffer\" to favor the latest model\n",
    "                best_val_loss = val_loss_avg\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        else:\n",
    "            print()  # Just move to the next line\n",
    "\n",
    "    # Load the best model weights\n",
    "    if return_lowest_val_loss and val_loader != None:\n",
    "        print(f'Lowest validation loss: {best_val_loss:.4f}')\n",
    "        print(f'Best validation accuracy: {best_val_accuracy:.2f}')\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Train the LSTM model\n",
    "print(\"Training LSTM Model\")\n",
    "train_model(lstm_model, lstm_optimizer, criterion, train_loader, lstm_params['num_epochs'], val_loader, hyperparams['return_lowest_val_loss'])\n",
    "\n",
    "# Train the GRU model\n",
    "print(\"Training GRU Model\")\n",
    "train_model(gru_model, gru_optimizer, criterion, train_loader, gru_params['num_epochs'], val_loader, hyperparams['return_lowest_val_loss']) # 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use base models to predict the validation data, this will be used as input to the Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2)\n"
     ]
    }
   ],
   "source": [
    "lstm_val_predictions = lstm_model(torch.tensor(x_val, dtype=torch.float32)).detach().numpy().reshape(-1,1)\n",
    "gru_val_predictions = gru_model(torch.tensor(x_val, dtype=torch.float32)).detach().numpy().reshape(-1,1)\n",
    "\n",
    "# lstm_pred = lstm_model.predict(X).reshape(-1, 1)\n",
    "# gru_pred = gru_model.predict(X).reshape(-1, 1)\n",
    "\n",
    "# Form and return new data set\n",
    "# new_X = np.hstack((lstm_pred, gru_pred))\n",
    "\n",
    "\n",
    "# Combine predictions to form new training data for the meta-learner\n",
    "meta_X_train = np.concatenate((lstm_val_predictions, gru_val_predictions), axis=1)#meta_X_train = np.hstack((lstm_val_predictions, gru_val_predictions))#\n",
    "\n",
    "print(meta_X_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.7085, Training Accuracy: 0.47\n",
      "Epoch 2/100, Training Loss: 0.7007, Training Accuracy: 0.47\n",
      "Epoch 3/100, Training Loss: 0.7060, Training Accuracy: 0.47\n",
      "Epoch 4/100, Training Loss: 0.7038, Training Accuracy: 0.47\n",
      "Epoch 5/100, Training Loss: 0.7074, Training Accuracy: 0.47\n",
      "Epoch 6/100, Training Loss: 0.7032, Training Accuracy: 0.47\n",
      "Epoch 7/100, Training Loss: 0.7048, Training Accuracy: 0.47\n",
      "Epoch 8/100, Training Loss: 0.7047, Training Accuracy: 0.47\n",
      "Epoch 9/100, Training Loss: 0.6992, Training Accuracy: 0.47\n",
      "Epoch 10/100, Training Loss: 0.7007, Training Accuracy: 0.47\n",
      "Epoch 11/100, Training Loss: 0.7023, Training Accuracy: 0.47\n",
      "Epoch 12/100, Training Loss: 0.7038, Training Accuracy: 0.47\n",
      "Epoch 13/100, Training Loss: 0.7001, Training Accuracy: 0.47\n",
      "Epoch 14/100, Training Loss: 0.6985, Training Accuracy: 0.47\n",
      "Epoch 15/100, Training Loss: 0.7016, Training Accuracy: 0.47\n",
      "Epoch 16/100, Training Loss: 0.7014, Training Accuracy: 0.47\n",
      "Epoch 17/100, Training Loss: 0.7028, Training Accuracy: 0.47\n",
      "Epoch 18/100, Training Loss: 0.7010, Training Accuracy: 0.47\n",
      "Epoch 19/100, Training Loss: 0.7026, Training Accuracy: 0.47\n",
      "Epoch 20/100, Training Loss: 0.7023, Training Accuracy: 0.47\n",
      "Epoch 21/100, Training Loss: 0.6990, Training Accuracy: 0.47\n",
      "Epoch 22/100, Training Loss: 0.7003, Training Accuracy: 0.47\n",
      "Epoch 23/100, Training Loss: 0.6999, Training Accuracy: 0.47\n",
      "Epoch 24/100, Training Loss: 0.6973, Training Accuracy: 0.47\n",
      "Epoch 25/100, Training Loss: 0.6997, Training Accuracy: 0.47\n",
      "Epoch 26/100, Training Loss: 0.7009, Training Accuracy: 0.47\n",
      "Epoch 27/100, Training Loss: 0.7010, Training Accuracy: 0.47\n",
      "Epoch 28/100, Training Loss: 0.6994, Training Accuracy: 0.47\n",
      "Epoch 29/100, Training Loss: 0.6977, Training Accuracy: 0.47\n",
      "Epoch 30/100, Training Loss: 0.6978, Training Accuracy: 0.47\n",
      "Epoch 31/100, Training Loss: 0.7013, Training Accuracy: 0.47\n",
      "Epoch 32/100, Training Loss: 0.6986, Training Accuracy: 0.47\n",
      "Epoch 33/100, Training Loss: 0.7000, Training Accuracy: 0.47\n",
      "Epoch 34/100, Training Loss: 0.6996, Training Accuracy: 0.47\n",
      "Epoch 35/100, Training Loss: 0.6985, Training Accuracy: 0.47\n",
      "Epoch 36/100, Training Loss: 0.6983, Training Accuracy: 0.47\n",
      "Epoch 37/100, Training Loss: 0.6991, Training Accuracy: 0.47\n",
      "Epoch 38/100, Training Loss: 0.6969, Training Accuracy: 0.47\n",
      "Epoch 39/100, Training Loss: 0.6979, Training Accuracy: 0.47\n",
      "Epoch 40/100, Training Loss: 0.6969, Training Accuracy: 0.47\n",
      "Epoch 41/100, Training Loss: 0.6976, Training Accuracy: 0.47\n",
      "Epoch 42/100, Training Loss: 0.6975, Training Accuracy: 0.47\n",
      "Epoch 43/100, Training Loss: 0.6946, Training Accuracy: 0.47\n",
      "Epoch 44/100, Training Loss: 0.6974, Training Accuracy: 0.47\n",
      "Epoch 45/100, Training Loss: 0.6973, Training Accuracy: 0.47\n",
      "Epoch 46/100, Training Loss: 0.6981, Training Accuracy: 0.47\n",
      "Epoch 47/100, Training Loss: 0.6981, Training Accuracy: 0.47\n",
      "Epoch 48/100, Training Loss: 0.6961, Training Accuracy: 0.47\n",
      "Epoch 49/100, Training Loss: 0.6961, Training Accuracy: 0.47\n",
      "Epoch 50/100, Training Loss: 0.6983, Training Accuracy: 0.47\n",
      "Epoch 51/100, Training Loss: 0.6976, Training Accuracy: 0.47\n",
      "Epoch 52/100, Training Loss: 0.6965, Training Accuracy: 0.47\n",
      "Epoch 53/100, Training Loss: 0.6951, Training Accuracy: 0.47\n",
      "Epoch 54/100, Training Loss: 0.6957, Training Accuracy: 0.47\n",
      "Epoch 55/100, Training Loss: 0.6964, Training Accuracy: 0.47\n",
      "Epoch 56/100, Training Loss: 0.6962, Training Accuracy: 0.47\n",
      "Epoch 57/100, Training Loss: 0.6964, Training Accuracy: 0.47\n",
      "Epoch 58/100, Training Loss: 0.6968, Training Accuracy: 0.47\n",
      "Epoch 59/100, Training Loss: 0.6975, Training Accuracy: 0.47\n",
      "Epoch 60/100, Training Loss: 0.6947, Training Accuracy: 0.47\n",
      "Epoch 61/100, Training Loss: 0.6952, Training Accuracy: 0.47\n",
      "Epoch 62/100, Training Loss: 0.6963, Training Accuracy: 0.47\n",
      "Epoch 63/100, Training Loss: 0.6953, Training Accuracy: 0.47\n",
      "Epoch 64/100, Training Loss: 0.6968, Training Accuracy: 0.47\n",
      "Epoch 65/100, Training Loss: 0.6949, Training Accuracy: 0.47\n",
      "Epoch 66/100, Training Loss: 0.6953, Training Accuracy: 0.47\n",
      "Epoch 67/100, Training Loss: 0.6950, Training Accuracy: 0.47\n",
      "Epoch 68/100, Training Loss: 0.6949, Training Accuracy: 0.47\n",
      "Epoch 69/100, Training Loss: 0.6958, Training Accuracy: 0.47\n",
      "Epoch 70/100, Training Loss: 0.6953, Training Accuracy: 0.47\n",
      "Epoch 71/100, Training Loss: 0.6953, Training Accuracy: 0.47\n",
      "Epoch 72/100, Training Loss: 0.6955, Training Accuracy: 0.47\n",
      "Epoch 73/100, Training Loss: 0.6940, Training Accuracy: 0.47\n",
      "Epoch 74/100, Training Loss: 0.6951, Training Accuracy: 0.47\n",
      "Epoch 75/100, Training Loss: 0.6946, Training Accuracy: 0.47\n",
      "Epoch 76/100, Training Loss: 0.6951, Training Accuracy: 0.47\n",
      "Epoch 77/100, Training Loss: 0.6950, Training Accuracy: 0.47\n",
      "Epoch 78/100, Training Loss: 0.6954, Training Accuracy: 0.47\n",
      "Epoch 79/100, Training Loss: 0.6940, Training Accuracy: 0.47\n",
      "Epoch 80/100, Training Loss: 0.6948, Training Accuracy: 0.47\n",
      "Epoch 81/100, Training Loss: 0.6945, Training Accuracy: 0.47\n",
      "Epoch 82/100, Training Loss: 0.6958, Training Accuracy: 0.47\n",
      "Epoch 83/100, Training Loss: 0.6948, Training Accuracy: 0.47\n",
      "Epoch 84/100, Training Loss: 0.6950, Training Accuracy: 0.47\n",
      "Epoch 85/100, Training Loss: 0.6943, Training Accuracy: 0.47\n",
      "Epoch 86/100, Training Loss: 0.6947, Training Accuracy: 0.47\n",
      "Epoch 87/100, Training Loss: 0.6946, Training Accuracy: 0.47\n",
      "Epoch 88/100, Training Loss: 0.6946, Training Accuracy: 0.47\n",
      "Epoch 89/100, Training Loss: 0.6950, Training Accuracy: 0.47\n",
      "Epoch 90/100, Training Loss: 0.6943, Training Accuracy: 0.47\n",
      "Epoch 91/100, Training Loss: 0.6946, Training Accuracy: 0.47\n",
      "Epoch 92/100, Training Loss: 0.6950, Training Accuracy: 0.47\n",
      "Epoch 93/100, Training Loss: 0.6949, Training Accuracy: 0.47\n",
      "Epoch 94/100, Training Loss: 0.6949, Training Accuracy: 0.47\n",
      "Epoch 95/100, Training Loss: 0.6941, Training Accuracy: 0.47\n",
      "Epoch 96/100, Training Loss: 0.6947, Training Accuracy: 0.47\n",
      "Epoch 97/100, Training Loss: 0.6943, Training Accuracy: 0.47\n",
      "Epoch 98/100, Training Loss: 0.6945, Training Accuracy: 0.47\n",
      "Epoch 99/100, Training Loss: 0.6941, Training Accuracy: 0.47\n",
      "Epoch 100/100, Training Loss: 0.6946, Training Accuracy: 0.47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MetaLearner(\n",
       "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (fc12): Linear(in_features=4, out_features=1, bias=False)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_X_train_tensor = torch.tensor(meta_X_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "meta_train_dataset = TensorDataset(meta_X_train_tensor, y_val_tensor)\n",
    "meta_train_loader = DataLoader(meta_train_dataset, batch_size=meta_learner_batch_size, shuffle=meta_params['shuffle'])\n",
    "\n",
    "\n",
    "\n",
    "train_model(meta_model, meta_optimizer, meta_criterion, meta_train_loader, meta_params['num_epochs'], return_lowest_val_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta predictions [[0.51113045]\n",
      " [0.5090996 ]\n",
      " [0.51020056]\n",
      " [0.5080761 ]\n",
      " [0.507032  ]\n",
      " [0.50691724]\n",
      " [0.501704  ]\n",
      " [0.5027114 ]\n",
      " [0.50720006]\n",
      " [0.5094635 ]\n",
      " [0.5111479 ]\n",
      " [0.5096528 ]\n",
      " [0.5117406 ]\n",
      " [0.5082308 ]\n",
      " [0.510304  ]\n",
      " [0.5057819 ]\n",
      " [0.5089369 ]\n",
      " [0.50841886]\n",
      " [0.506857  ]\n",
      " [0.5087054 ]\n",
      " [0.5039586 ]\n",
      " [0.50737154]\n",
      " [0.5096193 ]\n",
      " [0.5073389 ]\n",
      " [0.5136494 ]\n",
      " [0.50902003]\n",
      " [0.5114679 ]\n",
      " [0.5080706 ]\n",
      " [0.5077259 ]\n",
      " [0.5116215 ]]\n",
      "Accuracy: 0.5666666666666667\n",
      "Precision: 0.5666666666666667, Recall: 1.0, F1 Score: 0.723404255319149\n",
      "lstm predictions [[0.452587  ]\n",
      " [0.4895481 ]\n",
      " [0.49102682]\n",
      " [0.5404781 ]\n",
      " [0.57125294]\n",
      " [0.58678144]\n",
      " [0.62515205]\n",
      " [0.6117266 ]\n",
      " [0.5391221 ]\n",
      " [0.5009467 ]\n",
      " [0.46134025]\n",
      " [0.47794503]\n",
      " [0.4467628 ]\n",
      " [0.5036222 ]\n",
      " [0.47654337]\n",
      " [0.55356175]\n",
      " [0.5463313 ]\n",
      " [0.565579  ]\n",
      " [0.57224315]\n",
      " [0.6020307 ]\n",
      " [0.5968666 ]\n",
      " [0.5292081 ]\n",
      " [0.47653633]\n",
      " [0.509502  ]\n",
      " [0.41286322]\n",
      " [0.5022406 ]\n",
      " [0.49657902]\n",
      " [0.53235257]\n",
      " [0.5223618 ]\n",
      " [0.45456   ]]\n",
      "Accuracy for lstm: 0.5177777777777778\n",
      "Precision for lstm: 0.5789473684210527, Recall for lstm: 0.6470588235294118, F1 Score for lstm: 0.6111111111111112\n",
      "gru predictions [[0.4712723 ]\n",
      " [0.4935388 ]\n",
      " [0.5737077 ]\n",
      " [0.603312  ]\n",
      " [0.6250817 ]\n",
      " [0.6463419 ]\n",
      " [0.5759722 ]\n",
      " [0.5802329 ]\n",
      " [0.5787088 ]\n",
      " [0.5668107 ]\n",
      " [0.50996655]\n",
      " [0.4808881 ]\n",
      " [0.4872222 ]\n",
      " [0.49562278]\n",
      " [0.5185474 ]\n",
      " [0.54553187]\n",
      " [0.6338689 ]\n",
      " [0.65085024]\n",
      " [0.6222564 ]\n",
      " [0.714616  ]\n",
      " [0.5882419 ]\n",
      " [0.5477025 ]\n",
      " [0.47260305]\n",
      " [0.46102357]\n",
      " [0.4699224 ]\n",
      " [0.5426176 ]\n",
      " [0.61970836]\n",
      " [0.590553  ]\n",
      " [0.542107  ]\n",
      " [0.51265985]]\n",
      "Accuracy for gru: 0.5311111111111111\n",
      "Precision for gru: 0.5454545454545454, Recall for gru: 0.7058823529411765, F1 Score for gru: 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#  the test dataset will be input into the sub-models again to produce intermediate test data for the meta-learner. Afterward, the meta-learner will use the intermediate test predictions from the sub-models to make the final predictions.\n",
    "lstm_test_predictions = lstm_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "gru_test_predictions = gru_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "meta_X_test = np.concatenate((lstm_test_predictions, gru_test_predictions), axis=1)\n",
    "meta_X_test_tensor = torch.tensor(meta_X_test, dtype=torch.float32)\n",
    "\n",
    "meta_test_predictions = meta_model(meta_X_test_tensor).detach().numpy()\n",
    "print(\"meta predictions\", meta_test_predictions)\n",
    "# Evaluation metrics\n",
    "meta_test_predictions = np.round(meta_test_predictions+0.00001)\n",
    "accuracy = np.mean(meta_test_predictions == y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, meta_test_predictions, average='binary')\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "\n",
    "\n",
    "\n",
    "print(\"lstm predictions\", lstm_test_predictions)\n",
    "lstm_test_predictions = np.round(lstm_test_predictions)\n",
    "accuracy = np.mean(lstm_test_predictions == y_test)\n",
    "print(f'Accuracy for lstm: {accuracy}')\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, lstm_test_predictions, average='binary')\n",
    "print(f'Precision for lstm: {precision}, Recall for lstm: {recall}, F1 Score for lstm: {f1}')\n",
    "\n",
    "print(\"gru predictions\", gru_test_predictions)\n",
    "gru_test_predictions = np.round(gru_test_predictions)\n",
    "accuracy = np.mean(gru_test_predictions == y_test)\n",
    "print(f'Accuracy for gru: {accuracy}')\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, gru_test_predictions, average='binary')\n",
    "print(f'Precision for gru: {precision}, Recall for gru: {recall}, F1 Score for gru: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the models\n",
    "save_path = join('models/')\n",
    "# torch.save(lstm_model.state_dict(), save_path + 'lstm_model_amzn3.pth')\n",
    "# torch.save(gru_model.state_dict(), save_path + 'gru_model_amzn3.pth')\n",
    "# torch.save(meta_model.state_dict(), save_path + 'meta_model_amzn3.pth')\n",
    "\n",
    "# load the models\n",
    "# lstm_model = LSTMModel()\n",
    "# lstm_model.load_state_dict(torch.load(save_path + 'lstm_model_amzn2.pth'))\n",
    "# lstm_model.eval()\n",
    "\n",
    "# gru_model = GRUModel()\n",
    "# gru_model.load_state_dict(torch.load(save_path + 'gru_model_amzn2.pth'))\n",
    "# gru_model.eval()\n",
    "\n",
    "# meta_model = MetaLearner()\n",
    "# meta_model.load_state_dict(torch.load(save_path + 'meta_model_amzn2.pth'))\n",
    "# meta_model.eval()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
