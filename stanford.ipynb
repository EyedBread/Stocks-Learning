{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the level 1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Hyperparameters\n",
    "timesteps = 10  # Number of time steps\n",
    "num_features = 5  # Number of features\n",
    "n_epochs = 100  # Number of epochs\n",
    "learning_rate = 0.001\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(num_features, 60, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.lstm2 = nn.LSTM(60, 55, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.lstm3 = nn.LSTM(55, 40, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.lstm4 = nn.LSTM(40, 55, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(55, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = x[:, -1, :]  # Take the last output, remove this?\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(num_features, 60, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.gru2 = nn.GRU(60, 55, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.gru3 = nn.GRU(55, 40, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.gru4 = nn.GRU(40, 55, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(55, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.gru3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.gru4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = x[:, -1, :]  # Take the last output, remove this?\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a fully-connect neuralnetwork with three layers; the activation function for this model is the Rectified Linear Unit (ReLu).\n",
    "# NOTE: The paper doesn't specify the number of neurons in the hidden layers, so I'm basing on the stanford paper\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 30)\n",
    "        self.fc2 = nn.Linear(30, 25)\n",
    "        self.fc3 = nn.Linear(25, 20)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        # x = self.fc4(x)\n",
    "        x = self.sigmoid(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF TRAIN (83, 5)\n",
      "DF VAL (19, 5)\n",
      "DF TEST (19, 5)\n",
      "DF TRAIN    wsj_mean_compound  cnbc_mean_compound  fortune_mean_compound  \\\n",
      "0              0.296             -0.1366                 0.0000   \n",
      "1              0.000              0.0000                -0.2423   \n",
      "2              0.000              0.0000                 0.0000   \n",
      "3              0.000              0.0000                 0.0000   \n",
      "4              0.000              0.0000                 0.0000   \n",
      "\n",
      "   reuters_mean_compound    Adj Close  \n",
      "0                    0.0  2636.979980  \n",
      "1                    0.0  2651.500000  \n",
      "2                    0.0  2659.989990  \n",
      "3                    0.0  2664.110107  \n",
      "4                    0.0  2662.850098  \n",
      "10\n",
      "NEW\n",
      "x_train.shape (72, 10, 5)\n",
      "y_train.shape (72,)\n",
      "x_val.shape (8, 10, 5)\n",
      "y_val.shape (8,)\n",
      "x_test.shape (8, 10, 5)\n",
      "y_test.shape (8,)\n",
      "[[0.91548214 0.         0.82728766 0.10191495 0.19179757]\n",
      " [0.34690741 0.31576514 0.40678881 0.10191495 0.2415458 ]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.27063404]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.28475031]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.28043329]\n",
      " [0.01549494 0.31576514 0.82728766 0.10191495 0.24329318]\n",
      " [0.         0.31576514 0.82728766 0.10191495 0.32483647]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.37400167]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.34422836]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.33662233]]\n",
      "[1 0 0 1 1]\n",
      "[[0.32028808 0.41294732 0.82825113 0.1733249  0.25994479]\n",
      " [0.31745532 0.4259708  0.75866177 0.09811817 0.20964784]\n",
      " [0.29671103 0.45758086 0.67728109 0.15906251 0.28433877]\n",
      " [0.29118965 0.50551591 0.77864232 0.18551763 0.25799164]\n",
      " [0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      " [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      " [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      " [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      " [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      " [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]]\n",
      "0\n",
      "[[0.32610957 0.498087   0.76944387 0.26759267 0.31394061]\n",
      " [0.38574976 0.50526396 0.94749536 0.19756792 0.31150816]\n",
      " [0.39698747 0.51372288 0.83485069 0.2221625  0.40014387]\n",
      " [0.31509484 0.51934937 0.80658625 0.18104715 0.48675784]\n",
      " [0.27395342 0.48169417 0.77699124 0.18770471 0.50268925]\n",
      " [0.40076703 0.50464802 0.82945302 0.10321635 0.51094605]\n",
      " [0.30328349 0.48949446 0.80394212 0.10627921 0.44694521]\n",
      " [0.40488323 0.50721916 0.79442986 0.14275055 0.4846675 ]\n",
      " [0.30483637 0.48481091 0.65770141 0.09699912 0.47668423]\n",
      " [0.33894627 0.4781492  0.77809782 0.14909903 0.45215308]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "split = 0.69  # Adjust to allocate space for validation set\n",
    "val_split = 0.16  # 15% for validation, and implicitly 15% for test due to remaining percentage\n",
    "sequence_length = 10\n",
    "normalise = True\n",
    "batch_size = 100\n",
    "input_dim = 7\n",
    "input_timesteps = 9\n",
    "neurons = 50\n",
    "epochs = 5\n",
    "prediction_len = 1\n",
    "dense_output = 1\n",
    "drop_out = 0\n",
    "\n",
    "#filepath = 'data/original_dataset/source_price.csv'\n",
    "filepath = 'data/original_dataset/amzn_source_price.csv'\n",
    "\n",
    "# This approach does not normalize the compound scores in the range of 0 to 1\n",
    "def get_data_old(filepath: str):\n",
    "  # Load data, modify cols whenever necessary\n",
    "  dataframe = pd.read_csv(filepath)\n",
    "\n",
    "  #cols = ['Adj Close', 'wsj_mean_compound', 'cnbc_mean_compound', 'fortune_mean_compound', 'reuters_mean_compound']\n",
    "  cols = ['Adj Close', 'mean_compound_reuters', 'mean_compound_guardian', 'mean_compound_cnbc', 'mean_compound_other', 'mean_compound_stocktwits', 'mean_compound_twitter']\n",
    "\n",
    "  len_dataframe = dataframe.shape[0]\n",
    "\n",
    "  # Split data into train, validation, and test\n",
    "  i_split = int(len(dataframe) * split)\n",
    "  i_val = int(len(dataframe) * (split + val_split))\n",
    "\n",
    "  data_train = dataframe.get(cols).values[:i_split]\n",
    "  data_val = dataframe.get(cols).values[i_split:i_val]\n",
    "  data_test = dataframe.get(cols).values[i_val:]\n",
    "\n",
    "  # print(data_train[0:5,0])\n",
    "\n",
    "  len_train = len(data_train)\n",
    "  len_val = len(data_val)\n",
    "  len_test = len(data_test)\n",
    "  len_train_windows = None\n",
    "\n",
    "  # Process train data\n",
    "  data_windows = []\n",
    "  for i in range(len_train - sequence_length):\n",
    "      data_windows.append(data_train[i:i+sequence_length])\n",
    "  data_windows = np.array(data_windows).astype(float)\n",
    "  window_data = data_windows\n",
    "  win_num = window_data.shape[0]\n",
    "  col_num = window_data.shape[2]\n",
    "  normalised_data = []\n",
    "  record_min = []\n",
    "  record_max = []\n",
    "\n",
    "  # Normalize train data\n",
    "  for win_i in range(win_num):\n",
    "      normalised_window = []\n",
    "      for col_i in range(0,1):\n",
    "        temp_col = window_data[win_i,:,col_i]\n",
    "        temp_min = min(temp_col)\n",
    "        record_min.append(temp_min)\n",
    "        temp_col = temp_col - temp_min\n",
    "        temp_max = max(temp_col)\n",
    "        record_max.append(temp_max)\n",
    "        temp_col = temp_col / temp_max\n",
    "        normalised_window.append(temp_col)\n",
    "      for col_i in range(1,col_num):\n",
    "        normalised_window.append(window_data[win_i,:,col_i])\n",
    "      normalised_window = np.array(normalised_window).T\n",
    "      normalised_data.append(normalised_window)\n",
    "  normalised_data = np.array(normalised_data)\n",
    "  x_train = normalised_data[:, :-1]\n",
    "  # Classification problem now\n",
    "  y_train = []\n",
    "  for i in range(len_train - sequence_length):\n",
    "      current_last = data_train[i+sequence_length-1, 0]\n",
    "      next_first = data_train[i+sequence_length, 0]\n",
    "      y_train.append(1 if next_first > current_last else 0)\n",
    "  y_train = np.array(y_train)\n",
    "\n",
    "  # Process validation data\n",
    "  data_windows = []\n",
    "  for i in range(len_val - sequence_length):\n",
    "      data_windows.append(data_val[i:i+sequence_length])\n",
    "  data_windows = np.array(data_windows).astype(float)\n",
    "  window_data = data_windows\n",
    "  win_num = window_data.shape[0]\n",
    "  normalised_data = []\n",
    "\n",
    "  # Normalize validation data\n",
    "  for win_i in range(win_num):\n",
    "      normalised_window = []\n",
    "      for col_i in range(0,1):\n",
    "        temp_col = window_data[win_i,:,col_i]\n",
    "        temp_min = min(temp_col)\n",
    "        temp_col = temp_col - temp_min\n",
    "        temp_max = max(temp_col)\n",
    "        temp_col = temp_col / temp_max\n",
    "        normalised_window.append(temp_col)\n",
    "      for col_i in range(1,col_num):\n",
    "        normalised_window.append(window_data[win_i,:,col_i])\n",
    "      normalised_window = np.array(normalised_window).T\n",
    "      normalised_data.append(normalised_window)\n",
    "  normalised_data = np.array(normalised_data)\n",
    "  x_val = normalised_data[:, :-1]\n",
    "  y_val = []\n",
    "  for i in range(len_val - sequence_length):\n",
    "      current_last = data_val[i+sequence_length-1, 0]\n",
    "      next_first = data_val[i+sequence_length, 0]\n",
    "      y_val.append(1 if next_first > current_last else 0)\n",
    "  y_val = np.array(y_val)\n",
    "\n",
    "  # Process test data\n",
    "  data_windows = []\n",
    "  for i in range(len_test - sequence_length):\n",
    "      data_windows.append(data_test[i:i+sequence_length])\n",
    "  data_windows = np.array(data_windows).astype(float)\n",
    "  y_test_ori = data_windows[:, -1, [0]]\n",
    "  window_data = data_windows\n",
    "  win_num = window_data.shape[0]\n",
    "  normalised_data = []\n",
    "\n",
    "  # Normalize test data\n",
    "  for win_i in range(win_num):\n",
    "      normalised_window = []\n",
    "      for col_i in range(0,1):\n",
    "        temp_col = window_data[win_i,:,col_i]\n",
    "        temp_min = min(temp_col)\n",
    "        temp_col = temp_col - temp_min\n",
    "        temp_max = max(temp_col)\n",
    "        temp_col = temp_col / temp_max\n",
    "        normalised_window.append(temp_col)\n",
    "      for col_i in range(1,col_num):\n",
    "        normalised_window.append(window_data[win_i,:,col_i])\n",
    "      normalised_window = np.array(normalised_window).T\n",
    "      normalised_data.append(normalised_window)\n",
    "  normalised_data = np.array(normalised_data)\n",
    "  x_test = normalised_data[:, :-1]\n",
    "  y_test = []\n",
    "  for i in range(len_test - sequence_length):\n",
    "      current_last = data_test[i+sequence_length-1, 0]\n",
    "      next_first = data_test[i+sequence_length, 0]\n",
    "      y_test.append(1 if next_first > current_last else 0)\n",
    "  y_test = np.array(y_test)\n",
    "  return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "def get_data_normal(filepath: str):\n",
    "  df = pd.read_csv(filepath)\n",
    "  # Partition data into training, validation and test sets. Training data should be from date 12/07/2017 to 04/09/2018, validation data (from 04/10/2018 to 05/04/2018), and test data (from 05/07/2018 to 06/01/2018)\n",
    "\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "  # Count entries in 'date' column\n",
    "  num_entries = df['date'].count()\n",
    "\n",
    "  # Calculate indices for train, validation, and test splits\n",
    "  num_train = int(0.7 * num_entries)\n",
    "  num_val = int(0.15 * num_entries)\n",
    "  num_test = num_entries - num_train - num_val  # Ensuring all rows are included\n",
    "\n",
    "  # Split the data\n",
    "  df_train = df.loc[:num_train - 1]\n",
    "  df_val = df.loc[num_train:num_train + num_val - 1]\n",
    "  df_test = df.loc[num_train + num_val:]\n",
    "\n",
    "  # Hardcodidly extracting the exact dates for the partitioning\n",
    "  '''df_train = df.loc[0:82]\n",
    "  df_val = df.loc[83:101]\n",
    "  df_test = df.loc[102:]'''\n",
    "\n",
    "  # print(df_val.head())\n",
    "  # print(df_val.tail())\n",
    "  # print(df_test.head())\n",
    "  # print(df_val)\n",
    "\n",
    "  df_train = df_train.drop(columns=['date'])\n",
    "  df_val = df_val.drop(columns=['date'])\n",
    "  df_test = df_test.drop(columns=['date'])\n",
    "\n",
    "  print(\"DF TRAIN\", df_train.shape)\n",
    "  print(\"DF VAL\", df_val.shape)\n",
    "  print(\"DF TEST\", df_test.shape)\n",
    "\n",
    "  sc = MinMaxScaler(feature_range=(0,1))\n",
    "  print(\"DF TRAIN\", df_train.head())\n",
    "  df_train = sc.fit_transform(df_train)\n",
    "  df_val = sc.transform(df_val)\n",
    "  df_test = sc.transform(df_test)\n",
    "\n",
    "  def create_sequences_numpy_classification(data, n_days):\n",
    "      X, y = [], []\n",
    "      for i in range(n_days, len(data) - 1): \n",
    "          # print(\"X\")\n",
    "          X.append(data[i-n_days:i])\n",
    "          # print(data[i-n_days:i])\n",
    "          y.append(1 if data[i][-1] - data[i-1][-1] > 0 else 0) #Classification task\n",
    "          # print(data[i][-1], data[i-1][-1])\n",
    "          # print(\"Y\")\n",
    "          # print(y[-1])\n",
    "          \n",
    "      # Delete the first column of X\n",
    "      # X = np.delete(X, 0, axis=2)\n",
    "      return np.array(X), np.array(y)\n",
    "\n",
    "  # df = \n",
    "  print(timesteps)\n",
    "  x_train, y_train = create_sequences_numpy_classification(df_train, 10)\n",
    "  x_val, y_val = create_sequences_numpy_classification(df_val, 10)\n",
    "  x_test, y_test = create_sequences_numpy_classification(df_test, 10)\n",
    "  return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_data_normal(filepath)\n",
    "\n",
    "print(\"NEW\")\n",
    "\n",
    "print('x_train.shape', x_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('x_val.shape', x_val.shape)\n",
    "print('y_val.shape', y_val.shape)\n",
    "print('x_test.shape', x_test.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "print(y_train[0:5])\n",
    "\n",
    "print(x_val[0])\n",
    "\n",
    "print(y_val[0])\n",
    "\n",
    "print(x_test[0])\n",
    "\n",
    "print(y_test[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instatiate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Instantiate models\n",
    "lstm_model = LSTMModel()\n",
    "gru_model = GRUModel()\n",
    "meta_model = MetaLearner()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "lstm_optimizer = optim.RMSprop(lstm_model.parameters(), lr=0.0008) # 16 batch size, 150 epochs\n",
    "gru_optimizer = optim.RMSprop(gru_model.parameters(), lr=0.0008) # 16 batch size, 200 epochs\n",
    "base_models_batch_size = 16\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=0.008) # 100 epochs, 8 batch size\n",
    "meta_learner_batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 10, 5])\n",
      "torch.Size([72])\n",
      "Training LSTM Model\n",
      "Epoch 1/150, Loss: 0.7003460049629211\n",
      "Epoch 2/150, Loss: 0.6974965691566467\n",
      "Epoch 3/150, Loss: 0.6934533834457397\n",
      "Epoch 4/150, Loss: 0.692246949672699\n",
      "Epoch 5/150, Loss: 0.6908804178237915\n",
      "Epoch 6/150, Loss: 0.6869536638259888\n",
      "Epoch 7/150, Loss: 0.6912757635116578\n",
      "Epoch 8/150, Loss: 0.6914603590965271\n",
      "Epoch 9/150, Loss: 0.6851376533508301\n",
      "Epoch 10/150, Loss: 0.6872383236885071\n",
      "Epoch 11/150, Loss: 0.6875799655914306\n",
      "Epoch 12/150, Loss: 0.690709912776947\n",
      "Epoch 13/150, Loss: 0.6906359553337097\n",
      "Epoch 14/150, Loss: 0.6860170602798462\n",
      "Epoch 15/150, Loss: 0.6862020373344422\n",
      "Epoch 16/150, Loss: 0.6914613723754883\n",
      "Epoch 17/150, Loss: 0.6870274782180786\n",
      "Epoch 18/150, Loss: 0.6869072675704956\n",
      "Epoch 19/150, Loss: 0.6872516870498657\n",
      "Epoch 20/150, Loss: 0.6874948263168335\n",
      "Epoch 21/150, Loss: 0.6845704913139343\n",
      "Epoch 22/150, Loss: 0.6815796732902527\n",
      "Epoch 23/150, Loss: 0.6857285618782043\n",
      "Epoch 24/150, Loss: 0.6843027353286744\n",
      "Epoch 25/150, Loss: 0.6825546622276306\n",
      "Epoch 26/150, Loss: 0.6775046348571777\n",
      "Epoch 27/150, Loss: 0.6828066110610962\n",
      "Epoch 28/150, Loss: 0.6689561009407043\n",
      "Epoch 29/150, Loss: 0.673319673538208\n",
      "Epoch 30/150, Loss: 0.6810524344444275\n",
      "Epoch 31/150, Loss: 0.666046929359436\n",
      "Epoch 32/150, Loss: 0.6718045353889466\n",
      "Epoch 33/150, Loss: 0.6705293178558349\n",
      "Epoch 34/150, Loss: 0.6764765381813049\n",
      "Epoch 35/150, Loss: 0.671710479259491\n",
      "Epoch 36/150, Loss: 0.6631040930747986\n",
      "Epoch 37/150, Loss: 0.6764531970024109\n",
      "Epoch 38/150, Loss: 0.6682127237319946\n",
      "Epoch 39/150, Loss: 0.6670150279998779\n",
      "Epoch 40/150, Loss: 0.6563802003860474\n",
      "Epoch 41/150, Loss: 0.6664433121681214\n",
      "Epoch 42/150, Loss: 0.6733477354049683\n",
      "Epoch 43/150, Loss: 0.6730713605880737\n",
      "Epoch 44/150, Loss: 0.6651883006095887\n",
      "Epoch 45/150, Loss: 0.6755184769630432\n",
      "Epoch 46/150, Loss: 0.6492196083068847\n",
      "Epoch 47/150, Loss: 0.6592747569084167\n",
      "Epoch 48/150, Loss: 0.6837239027023315\n",
      "Epoch 49/150, Loss: 0.6605489015579223\n",
      "Epoch 50/150, Loss: 0.6457675576210022\n",
      "Epoch 51/150, Loss: 0.6589687585830688\n",
      "Epoch 52/150, Loss: 0.6540608048439026\n",
      "Epoch 53/150, Loss: 0.6531304359436035\n",
      "Epoch 54/150, Loss: 0.6596420168876648\n",
      "Epoch 55/150, Loss: 0.6543422341346741\n",
      "Epoch 56/150, Loss: 0.644822359085083\n",
      "Epoch 57/150, Loss: 0.6540200114250183\n",
      "Epoch 58/150, Loss: 0.6484745383262634\n",
      "Epoch 59/150, Loss: 0.6488949060440063\n",
      "Epoch 60/150, Loss: 0.6543043613433838\n",
      "Epoch 61/150, Loss: 0.6434768319129944\n",
      "Epoch 62/150, Loss: 0.6362223505973816\n",
      "Epoch 63/150, Loss: 0.6621699094772339\n",
      "Epoch 64/150, Loss: 0.644167423248291\n",
      "Epoch 65/150, Loss: 0.6556861877441407\n",
      "Epoch 66/150, Loss: 0.6407621622085571\n",
      "Epoch 67/150, Loss: 0.6391911268234253\n",
      "Epoch 68/150, Loss: 0.6288092255592346\n",
      "Epoch 69/150, Loss: 0.6487741470336914\n",
      "Epoch 70/150, Loss: 0.6473896622657775\n",
      "Epoch 71/150, Loss: 0.6385205268859864\n",
      "Epoch 72/150, Loss: 0.6294607877731323\n",
      "Epoch 73/150, Loss: 0.6441179275512695\n",
      "Epoch 74/150, Loss: 0.6522816181182861\n",
      "Epoch 75/150, Loss: 0.638844656944275\n",
      "Epoch 76/150, Loss: 0.6522531270980835\n",
      "Epoch 77/150, Loss: 0.6459657907485962\n",
      "Epoch 78/150, Loss: 0.6480371832847596\n",
      "Epoch 79/150, Loss: 0.6339888334274292\n",
      "Epoch 80/150, Loss: 0.6405140042304993\n",
      "Epoch 81/150, Loss: 0.6304888725280762\n",
      "Epoch 82/150, Loss: 0.6257286548614502\n",
      "Epoch 83/150, Loss: 0.6459786415100097\n",
      "Epoch 84/150, Loss: 0.6311799764633179\n",
      "Epoch 85/150, Loss: 0.6257369160652161\n",
      "Epoch 86/150, Loss: 0.6248991847038269\n",
      "Epoch 87/150, Loss: 0.6315141320228577\n",
      "Epoch 88/150, Loss: 0.6602330803871155\n",
      "Epoch 89/150, Loss: 0.6403735041618347\n",
      "Epoch 90/150, Loss: 0.6400546789169311\n",
      "Epoch 91/150, Loss: 0.6513618230819702\n",
      "Epoch 92/150, Loss: 0.6212706446647644\n",
      "Epoch 93/150, Loss: 0.6329151749610901\n",
      "Epoch 94/150, Loss: 0.6221989989280701\n",
      "Epoch 95/150, Loss: 0.6395546436309815\n",
      "Epoch 96/150, Loss: 0.6407634973526001\n",
      "Epoch 97/150, Loss: 0.6172132849693298\n",
      "Epoch 98/150, Loss: 0.6068729877471923\n",
      "Epoch 99/150, Loss: 0.6424711227416993\n",
      "Epoch 100/150, Loss: 0.6151341319084167\n",
      "Epoch 101/150, Loss: 0.6154982089996338\n",
      "Epoch 102/150, Loss: 0.6185618996620178\n",
      "Epoch 103/150, Loss: 0.6106028437614441\n",
      "Epoch 104/150, Loss: 0.6294889450073242\n",
      "Epoch 105/150, Loss: 0.6434095621109008\n",
      "Epoch 106/150, Loss: 0.6282450318336487\n",
      "Epoch 107/150, Loss: 0.6360105037689209\n",
      "Epoch 108/150, Loss: 0.6431203484535217\n",
      "Epoch 109/150, Loss: 0.6315492153167724\n",
      "Epoch 110/150, Loss: 0.6171732544898987\n",
      "Epoch 111/150, Loss: 0.6196479082107544\n",
      "Epoch 112/150, Loss: 0.6066284060478211\n",
      "Epoch 113/150, Loss: 0.6040421247482299\n",
      "Epoch 114/150, Loss: 0.6263595342636108\n",
      "Epoch 115/150, Loss: 0.6292592525482178\n",
      "Epoch 116/150, Loss: 0.6303136587142945\n",
      "Epoch 117/150, Loss: 0.641015088558197\n",
      "Epoch 118/150, Loss: 0.6282219409942627\n",
      "Epoch 119/150, Loss: 0.5934190273284912\n",
      "Epoch 120/150, Loss: 0.6172554612159729\n",
      "Epoch 121/150, Loss: 0.6085163831710816\n",
      "Epoch 122/150, Loss: 0.6178853511810303\n",
      "Epoch 123/150, Loss: 0.6426041007041932\n",
      "Epoch 124/150, Loss: 0.6079962015151977\n",
      "Epoch 125/150, Loss: 0.6265937328338623\n",
      "Epoch 126/150, Loss: 0.6088129162788392\n",
      "Epoch 127/150, Loss: 0.6326120853424072\n",
      "Epoch 128/150, Loss: 0.635901665687561\n",
      "Epoch 129/150, Loss: 0.6304748296737671\n",
      "Epoch 130/150, Loss: 0.6080274701118469\n",
      "Epoch 131/150, Loss: 0.6032908320426941\n",
      "Epoch 132/150, Loss: 0.5931442379951477\n",
      "Epoch 133/150, Loss: 0.5849923849105835\n",
      "Epoch 134/150, Loss: 0.6196175575256347\n",
      "Epoch 135/150, Loss: 0.5939917683601379\n",
      "Epoch 136/150, Loss: 0.6086485385894775\n",
      "Epoch 137/150, Loss: 0.6175013065338135\n",
      "Epoch 138/150, Loss: 0.6410298228263855\n",
      "Epoch 139/150, Loss: 0.6335746884346009\n",
      "Epoch 140/150, Loss: 0.6185372710227967\n",
      "Epoch 141/150, Loss: 0.6157740950584412\n",
      "Epoch 142/150, Loss: 0.6114486694335938\n",
      "Epoch 143/150, Loss: 0.5854355454444885\n",
      "Epoch 144/150, Loss: 0.5955761790275573\n",
      "Epoch 145/150, Loss: 0.5868746042251587\n",
      "Epoch 146/150, Loss: 0.6249194025993348\n",
      "Epoch 147/150, Loss: 0.6510078549385071\n",
      "Epoch 148/150, Loss: 0.6079158306121826\n",
      "Epoch 149/150, Loss: 0.5894520878791809\n",
      "Epoch 150/150, Loss: 0.6122908592224121\n",
      "Training GRU Model\n",
      "Epoch 1/200, Loss: 0.7043912529945373\n",
      "Epoch 2/200, Loss: 0.6928617238998414\n",
      "Epoch 3/200, Loss: 0.689569890499115\n",
      "Epoch 4/200, Loss: 0.6888657450675965\n",
      "Epoch 5/200, Loss: 0.6880431413650513\n",
      "Epoch 6/200, Loss: 0.6912811636924744\n",
      "Epoch 7/200, Loss: 0.6898053407669067\n",
      "Epoch 8/200, Loss: 0.691400671005249\n",
      "Epoch 9/200, Loss: 0.6869827151298523\n",
      "Epoch 10/200, Loss: 0.685366690158844\n",
      "Epoch 11/200, Loss: 0.6807061672210694\n",
      "Epoch 12/200, Loss: 0.6754448294639588\n",
      "Epoch 13/200, Loss: 0.6757916569709778\n",
      "Epoch 14/200, Loss: 0.6708906292915344\n",
      "Epoch 15/200, Loss: 0.6637634634971619\n",
      "Epoch 16/200, Loss: 0.673380434513092\n",
      "Epoch 17/200, Loss: 0.6681013107299805\n",
      "Epoch 18/200, Loss: 0.665096664428711\n",
      "Epoch 19/200, Loss: 0.6633304357528687\n",
      "Epoch 20/200, Loss: 0.6627350449562073\n",
      "Epoch 21/200, Loss: 0.6581379771232605\n",
      "Epoch 22/200, Loss: 0.6689114451408387\n",
      "Epoch 23/200, Loss: 0.6616144418716431\n",
      "Epoch 24/200, Loss: 0.6546154260635376\n",
      "Epoch 25/200, Loss: 0.6565430521965027\n",
      "Epoch 26/200, Loss: 0.6552096843719483\n",
      "Epoch 27/200, Loss: 0.6552441358566284\n",
      "Epoch 28/200, Loss: 0.6572003722190857\n",
      "Epoch 29/200, Loss: 0.6509425401687622\n",
      "Epoch 30/200, Loss: 0.6515372037887573\n",
      "Epoch 31/200, Loss: 0.6511563777923584\n",
      "Epoch 32/200, Loss: 0.6469636201858521\n",
      "Epoch 33/200, Loss: 0.6492560625076294\n",
      "Epoch 34/200, Loss: 0.6459864377975464\n",
      "Epoch 35/200, Loss: 0.6359035849571228\n",
      "Epoch 36/200, Loss: 0.6414318442344665\n",
      "Epoch 37/200, Loss: 0.6385035037994384\n",
      "Epoch 38/200, Loss: 0.6391935706138611\n",
      "Epoch 39/200, Loss: 0.6411311030387878\n",
      "Epoch 40/200, Loss: 0.637386429309845\n",
      "Epoch 41/200, Loss: 0.6378549575805664\n",
      "Epoch 42/200, Loss: 0.6329157948493958\n",
      "Epoch 43/200, Loss: 0.6455176830291748\n",
      "Epoch 44/200, Loss: 0.6339334964752197\n",
      "Epoch 45/200, Loss: 0.6349519729614258\n",
      "Epoch 46/200, Loss: 0.6245455980300904\n",
      "Epoch 47/200, Loss: 0.631574833393097\n",
      "Epoch 48/200, Loss: 0.6430026769638062\n",
      "Epoch 49/200, Loss: 0.6207963585853576\n",
      "Epoch 50/200, Loss: 0.6358530759811402\n",
      "Epoch 51/200, Loss: 0.6234001040458679\n",
      "Epoch 52/200, Loss: 0.6282943367958069\n",
      "Epoch 53/200, Loss: 0.6196016907691956\n",
      "Epoch 54/200, Loss: 0.6182052254676819\n",
      "Epoch 55/200, Loss: 0.607943058013916\n",
      "Epoch 56/200, Loss: 0.6169657707214355\n",
      "Epoch 57/200, Loss: 0.6067275166511535\n",
      "Epoch 58/200, Loss: 0.6437430143356323\n",
      "Epoch 59/200, Loss: 0.5968945145606994\n",
      "Epoch 60/200, Loss: 0.6134971737861633\n",
      "Epoch 61/200, Loss: 0.6221610188484192\n",
      "Epoch 62/200, Loss: 0.6059638977050781\n",
      "Epoch 63/200, Loss: 0.6027214527130127\n",
      "Epoch 64/200, Loss: 0.6003394603729248\n",
      "Epoch 65/200, Loss: 0.6090511083602905\n",
      "Epoch 66/200, Loss: 0.6081276416778565\n",
      "Epoch 67/200, Loss: 0.5969079017639161\n",
      "Epoch 68/200, Loss: 0.6025028944015502\n",
      "Epoch 69/200, Loss: 0.599382197856903\n",
      "Epoch 70/200, Loss: 0.5892079472541809\n",
      "Epoch 71/200, Loss: 0.5937203168869019\n",
      "Epoch 72/200, Loss: 0.6145278811454773\n",
      "Epoch 73/200, Loss: 0.6205615639686585\n",
      "Epoch 74/200, Loss: 0.5876350402832031\n",
      "Epoch 75/200, Loss: 0.5737467169761657\n",
      "Epoch 76/200, Loss: 0.5674224853515625\n",
      "Epoch 77/200, Loss: 0.5604165613651275\n",
      "Epoch 78/200, Loss: 0.577195155620575\n",
      "Epoch 79/200, Loss: 0.5457293629646301\n",
      "Epoch 80/200, Loss: 0.5588548898696899\n",
      "Epoch 81/200, Loss: 0.5205901265144348\n",
      "Epoch 82/200, Loss: 0.5729586005210876\n",
      "Epoch 83/200, Loss: 0.567579573392868\n",
      "Epoch 84/200, Loss: 0.5280941963195801\n",
      "Epoch 85/200, Loss: 0.5890371561050415\n",
      "Epoch 86/200, Loss: 0.6263591170310974\n",
      "Epoch 87/200, Loss: 0.6074291586875915\n",
      "Epoch 88/200, Loss: 0.5483620584011077\n",
      "Epoch 89/200, Loss: 0.5255719423294067\n",
      "Epoch 90/200, Loss: 0.5206998705863952\n",
      "Epoch 91/200, Loss: 0.5951040387153625\n",
      "Epoch 92/200, Loss: 0.6080666899681091\n",
      "Epoch 93/200, Loss: 0.6078467726707458\n",
      "Epoch 94/200, Loss: 0.5372323393821716\n",
      "Epoch 95/200, Loss: 0.5348886251449585\n",
      "Epoch 96/200, Loss: 0.4969188094139099\n",
      "Epoch 97/200, Loss: 0.4950201749801636\n",
      "Epoch 98/200, Loss: 0.5067884683609009\n",
      "Epoch 99/200, Loss: 0.4922424376010895\n",
      "Epoch 100/200, Loss: 0.5192986190319061\n",
      "Epoch 101/200, Loss: 0.4884155511856079\n",
      "Epoch 102/200, Loss: 0.4705485045909882\n",
      "Epoch 103/200, Loss: 0.5539828658103942\n",
      "Epoch 104/200, Loss: 0.5516649127006531\n",
      "Epoch 105/200, Loss: 0.615984034538269\n",
      "Epoch 106/200, Loss: 0.5615340590476989\n",
      "Epoch 107/200, Loss: 0.5166870415210724\n",
      "Epoch 108/200, Loss: 0.49374725818634035\n",
      "Epoch 109/200, Loss: 0.4663411021232605\n",
      "Epoch 110/200, Loss: 0.5051673650741577\n",
      "Epoch 111/200, Loss: 0.5269082367420197\n",
      "Epoch 112/200, Loss: 0.5099833548069\n",
      "Epoch 113/200, Loss: 0.4577674686908722\n",
      "Epoch 114/200, Loss: 0.46506637930870054\n",
      "Epoch 115/200, Loss: 0.49482194185256956\n",
      "Epoch 116/200, Loss: 0.4578897714614868\n",
      "Epoch 117/200, Loss: 0.6556384265422821\n",
      "Epoch 118/200, Loss: 0.4923834681510925\n",
      "Epoch 119/200, Loss: 0.4570115149021149\n",
      "Epoch 120/200, Loss: 0.6234826564788818\n",
      "Epoch 121/200, Loss: 0.488683420419693\n",
      "Epoch 122/200, Loss: 0.4497968673706055\n",
      "Epoch 123/200, Loss: 0.47564404010772704\n",
      "Epoch 124/200, Loss: 0.49091712236404417\n",
      "Epoch 125/200, Loss: 0.46196592450141905\n",
      "Epoch 126/200, Loss: 0.41154206693172457\n",
      "Epoch 127/200, Loss: 0.5062426865100861\n",
      "Epoch 128/200, Loss: 0.4290536642074585\n",
      "Epoch 129/200, Loss: 0.43687511682510377\n",
      "Epoch 130/200, Loss: 0.4252239465713501\n",
      "Epoch 131/200, Loss: 0.4068194508552551\n",
      "Epoch 132/200, Loss: 0.41960949897766114\n",
      "Epoch 133/200, Loss: 0.40381933450698854\n",
      "Epoch 134/200, Loss: 0.40019531846046447\n",
      "Epoch 135/200, Loss: 0.3782386690378189\n",
      "Epoch 136/200, Loss: 0.473230367898941\n",
      "Epoch 137/200, Loss: 0.43149577975273135\n",
      "Epoch 138/200, Loss: 0.4665416181087494\n",
      "Epoch 139/200, Loss: 0.4877664625644684\n",
      "Epoch 140/200, Loss: 0.40602561831474304\n",
      "Epoch 141/200, Loss: 0.4961126834154129\n",
      "Epoch 142/200, Loss: 0.49125100374221803\n",
      "Epoch 143/200, Loss: 0.5037389159202575\n",
      "Epoch 144/200, Loss: 0.6058984935283661\n",
      "Epoch 145/200, Loss: 0.5501773953437805\n",
      "Epoch 146/200, Loss: 0.5050676524639129\n",
      "Epoch 147/200, Loss: 0.4016780823469162\n",
      "Epoch 148/200, Loss: 0.42156217694282533\n",
      "Epoch 149/200, Loss: 0.3759755343198776\n",
      "Epoch 150/200, Loss: 0.3791791319847107\n",
      "Epoch 151/200, Loss: 0.39334464967250826\n",
      "Epoch 152/200, Loss: 0.38864626586437223\n",
      "Epoch 153/200, Loss: 0.39425632655620574\n",
      "Epoch 154/200, Loss: 0.46204487681388856\n",
      "Epoch 155/200, Loss: 0.4911055028438568\n",
      "Epoch 156/200, Loss: 0.3822517603635788\n",
      "Epoch 157/200, Loss: 0.39491266906261446\n",
      "Epoch 158/200, Loss: 0.3577442839741707\n",
      "Epoch 159/200, Loss: 0.40706728100776673\n",
      "Epoch 160/200, Loss: 0.3778116375207901\n",
      "Epoch 161/200, Loss: 0.3734789937734604\n",
      "Epoch 162/200, Loss: 0.42606101632118226\n",
      "Epoch 163/200, Loss: 0.48718694448471067\n",
      "Epoch 164/200, Loss: 0.4425767719745636\n",
      "Epoch 165/200, Loss: 0.479167228937149\n",
      "Epoch 166/200, Loss: 0.4691519021987915\n",
      "Epoch 167/200, Loss: 0.38332957625389097\n",
      "Epoch 168/200, Loss: 0.3734671175479889\n",
      "Epoch 169/200, Loss: 0.3629035592079163\n",
      "Epoch 170/200, Loss: 0.3498775914311409\n",
      "Epoch 171/200, Loss: 0.3743247717618942\n",
      "Epoch 172/200, Loss: 0.4995280742645264\n",
      "Epoch 173/200, Loss: 0.5330666244029999\n",
      "Epoch 174/200, Loss: 0.35596444457769394\n",
      "Epoch 175/200, Loss: 0.3545511156320572\n",
      "Epoch 176/200, Loss: 0.3557142555713654\n",
      "Epoch 177/200, Loss: 0.3666003540158272\n",
      "Epoch 178/200, Loss: 0.38414123356342317\n",
      "Epoch 179/200, Loss: 0.3771702259778976\n",
      "Epoch 180/200, Loss: 0.3399245575070381\n",
      "Epoch 181/200, Loss: 0.42344374060630796\n",
      "Epoch 182/200, Loss: 0.44234163761138917\n",
      "Epoch 183/200, Loss: 0.4835307240486145\n",
      "Epoch 184/200, Loss: 0.3814560443162918\n",
      "Epoch 185/200, Loss: 0.38171977996826173\n",
      "Epoch 186/200, Loss: 0.3630046397447586\n",
      "Epoch 187/200, Loss: 0.43400720357894895\n",
      "Epoch 188/200, Loss: 0.3695653647184372\n",
      "Epoch 189/200, Loss: 0.3347278356552124\n",
      "Epoch 190/200, Loss: 0.4112365394830704\n",
      "Epoch 191/200, Loss: 0.38921753168106077\n",
      "Epoch 192/200, Loss: 0.4484695136547089\n",
      "Epoch 193/200, Loss: 0.36473415791988373\n",
      "Epoch 194/200, Loss: 0.5797397673130036\n",
      "Epoch 195/200, Loss: 0.3557407021522522\n",
      "Epoch 196/200, Loss: 0.36890835911035535\n",
      "Epoch 197/200, Loss: 0.36812515556812286\n",
      "Epoch 198/200, Loss: 0.33937751352787016\n",
      "Epoch 199/200, Loss: 0.3557193160057068\n",
      "Epoch 200/200, Loss: 0.3651784807443619\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors and create DataLoader\n",
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=base_models_batch_size, shuffle=False)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, optimizer, criterion, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "# Train the LSTM model\n",
    "print(\"Training LSTM Model\")\n",
    "train_model(lstm_model, lstm_optimizer, criterion, train_loader, 150)\n",
    "\n",
    "# Train the GRU model\n",
    "print(\"Training GRU Model\")\n",
    "train_model(gru_model, gru_optimizer, criterion, train_loader, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use base models to predict the validation data, this will be used as input to the Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2)\n",
      "[[0.47509038 0.91230047]\n",
      " [0.53691447 0.97367114]\n",
      " [0.4969233  0.97946036]\n",
      " [0.5670254  0.9718755 ]\n",
      " [0.61151016 0.9946128 ]\n",
      " [0.7361504  0.9928397 ]\n",
      " [0.6731145  0.99363685]\n",
      " [0.70976865 0.9773311 ]]\n",
      "[0 1 1 1 0 1 0 0]\n",
      "[[[0.32028808 0.41294732 0.82825113 0.1733249  0.25994479]\n",
      "  [0.31745532 0.4259708  0.75866177 0.09811817 0.20964784]\n",
      "  [0.29671103 0.45758086 0.67728109 0.15906251 0.28433877]\n",
      "  [0.29118965 0.50551591 0.77864232 0.18551763 0.25799164]\n",
      "  [0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      "  [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]]\n",
      "\n",
      " [[0.31745532 0.4259708  0.75866177 0.09811817 0.20964784]\n",
      "  [0.29671103 0.45758086 0.67728109 0.15906251 0.28433877]\n",
      "  [0.29118965 0.50551591 0.77864232 0.18551763 0.25799164]\n",
      "  [0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      "  [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]]\n",
      "\n",
      " [[0.29671103 0.45758086 0.67728109 0.15906251 0.28433877]\n",
      "  [0.29118965 0.50551591 0.77864232 0.18551763 0.25799164]\n",
      "  [0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      "  [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]]\n",
      "\n",
      " [[0.29118965 0.50551591 0.77864232 0.18551763 0.25799164]\n",
      "  [0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      "  [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]\n",
      "  [0.3768951  0.51940223 0.67926641 0.32705394 0.29444584]]\n",
      "\n",
      " [[0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      "  [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]\n",
      "  [0.3768951  0.51940223 0.67926641 0.32705394 0.29444584]\n",
      "  [0.36808754 0.51518182 0.87763535 0.34183591 0.3046215 ]]\n",
      "\n",
      " [[0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]\n",
      "  [0.3768951  0.51940223 0.67926641 0.32705394 0.29444584]\n",
      "  [0.36808754 0.51518182 0.87763535 0.34183591 0.3046215 ]\n",
      "  [0.32870867 0.46383826 0.86430149 0.24956852 0.22972564]]\n",
      "\n",
      " [[0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]\n",
      "  [0.3768951  0.51940223 0.67926641 0.32705394 0.29444584]\n",
      "  [0.36808754 0.51518182 0.87763535 0.34183591 0.3046215 ]\n",
      "  [0.32870867 0.46383826 0.86430149 0.24956852 0.22972564]\n",
      "  [0.2607303  0.51050885 0.80422258 0.25600435 0.25285236]]\n",
      "\n",
      " [[0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]\n",
      "  [0.3768951  0.51940223 0.67926641 0.32705394 0.29444584]\n",
      "  [0.36808754 0.51518182 0.87763535 0.34183591 0.3046215 ]\n",
      "  [0.32870867 0.46383826 0.86430149 0.24956852 0.22972564]\n",
      "  [0.2607303  0.51050885 0.80422258 0.25600435 0.25285236]\n",
      "  [0.34807525 0.51931866 0.90250017 0.22192608 0.18730908]]]\n"
     ]
    }
   ],
   "source": [
    "lstm_val_predictions = lstm_model(torch.tensor(x_val, dtype=torch.float32)).detach().numpy()\n",
    "gru_val_predictions = gru_model(torch.tensor(x_val, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "\n",
    "# Combine predictions to form new training data for the meta-learner\n",
    "meta_X_train = np.concatenate((lstm_val_predictions, gru_val_predictions), axis=1)\n",
    "\n",
    "print(meta_X_train.shape)\n",
    "\n",
    "print(meta_X_train)\n",
    "\n",
    "print(y_val)\n",
    "\n",
    "print(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6997645497322083\n",
      "Epoch 2/20, Loss: 0.696587324142456\n",
      "Epoch 3/20, Loss: 0.6941204071044922\n",
      "Epoch 4/20, Loss: 0.693105161190033\n",
      "Epoch 5/20, Loss: 0.693304181098938\n",
      "Epoch 6/20, Loss: 0.6935972571372986\n",
      "Epoch 7/20, Loss: 0.6932333707809448\n",
      "Epoch 8/20, Loss: 0.6927485466003418\n",
      "Epoch 9/20, Loss: 0.6925041079521179\n",
      "Epoch 10/20, Loss: 0.6923808455467224\n",
      "Epoch 11/20, Loss: 0.6922394633293152\n",
      "Epoch 12/20, Loss: 0.6920518279075623\n",
      "Epoch 13/20, Loss: 0.6918182969093323\n",
      "Epoch 14/20, Loss: 0.6915668249130249\n",
      "Epoch 15/20, Loss: 0.6913098692893982\n",
      "Epoch 16/20, Loss: 0.6910309791564941\n",
      "Epoch 17/20, Loss: 0.6906957030296326\n",
      "Epoch 18/20, Loss: 0.6902801394462585\n",
      "Epoch 19/20, Loss: 0.6897610425949097\n",
      "Epoch 20/20, Loss: 0.6893341541290283\n"
     ]
    }
   ],
   "source": [
    "meta_model = MetaLearner()\n",
    "meta_criterion = nn.BCELoss()\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=0.008)\n",
    "\n",
    "meta_X_train_tensor = torch.tensor(meta_X_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "meta_train_dataset = TensorDataset(meta_X_train_tensor, y_val_tensor)\n",
    "meta_train_loader = DataLoader(meta_train_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "train_model(meta_model, meta_optimizer, meta_criterion, meta_train_loader, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.32610957  0.498087    0.76944387  0.26759267  0.31394061]\n",
      "  [ 0.38574976  0.50526396  0.94749536  0.19756792  0.31150816]\n",
      "  [ 0.39698747  0.51372288  0.83485069  0.2221625   0.40014387]\n",
      "  [ 0.31509484  0.51934937  0.80658625  0.18104715  0.48675784]\n",
      "  [ 0.27395342  0.48169417  0.77699124  0.18770471  0.50268925]\n",
      "  [ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]]\n",
      "\n",
      " [[ 0.38574976  0.50526396  0.94749536  0.19756792  0.31150816]\n",
      "  [ 0.39698747  0.51372288  0.83485069  0.2221625   0.40014387]\n",
      "  [ 0.31509484  0.51934937  0.80658625  0.18104715  0.48675784]\n",
      "  [ 0.27395342  0.48169417  0.77699124  0.18770471  0.50268925]\n",
      "  [ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]]\n",
      "\n",
      " [[ 0.39698747  0.51372288  0.83485069  0.2221625   0.40014387]\n",
      "  [ 0.31509484  0.51934937  0.80658625  0.18104715  0.48675784]\n",
      "  [ 0.27395342  0.48169417  0.77699124  0.18770471  0.50268925]\n",
      "  [ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]]\n",
      "\n",
      " [[ 0.31509484  0.51934937  0.80658625  0.18104715  0.48675784]\n",
      "  [ 0.27395342  0.48169417  0.77699124  0.18770471  0.50268925]\n",
      "  [ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]\n",
      "  [ 0.39636161  0.47074905  0.69749021  0.11735529  0.52177332]]\n",
      "\n",
      " [[ 0.27395342  0.48169417  0.77699124  0.18770471  0.50268925]\n",
      "  [ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]\n",
      "  [ 0.39636161  0.47074905  0.69749021  0.11735529  0.52177332]\n",
      "  [ 0.40165     0.40960394  0.72438152  0.10560386  0.50282643]]\n",
      "\n",
      " [[ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]\n",
      "  [ 0.39636161  0.47074905  0.69749021  0.11735529  0.52177332]\n",
      "  [ 0.40165     0.40960394  0.72438152  0.10560386  0.50282643]\n",
      "  [ 0.40509072  0.42541211  0.84733207  0.04562473  0.48079632]]\n",
      "\n",
      " [[ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]\n",
      "  [ 0.39636161  0.47074905  0.69749021  0.11735529  0.52177332]\n",
      "  [ 0.40165     0.40960394  0.72438152  0.10560386  0.50282643]\n",
      "  [ 0.40509072  0.42541211  0.84733207  0.04562473  0.48079632]\n",
      "  [ 0.24549445  0.47860006  0.78264948  0.02243377  0.37297449]]\n",
      "\n",
      " [[ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]\n",
      "  [ 0.39636161  0.47074905  0.69749021  0.11735529  0.52177332]\n",
      "  [ 0.40165     0.40960394  0.72438152  0.10560386  0.50282643]\n",
      "  [ 0.40509072  0.42541211  0.84733207  0.04562473  0.48079632]\n",
      "  [ 0.24549445  0.47860006  0.78264948  0.02243377  0.37297449]\n",
      "  [ 0.31354731  0.4038811   0.69371981 -0.03926118  0.48997825]]]\n",
      "tensor([[0.4606, 0.9688],\n",
      "        [0.4754, 0.7211],\n",
      "        [0.4413, 0.0442],\n",
      "        [0.5693, 0.9072],\n",
      "        [0.5584, 0.8887],\n",
      "        [0.5484, 0.8667],\n",
      "        [0.5416, 0.7983],\n",
      "        [0.5073, 0.9096]])\n",
      "[[0.5124894 ]\n",
      " [0.5030034 ]\n",
      " [0.49623203]\n",
      " [0.49592164]\n",
      " [0.4967462 ]\n",
      " [0.4973156 ]\n",
      " [0.496111  ]\n",
      " [0.50435245]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Accuracy: 0.53125\n",
      "Precision: 0.3333333333333333, Recall: 0.3333333333333333, F1 Score: 0.3333333333333333\n",
      "Accuracy for lstm: 0.46875\n",
      "Precision for lstm: 0.2, Recall for lstm: 0.3333333333333333, F1 Score for lstm: 0.25\n",
      "lstm predictions [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Accuracy for gru: 0.40625\n",
      "gru predictions [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Precision for gru: 0.2857142857142857, Recall for gru: 0.6666666666666666, F1 Score for gru: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#  the test dataset will be input into the sub-models again to produce intermediate test data for the meta-learner. Afterward, the meta-learner will use the intermediate test predictions from the sub-models to make the final predictions.\n",
    "print(x_test)\n",
    "lstm_test_predictions = lstm_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "gru_test_predictions = gru_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "meta_X_test = np.concatenate((lstm_test_predictions, gru_test_predictions), axis=1)\n",
    "meta_X_test_tensor = torch.tensor(meta_X_test, dtype=torch.float32)\n",
    "print(meta_X_test_tensor)\n",
    "\n",
    "meta_test_predictions = meta_model(meta_X_test_tensor).detach().numpy()\n",
    "print(meta_test_predictions)\n",
    "\n",
    "# Evaluation metrics\n",
    "meta_test_predictions = np.round(meta_test_predictions)\n",
    "print(meta_test_predictions)\n",
    "accuracy = np.mean(meta_test_predictions == y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, meta_test_predictions, average='binary')\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "\n",
    "\n",
    "lstm_test_predictions = np.round(lstm_test_predictions)\n",
    "accuracy = np.mean(lstm_test_predictions == y_test)\n",
    "print(f'Accuracy for lstm: {accuracy}')\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, lstm_test_predictions, average='binary')\n",
    "print(f'Precision for lstm: {precision}, Recall for lstm: {recall}, F1 Score for lstm: {f1}')\n",
    "print(\"lstm predictions\", lstm_test_predictions)\n",
    "\n",
    "gru_test_predictions = np.round(gru_test_predictions)\n",
    "accuracy = np.mean(gru_test_predictions == y_test)\n",
    "print(f'Accuracy for gru: {accuracy}')\n",
    "print(\"gru predictions\", gru_test_predictions)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, gru_test_predictions, average='binary')\n",
    "print(f'Precision for gru: {precision}, Recall for gru: {recall}, F1 Score for gru: {f1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
