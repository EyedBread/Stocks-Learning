{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the level 1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import data_prep\n",
    "\n",
    "# Hyperparameters\n",
    "timesteps = 10  # Number of time steps\n",
    "num_features = 6  # Number of features\n",
    "n_epochs = 100  # Number of epochs\n",
    "learning_rate = 0.001\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(num_features, 60, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.lstm2 = nn.LSTM(60, 55, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.lstm3 = nn.LSTM(55, 50, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.lstm4 = nn.LSTM(50, 45, batch_first=True)  \n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(45, 1) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = x[:, -1, :]  # Take the last output from the last LSTM layer\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(num_features, 60, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.gru2 = nn.GRU(60, 55, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.gru3 = nn.GRU(55, 50, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.gru4 = nn.GRU(50, 45, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(45, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.gru3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.gru4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = x[:, -1, :]  # Take the last output\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a fully-connect neuralnetwork with three layers; the activation function for this model is the Rectified Linear Unit (ReLu).\n",
    "# NOTE: The paper doesn't specify the number of neurons in the hidden layers, so I'm basing on the stanford paper\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 30)\n",
    "        self.fc2 = nn.Linear(30, 25)\n",
    "        self.fc3 = nn.Linear(25, 20)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape (666, 10, 6)\n",
      "y_train.shape (666,)\n",
      "x_val.shape (40, 10, 6)\n",
      "y_val.shape (40,)\n",
      "x_test.shape (40, 10, 6)\n",
      "y_test.shape (40,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "split = 0.69  # Adjust to allocate space for validation set\n",
    "val_split = 0.16  # 15% for validation, and implicitly 15% for test due to remaining percentage\n",
    "sequence_length = 10\n",
    "normalise = True\n",
    "batch_size = 100\n",
    "input_dim = 7\n",
    "input_timesteps = 9\n",
    "neurons = 50\n",
    "epochs = 5\n",
    "prediction_len = 1\n",
    "dense_output = 1\n",
    "drop_out = 0\n",
    "\n",
    "#filepath = 'data/original_dataset/source_price.csv'\n",
    "filepath = 'data/original_dataset/amzn_source_price.csv'\n",
    "\n",
    "# This approach does not normalize the compound scores in the range of 0 to 1\n",
    "def get_data_old(filepath: str):\n",
    "  # Load data, modify cols whenever necessary\n",
    "  dataframe = pd.read_csv(filepath)\n",
    "\n",
    "  #cols = ['Adj Close', 'wsj_mean_compound', 'cnbc_mean_compound', 'fortune_mean_compound', 'reuters_mean_compound']\n",
    "  cols = ['Adj Close', 'mean_compound_reuters', 'mean_compound_guardian', 'mean_compound_cnbc', 'mean_compound_other', 'mean_compound_stocktwits', 'mean_compound_twitter']\n",
    "\n",
    "  len_dataframe = dataframe.shape[0]\n",
    "\n",
    "  # Split data into train, validation, and test\n",
    "  i_split = int(len(dataframe) * split)\n",
    "  i_val = int(len(dataframe) * (split + val_split))\n",
    "\n",
    "  data_train = dataframe.get(cols).values[:i_split]\n",
    "  data_val = dataframe.get(cols).values[i_split:i_val]\n",
    "  data_test = dataframe.get(cols).values[i_val:]\n",
    "\n",
    "  # print(data_train[0:5,0])\n",
    "\n",
    "  len_train = len(data_train)\n",
    "  len_val = len(data_val)\n",
    "  len_test = len(data_test)\n",
    "  len_train_windows = None\n",
    "\n",
    "  # Process train data\n",
    "  data_windows = []\n",
    "  for i in range(len_train - sequence_length):\n",
    "      data_windows.append(data_train[i:i+sequence_length])\n",
    "  data_windows = np.array(data_windows).astype(float)\n",
    "  window_data = data_windows\n",
    "  win_num = window_data.shape[0]\n",
    "  col_num = window_data.shape[2]\n",
    "  normalised_data = []\n",
    "  record_min = []\n",
    "  record_max = []\n",
    "\n",
    "  # Normalize train data\n",
    "  for win_i in range(win_num):\n",
    "      normalised_window = []\n",
    "      for col_i in range(0,1):\n",
    "        temp_col = window_data[win_i,:,col_i]\n",
    "        temp_min = min(temp_col)\n",
    "        record_min.append(temp_min)\n",
    "        temp_col = temp_col - temp_min\n",
    "        temp_max = max(temp_col)\n",
    "        record_max.append(temp_max)\n",
    "        temp_col = temp_col / temp_max\n",
    "        normalised_window.append(temp_col)\n",
    "      for col_i in range(1,col_num):\n",
    "        normalised_window.append(window_data[win_i,:,col_i])\n",
    "      normalised_window = np.array(normalised_window).T\n",
    "      normalised_data.append(normalised_window)\n",
    "  normalised_data = np.array(normalised_data)\n",
    "  x_train = normalised_data[:, :-1]\n",
    "  # Classification problem now\n",
    "  y_train = []\n",
    "  for i in range(len_train - sequence_length):\n",
    "      current_last = data_train[i+sequence_length-1, 0]\n",
    "      next_first = data_train[i+sequence_length, 0]\n",
    "      y_train.append(1 if next_first > current_last else 0)\n",
    "  y_train = np.array(y_train)\n",
    "\n",
    "  # Process validation data\n",
    "  data_windows = []\n",
    "  for i in range(len_val - sequence_length):\n",
    "      data_windows.append(data_val[i:i+sequence_length])\n",
    "  data_windows = np.array(data_windows).astype(float)\n",
    "  window_data = data_windows\n",
    "  win_num = window_data.shape[0]\n",
    "  normalised_data = []\n",
    "\n",
    "  # Normalize validation data\n",
    "  for win_i in range(win_num):\n",
    "      normalised_window = []\n",
    "      for col_i in range(0,1):\n",
    "        temp_col = window_data[win_i,:,col_i]\n",
    "        temp_min = min(temp_col)\n",
    "        temp_col = temp_col - temp_min\n",
    "        temp_max = max(temp_col)\n",
    "        temp_col = temp_col / temp_max\n",
    "        normalised_window.append(temp_col)\n",
    "      for col_i in range(1,col_num):\n",
    "        normalised_window.append(window_data[win_i,:,col_i])\n",
    "      normalised_window = np.array(normalised_window).T\n",
    "      normalised_data.append(normalised_window)\n",
    "  normalised_data = np.array(normalised_data)\n",
    "  x_val = normalised_data[:, :-1]\n",
    "  y_val = []\n",
    "  for i in range(len_val - sequence_length):\n",
    "      current_last = data_val[i+sequence_length-1, 0]\n",
    "      next_first = data_val[i+sequence_length, 0]\n",
    "      y_val.append(1 if next_first > current_last else 0)\n",
    "  y_val = np.array(y_val)\n",
    "\n",
    "  # Process test data\n",
    "  data_windows = []\n",
    "  for i in range(len_test - sequence_length):\n",
    "      data_windows.append(data_test[i:i+sequence_length])\n",
    "  data_windows = np.array(data_windows).astype(float)\n",
    "  y_test_ori = data_windows[:, -1, [0]]\n",
    "  window_data = data_windows\n",
    "  win_num = window_data.shape[0]\n",
    "  normalised_data = []\n",
    "\n",
    "  # Normalize test data\n",
    "  for win_i in range(win_num):\n",
    "      normalised_window = []\n",
    "      for col_i in range(0,1):\n",
    "        temp_col = window_data[win_i,:,col_i]\n",
    "        temp_min = min(temp_col)\n",
    "        temp_col = temp_col - temp_min\n",
    "        temp_max = max(temp_col)\n",
    "        temp_col = temp_col / temp_max\n",
    "        normalised_window.append(temp_col)\n",
    "      for col_i in range(1,col_num):\n",
    "        normalised_window.append(window_data[win_i,:,col_i])\n",
    "      normalised_window = np.array(normalised_window).T\n",
    "      normalised_data.append(normalised_window)\n",
    "  normalised_data = np.array(normalised_data)\n",
    "  x_test = normalised_data[:, :-1]\n",
    "  y_test = []\n",
    "  for i in range(len_test - sequence_length):\n",
    "      current_last = data_test[i+sequence_length-1, 0]\n",
    "      next_first = data_test[i+sequence_length, 0]\n",
    "      y_test.append(1 if next_first > current_last else 0)\n",
    "  y_test = np.array(y_test)\n",
    "  return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "def get_data_normal(filepath: str):\n",
    "  df = pd.read_csv(filepath)\n",
    "  # Partition data into training, validation and test sets. Training data should be from date 12/07/2017 to 04/09/2018, validation data (from 04/10/2018 to 05/04/2018), and test data (from 05/07/2018 to 06/01/2018)\n",
    "\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "  # Count entries in 'date' column\n",
    "  num_entries = df['date'].count()\n",
    "\n",
    "  # Calculate indices for train, validation, and test splits\n",
    "  num_train = int(0.7 * num_entries)\n",
    "  num_val = int(0.15 * num_entries)\n",
    "  num_test = num_entries - num_train - num_val  # Ensuring all rows are included\n",
    "\n",
    "  # Split the data\n",
    "  df_train = df.loc[:num_train - 1]\n",
    "  df_val = df.loc[num_train:num_train + num_val - 1]\n",
    "  df_test = df.loc[num_train + num_val:]\n",
    "\n",
    "  # Hardcodidly extracting the exact dates for the partitioning\n",
    "  # '''df_train = df.loc[0:82]\n",
    "  # df_val = df.loc[83:101]\n",
    "  # df_test = df.loc[102:]'''\n",
    "\n",
    "  # print(df_val.head())\n",
    "  # print(df_val.tail())\n",
    "  # print(df_test.head())\n",
    "  # print(df_val)\n",
    "\n",
    "  df_train = df_train.drop(columns=['date'])\n",
    "  df_val = df_val.drop(columns=['date'])\n",
    "  df_test = df_test.drop(columns=['date'])\n",
    "\n",
    "  sc = MinMaxScaler(feature_range=(0,1))\n",
    "  print(\"DF TRAIN\", df_train.head())\n",
    "  df_train = sc.fit_transform(df_train)\n",
    "  df_val = sc.transform(df_val)\n",
    "  df_test = sc.transform(df_test)\n",
    "\n",
    "  def create_sequences_numpy_classification(data, n_days):\n",
    "      X, y = [], []\n",
    "      for i in range(n_days, len(data) - 1): \n",
    "          # print(\"X\")\n",
    "          X.append(data[i-n_days:i])\n",
    "          # print(data[i-n_days:i])\n",
    "          y.append(1 if data[i][-1] - data[i-1][-1] > 0 else 0) #Classification task\n",
    "          # print(data[i][-1], data[i-1][-1])\n",
    "          # print(\"Y\")\n",
    "          # print(y[-1])\n",
    "          \n",
    "      # Delete the first column of X\n",
    "      # X = np.delete(X, 0, axis=2)\n",
    "      return np.array(X), np.array(y)\n",
    "\n",
    "  x_train, y_train = create_sequences_numpy_classification(df_train, 10)\n",
    "  x_val, y_val = create_sequences_numpy_classification(df_val, 10)\n",
    "  x_test, y_test = create_sequences_numpy_classification(df_test, 10)\n",
    "  return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "def get_data_stanford():\n",
    "    DATA_PATH = join('data', 'original_dataset', 'cleaned_tesla_data.csv')\n",
    "\n",
    "    HORIZON = 10\n",
    "    DAYS_FORWARD = 1\n",
    "    END_SPLIT = 40\n",
    "\n",
    "\n",
    "    # split_y: tuple\n",
    "    #     (train_y, validate_y, test_y)\n",
    "    # split_X: tuple\n",
    "    #     (train_X, validate_X, test_X)\n",
    "    split_y, split_X = data_prep.data_prep(DATA_PATH, HORIZON, DAYS_FORWARD, END_SPLIT)\n",
    "    return split_X, split_y\n",
    "\n",
    "# x_train, y_train, x_val, y_val, x_test, y_test = get_data_normal(filepath)\n",
    "\n",
    "(x_train, x_val, x_test), (y_train, y_val, y_test) = get_data_stanford()\n",
    "\n",
    "# print(\"train X\", split_X[0].shape)\n",
    "# print(\"val X\", split_X[1].shape)\n",
    "# print(\"test X\", split_X[2].shape)\n",
    "# print(\"train y\", split_y[0].shape)\n",
    "# print(\"val y\", split_y[1].shape)\n",
    "# print(\"test y\", split_y[2].shape)\n",
    "# print(\"NEW\")\n",
    "\n",
    "print('x_train.shape', x_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('x_val.shape', x_val.shape)\n",
    "print('y_val.shape', y_val.shape)\n",
    "print('x_test.shape', x_test.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# print(x_train[0])\n",
    "\n",
    "# print(y_train[0:5])\n",
    "\n",
    "# print(x_val[0])\n",
    "\n",
    "# print(y_val[0])\n",
    "\n",
    "# print(x_test[0])\n",
    "\n",
    "# print(y_test[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instatiate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Instantiate models\n",
    "lstm_model = LSTMModel()\n",
    "gru_model = GRUModel()\n",
    "meta_model = MetaLearner()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "lstm_optimizer = optim.RMSprop(lstm_model.parameters(), lr=0.0008) # 16 batch size, 150 epochs\n",
    "gru_optimizer = optim.RMSprop(gru_model.parameters(), lr=0.0008) # 16 batch size, 200 epochs\n",
    "base_models_batch_size = 16\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=0.001) # 100 epochs, 8 batch size\n",
    "meta_learner_batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([666, 10, 6])\n",
      "torch.Size([666])\n",
      "Training LSTM Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Loss: 0.6925367911656698\n",
      "Epoch 2/150, Loss: 0.6896093218099504\n",
      "Epoch 3/150, Loss: 0.6927688320477804\n",
      "Epoch 4/150, Loss: 0.6925106531097776\n",
      "Epoch 5/150, Loss: 0.693042591923759\n",
      "Epoch 6/150, Loss: 0.6917174756526947\n",
      "Epoch 7/150, Loss: 0.6936698172773633\n",
      "Epoch 8/150, Loss: 0.6914670836357844\n",
      "Epoch 9/150, Loss: 0.6930848033655257\n",
      "Epoch 10/150, Loss: 0.6928743053050268\n",
      "Epoch 11/150, Loss: 0.6920885528836932\n",
      "Epoch 12/150, Loss: 0.6916447764351255\n",
      "Epoch 13/150, Loss: 0.6930122063273475\n",
      "Epoch 14/150, Loss: 0.6933688450427282\n",
      "Epoch 15/150, Loss: 0.691185264360337\n",
      "Epoch 16/150, Loss: 0.6923364358288902\n",
      "Epoch 17/150, Loss: 0.6917492293176197\n",
      "Epoch 18/150, Loss: 0.6926387094316029\n",
      "Epoch 19/150, Loss: 0.6920432987667265\n",
      "Epoch 20/150, Loss: 0.6926585379100981\n",
      "Epoch 21/150, Loss: 0.6912643285024733\n",
      "Epoch 22/150, Loss: 0.6925120367890313\n",
      "Epoch 23/150, Loss: 0.690949349176316\n",
      "Epoch 24/150, Loss: 0.6913746623765855\n",
      "Epoch 25/150, Loss: 0.6925936738650004\n",
      "Epoch 26/150, Loss: 0.6921940346558889\n",
      "Epoch 27/150, Loss: 0.6928385964461735\n",
      "Epoch 28/150, Loss: 0.6932365539528075\n",
      "Epoch 29/150, Loss: 0.6910313027245658\n",
      "Epoch 30/150, Loss: 0.6917232927821931\n",
      "Epoch 31/150, Loss: 0.6912154129573277\n",
      "Epoch 32/150, Loss: 0.6924361912977128\n",
      "Epoch 33/150, Loss: 0.6928682114396777\n",
      "Epoch 34/150, Loss: 0.6924073752902803\n",
      "Epoch 35/150, Loss: 0.6908371931030637\n",
      "Epoch 36/150, Loss: 0.6921518771421342\n",
      "Epoch 37/150, Loss: 0.6925103806314015\n",
      "Epoch 38/150, Loss: 0.6913313070933024\n",
      "Epoch 39/150, Loss: 0.6912756462891897\n",
      "Epoch 40/150, Loss: 0.691696313165483\n",
      "Epoch 41/150, Loss: 0.6916813041482653\n",
      "Epoch 42/150, Loss: 0.6920533719516936\n",
      "Epoch 43/150, Loss: 0.6907625482195899\n",
      "Epoch 44/150, Loss: 0.6916084729489826\n",
      "Epoch 45/150, Loss: 0.692583075591496\n",
      "Epoch 46/150, Loss: 0.6917829371633983\n",
      "Epoch 47/150, Loss: 0.6925720771153768\n",
      "Epoch 48/150, Loss: 0.691070807831628\n",
      "Epoch 49/150, Loss: 0.6914412109624772\n",
      "Epoch 50/150, Loss: 0.6915261135214851\n",
      "Epoch 51/150, Loss: 0.6908756622246334\n",
      "Epoch 52/150, Loss: 0.6914226795945849\n",
      "Epoch 53/150, Loss: 0.6911463382698241\n",
      "Epoch 54/150, Loss: 0.691315294731231\n",
      "Epoch 55/150, Loss: 0.6906197014309111\n",
      "Epoch 56/150, Loss: 0.6927173123473213\n",
      "Epoch 57/150, Loss: 0.6901373338131678\n",
      "Epoch 58/150, Loss: 0.6921937394709814\n",
      "Epoch 59/150, Loss: 0.6898659368356069\n",
      "Epoch 60/150, Loss: 0.6922077721073514\n",
      "Epoch 61/150, Loss: 0.6918927402723403\n",
      "Epoch 62/150, Loss: 0.6872208203588214\n",
      "Epoch 63/150, Loss: 0.6915301368350074\n",
      "Epoch 64/150, Loss: 0.6897253337360564\n",
      "Epoch 65/150, Loss: 0.6912861296108791\n",
      "Epoch 66/150, Loss: 0.6904383443650746\n",
      "Epoch 67/150, Loss: 0.6909243237404596\n",
      "Epoch 68/150, Loss: 0.689183615502857\n",
      "Epoch 69/150, Loss: 0.6896469536281767\n",
      "Epoch 70/150, Loss: 0.6901538627488273\n",
      "Epoch 71/150, Loss: 0.6898739792051769\n",
      "Epoch 72/150, Loss: 0.691182575055531\n",
      "Epoch 73/150, Loss: 0.6913638569059826\n",
      "Epoch 74/150, Loss: 0.6905957403637114\n",
      "Epoch 75/150, Loss: 0.6906169695513589\n",
      "Epoch 76/150, Loss: 0.6895567192917779\n",
      "Epoch 77/150, Loss: 0.6902498844124022\n",
      "Epoch 78/150, Loss: 0.6913717644555228\n",
      "Epoch 79/150, Loss: 0.6891215443611145\n",
      "Epoch 80/150, Loss: 0.6887114189919972\n",
      "Epoch 81/150, Loss: 0.6889132559299469\n",
      "Epoch 82/150, Loss: 0.6890635958739689\n",
      "Epoch 83/150, Loss: 0.6896455316316514\n",
      "Epoch 84/150, Loss: 0.69187464316686\n",
      "Epoch 85/150, Loss: 0.6891611700966245\n",
      "Epoch 86/150, Loss: 0.6912350072747185\n",
      "Epoch 87/150, Loss: 0.6891762032395318\n",
      "Epoch 88/150, Loss: 0.6902406825905755\n",
      "Epoch 89/150, Loss: 0.686435056584222\n",
      "Epoch 90/150, Loss: 0.6906477709611257\n",
      "Epoch 91/150, Loss: 0.6894859671592712\n",
      "Epoch 92/150, Loss: 0.691274691195715\n",
      "Epoch 93/150, Loss: 0.6890936635789418\n",
      "Epoch 94/150, Loss: 0.6894921660423279\n",
      "Epoch 95/150, Loss: 0.6905124669983274\n",
      "Epoch 96/150, Loss: 0.6891220240365892\n",
      "Epoch 97/150, Loss: 0.6909035075278509\n",
      "Epoch 98/150, Loss: 0.6907731152716137\n",
      "Epoch 99/150, Loss: 0.6894462151186806\n",
      "Epoch 100/150, Loss: 0.6883281597069332\n",
      "Epoch 101/150, Loss: 0.6908068926561446\n",
      "Epoch 102/150, Loss: 0.688221404949824\n",
      "Epoch 103/150, Loss: 0.6911743751593998\n",
      "Epoch 104/150, Loss: 0.6901064401581174\n",
      "Epoch 105/150, Loss: 0.6899256152766091\n",
      "Epoch 106/150, Loss: 0.6869269396577563\n",
      "Epoch 107/150, Loss: 0.6873166830766768\n",
      "Epoch 108/150, Loss: 0.688157917488189\n",
      "Epoch 109/150, Loss: 0.6880383505707696\n",
      "Epoch 110/150, Loss: 0.6870965929258437\n",
      "Epoch 111/150, Loss: 0.6875424910159338\n",
      "Epoch 112/150, Loss: 0.6885894026075091\n",
      "Epoch 113/150, Loss: 0.6876146168935866\n",
      "Epoch 114/150, Loss: 0.689439207315445\n",
      "Epoch 115/150, Loss: 0.6888176671096257\n",
      "Epoch 116/150, Loss: 0.6881545909813472\n",
      "Epoch 117/150, Loss: 0.6877876662072682\n",
      "Epoch 118/150, Loss: 0.6905014330432528\n",
      "Epoch 119/150, Loss: 0.6887228857903254\n",
      "Epoch 120/150, Loss: 0.6877950841472262\n",
      "Epoch 121/150, Loss: 0.687990780387606\n",
      "Epoch 122/150, Loss: 0.6888619462649027\n",
      "Epoch 123/150, Loss: 0.6898535518419175\n",
      "Epoch 124/150, Loss: 0.6868542858532497\n",
      "Epoch 125/150, Loss: 0.6897786004202706\n",
      "Epoch 126/150, Loss: 0.6891801868166242\n",
      "Epoch 127/150, Loss: 0.6875567861965725\n",
      "Epoch 128/150, Loss: 0.6866967181364695\n",
      "Epoch 129/150, Loss: 0.6880401841231755\n",
      "Epoch 130/150, Loss: 0.6883747080961863\n",
      "Epoch 131/150, Loss: 0.6888516133739835\n",
      "Epoch 132/150, Loss: 0.6885653010436467\n",
      "Epoch 133/150, Loss: 0.6866224848088764\n",
      "Epoch 134/150, Loss: 0.6884020467599233\n",
      "Epoch 135/150, Loss: 0.689257644471668\n",
      "Epoch 136/150, Loss: 0.6885921018464225\n",
      "Epoch 137/150, Loss: 0.6882165911651793\n",
      "Epoch 138/150, Loss: 0.6873288608732677\n",
      "Epoch 139/150, Loss: 0.6871732019242787\n",
      "Epoch 140/150, Loss: 0.6870988678364527\n",
      "Epoch 141/150, Loss: 0.6865770901952472\n",
      "Epoch 142/150, Loss: 0.6898913922764006\n",
      "Epoch 143/150, Loss: 0.6853184458755311\n",
      "Epoch 144/150, Loss: 0.6865099810418629\n",
      "Epoch 145/150, Loss: 0.6883142604714348\n",
      "Epoch 146/150, Loss: 0.6889260496412005\n",
      "Epoch 147/150, Loss: 0.6883582586333865\n",
      "Epoch 148/150, Loss: 0.6872245584215436\n",
      "Epoch 149/150, Loss: 0.688795533918199\n",
      "Epoch 150/150, Loss: 0.6860315657797313\n",
      "Training GRU Model\n",
      "Epoch 1/200, Loss: 0.6950249813851856\n",
      "Epoch 2/200, Loss: 0.6927876600197383\n",
      "Epoch 3/200, Loss: 0.6927236460504078\n",
      "Epoch 4/200, Loss: 0.6918212515967233\n",
      "Epoch 5/200, Loss: 0.6931543634051368\n",
      "Epoch 6/200, Loss: 0.6923054201262338\n",
      "Epoch 7/200, Loss: 0.6910169607117063\n",
      "Epoch 8/200, Loss: 0.6915229672477359\n",
      "Epoch 9/200, Loss: 0.6925784973871141\n",
      "Epoch 10/200, Loss: 0.692297963869004\n",
      "Epoch 11/200, Loss: 0.6925121886389596\n",
      "Epoch 12/200, Loss: 0.6916238097917466\n",
      "Epoch 13/200, Loss: 0.6914030129001254\n",
      "Epoch 14/200, Loss: 0.6909628098919278\n",
      "Epoch 15/200, Loss: 0.6912724489257449\n",
      "Epoch 16/200, Loss: 0.6912121020612263\n",
      "Epoch 17/200, Loss: 0.6914093309924716\n",
      "Epoch 18/200, Loss: 0.6912369784854707\n",
      "Epoch 19/200, Loss: 0.6919953879855928\n",
      "Epoch 20/200, Loss: 0.6929851529144105\n",
      "Epoch 21/200, Loss: 0.6921216362998599\n",
      "Epoch 22/200, Loss: 0.6917586340790703\n",
      "Epoch 23/200, Loss: 0.6913792874131884\n",
      "Epoch 24/200, Loss: 0.6916646403925759\n",
      "Epoch 25/200, Loss: 0.69277290503184\n",
      "Epoch 26/200, Loss: 0.691237136012032\n",
      "Epoch 27/200, Loss: 0.6921508042585283\n",
      "Epoch 28/200, Loss: 0.6923320123127529\n",
      "Epoch 29/200, Loss: 0.6913699266456422\n",
      "Epoch 30/200, Loss: 0.6920859813690186\n",
      "Epoch 31/200, Loss: 0.6904063040301913\n",
      "Epoch 32/200, Loss: 0.6912969592071715\n",
      "Epoch 33/200, Loss: 0.6915721070198786\n",
      "Epoch 34/200, Loss: 0.6914211681910923\n",
      "Epoch 35/200, Loss: 0.6902549905436379\n",
      "Epoch 36/200, Loss: 0.6908055998030163\n",
      "Epoch 37/200, Loss: 0.6922539288089389\n",
      "Epoch 38/200, Loss: 0.6919867069948287\n",
      "Epoch 39/200, Loss: 0.6915215225446791\n",
      "Epoch 40/200, Loss: 0.6904722040607816\n",
      "Epoch 41/200, Loss: 0.691241443157196\n",
      "Epoch 42/200, Loss: 0.6910497006915864\n",
      "Epoch 43/200, Loss: 0.6922938625017802\n",
      "Epoch 44/200, Loss: 0.691314473038628\n",
      "Epoch 45/200, Loss: 0.6910846999713353\n",
      "Epoch 46/200, Loss: 0.6907655568349929\n",
      "Epoch 47/200, Loss: 0.6915867697624933\n",
      "Epoch 48/200, Loss: 0.6915897201924097\n",
      "Epoch 49/200, Loss: 0.6912684738636017\n",
      "Epoch 50/200, Loss: 0.6925641496976217\n",
      "Epoch 51/200, Loss: 0.6912001655215309\n",
      "Epoch 52/200, Loss: 0.6912774855182284\n",
      "Epoch 53/200, Loss: 0.6920101401351747\n",
      "Epoch 54/200, Loss: 0.6912213067213694\n",
      "Epoch 55/200, Loss: 0.691930951107116\n",
      "Epoch 56/200, Loss: 0.6913577545256842\n",
      "Epoch 57/200, Loss: 0.6915019807361421\n",
      "Epoch 58/200, Loss: 0.6901419872329349\n",
      "Epoch 59/200, Loss: 0.6909511685371399\n",
      "Epoch 60/200, Loss: 0.6911278608299437\n",
      "Epoch 61/200, Loss: 0.6927299925259182\n",
      "Epoch 62/200, Loss: 0.6916440512452807\n",
      "Epoch 63/200, Loss: 0.6909510833876473\n",
      "Epoch 64/200, Loss: 0.6907587349414825\n",
      "Epoch 65/200, Loss: 0.6914672454198202\n",
      "Epoch 66/200, Loss: 0.6902507174582708\n",
      "Epoch 67/200, Loss: 0.6920579217729115\n",
      "Epoch 68/200, Loss: 0.6918364522002992\n",
      "Epoch 69/200, Loss: 0.6916726884387788\n",
      "Epoch 70/200, Loss: 0.6914238816215879\n",
      "Epoch 71/200, Loss: 0.6904786002068293\n",
      "Epoch 72/200, Loss: 0.6913480474835351\n",
      "Epoch 73/200, Loss: 0.6910343553338733\n",
      "Epoch 74/200, Loss: 0.6919481413705009\n",
      "Epoch 75/200, Loss: 0.6907018564996266\n",
      "Epoch 76/200, Loss: 0.6909627062933785\n",
      "Epoch 77/200, Loss: 0.6916848903610593\n",
      "Epoch 78/200, Loss: 0.6923428362324124\n",
      "Epoch 79/200, Loss: 0.6919317472548712\n",
      "Epoch 80/200, Loss: 0.6904967838809604\n",
      "Epoch 81/200, Loss: 0.6917045485405695\n",
      "Epoch 82/200, Loss: 0.6899020160947528\n",
      "Epoch 83/200, Loss: 0.690835945662998\n",
      "Epoch 84/200, Loss: 0.6909841142949604\n",
      "Epoch 85/200, Loss: 0.6910859914053054\n",
      "Epoch 86/200, Loss: 0.6914422852652413\n",
      "Epoch 87/200, Loss: 0.6912367684500558\n",
      "Epoch 88/200, Loss: 0.6889104488350096\n",
      "Epoch 89/200, Loss: 0.6907419079825992\n",
      "Epoch 90/200, Loss: 0.688676811399914\n",
      "Epoch 91/200, Loss: 0.6922396123409271\n",
      "Epoch 92/200, Loss: 0.690294379279727\n",
      "Epoch 93/200, Loss: 0.6893961670852843\n",
      "Epoch 94/200, Loss: 0.6921922096184322\n",
      "Epoch 95/200, Loss: 0.6906846321764446\n",
      "Epoch 96/200, Loss: 0.688561886548996\n",
      "Epoch 97/200, Loss: 0.6914921502272288\n",
      "Epoch 98/200, Loss: 0.6891825525533586\n",
      "Epoch 99/200, Loss: 0.6898346756185804\n",
      "Epoch 100/200, Loss: 0.6910868414810726\n",
      "Epoch 101/200, Loss: 0.6898828333332425\n",
      "Epoch 102/200, Loss: 0.6907407187280201\n",
      "Epoch 103/200, Loss: 0.6894497913973672\n",
      "Epoch 104/200, Loss: 0.6917049969945636\n",
      "Epoch 105/200, Loss: 0.6899138121377855\n",
      "Epoch 106/200, Loss: 0.6895540356636047\n",
      "Epoch 107/200, Loss: 0.6895469753515153\n",
      "Epoch 108/200, Loss: 0.6897945162795839\n",
      "Epoch 109/200, Loss: 0.6905494005907149\n",
      "Epoch 110/200, Loss: 0.689209847223191\n",
      "Epoch 111/200, Loss: 0.6902308520816621\n",
      "Epoch 112/200, Loss: 0.6896975480374836\n",
      "Epoch 113/200, Loss: 0.6892759672233036\n",
      "Epoch 114/200, Loss: 0.6896189749240875\n",
      "Epoch 115/200, Loss: 0.6898380588917505\n",
      "Epoch 116/200, Loss: 0.6876020431518555\n",
      "Epoch 117/200, Loss: 0.6888706144832429\n",
      "Epoch 118/200, Loss: 0.6888751501128787\n",
      "Epoch 119/200, Loss: 0.6890146093709129\n",
      "Epoch 120/200, Loss: 0.6881225648380461\n",
      "Epoch 121/200, Loss: 0.6900108797209603\n",
      "Epoch 122/200, Loss: 0.6913741372880482\n",
      "Epoch 123/200, Loss: 0.6889336378801436\n",
      "Epoch 124/200, Loss: 0.6883260621911004\n",
      "Epoch 125/200, Loss: 0.6881678586914426\n",
      "Epoch 126/200, Loss: 0.6900765739736103\n",
      "Epoch 127/200, Loss: 0.6902734694026765\n",
      "Epoch 128/200, Loss: 0.6894660208906446\n",
      "Epoch 129/200, Loss: 0.6917122872102828\n",
      "Epoch 130/200, Loss: 0.6893501920359475\n",
      "Epoch 131/200, Loss: 0.6891773045063019\n",
      "Epoch 132/200, Loss: 0.688193203437896\n",
      "Epoch 133/200, Loss: 0.6896736607665107\n",
      "Epoch 134/200, Loss: 0.6887682733081636\n",
      "Epoch 135/200, Loss: 0.6880357577687218\n",
      "Epoch 136/200, Loss: 0.6879708142507643\n",
      "Epoch 137/200, Loss: 0.689765609446026\n",
      "Epoch 138/200, Loss: 0.6893081466356913\n",
      "Epoch 139/200, Loss: 0.6895857368196759\n",
      "Epoch 140/200, Loss: 0.6896768850939614\n",
      "Epoch 141/200, Loss: 0.6887686380318233\n",
      "Epoch 142/200, Loss: 0.6894305873484838\n",
      "Epoch 143/200, Loss: 0.6886619684242067\n",
      "Epoch 144/200, Loss: 0.6877454590229761\n",
      "Epoch 145/200, Loss: 0.6885689482802436\n",
      "Epoch 146/200, Loss: 0.6869862831774212\n",
      "Epoch 147/200, Loss: 0.688938612029666\n",
      "Epoch 148/200, Loss: 0.6873651913234166\n",
      "Epoch 149/200, Loss: 0.6890479851336706\n",
      "Epoch 150/200, Loss: 0.6875819592248826\n",
      "Epoch 151/200, Loss: 0.6907000882284982\n",
      "Epoch 152/200, Loss: 0.6879851661977314\n",
      "Epoch 153/200, Loss: 0.6873965589773088\n",
      "Epoch 154/200, Loss: 0.6871351315861657\n",
      "Epoch 155/200, Loss: 0.6878953349022638\n",
      "Epoch 156/200, Loss: 0.6882873603275844\n",
      "Epoch 157/200, Loss: 0.6876947993323916\n",
      "Epoch 158/200, Loss: 0.6859719114644187\n",
      "Epoch 159/200, Loss: 0.6878417815480914\n",
      "Epoch 160/200, Loss: 0.6874847468875703\n",
      "Epoch 161/200, Loss: 0.6841197467985607\n",
      "Epoch 162/200, Loss: 0.6885078421660832\n",
      "Epoch 163/200, Loss: 0.6836117264770326\n",
      "Epoch 164/200, Loss: 0.6873505924429212\n",
      "Epoch 165/200, Loss: 0.6851234180586678\n",
      "Epoch 166/200, Loss: 0.685895677123751\n",
      "Epoch 167/200, Loss: 0.6868589704944974\n",
      "Epoch 168/200, Loss: 0.6861742408502669\n",
      "Epoch 169/200, Loss: 0.6832023192019689\n",
      "Epoch 170/200, Loss: 0.6867184071313768\n",
      "Epoch 171/200, Loss: 0.685794255563191\n",
      "Epoch 172/200, Loss: 0.6870718059085664\n",
      "Epoch 173/200, Loss: 0.6849686631134578\n",
      "Epoch 174/200, Loss: 0.6820827637399945\n",
      "Epoch 175/200, Loss: 0.6868951419989268\n",
      "Epoch 176/200, Loss: 0.6831077763012477\n",
      "Epoch 177/200, Loss: 0.6879785458246866\n",
      "Epoch 178/200, Loss: 0.6839095112823305\n",
      "Epoch 179/200, Loss: 0.681294607264655\n",
      "Epoch 180/200, Loss: 0.6816898740473247\n",
      "Epoch 181/200, Loss: 0.6830671997297377\n",
      "Epoch 182/200, Loss: 0.683060279914311\n",
      "Epoch 183/200, Loss: 0.6819220242046174\n",
      "Epoch 184/200, Loss: 0.6857688242480868\n",
      "Epoch 185/200, Loss: 0.6793883031322843\n",
      "Epoch 186/200, Loss: 0.6821398649896894\n",
      "Epoch 187/200, Loss: 0.6838440398375193\n",
      "Epoch 188/200, Loss: 0.6808271677721114\n",
      "Epoch 189/200, Loss: 0.6825300327369145\n",
      "Epoch 190/200, Loss: 0.6835314404396784\n",
      "Epoch 191/200, Loss: 0.6821130116780599\n",
      "Epoch 192/200, Loss: 0.6803835956823259\n",
      "Epoch 193/200, Loss: 0.6824412374269395\n",
      "Epoch 194/200, Loss: 0.6816898385683695\n",
      "Epoch 195/200, Loss: 0.6825990875562032\n",
      "Epoch 196/200, Loss: 0.6812548069726854\n",
      "Epoch 197/200, Loss: 0.678952523640224\n",
      "Epoch 198/200, Loss: 0.6772587441262745\n",
      "Epoch 199/200, Loss: 0.6811883208297548\n",
      "Epoch 200/200, Loss: 0.6788699697880518\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors and create DataLoader\n",
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=base_models_batch_size, shuffle=True) #Stanford had shuffle true\n",
    "\n",
    "# Training function\n",
    "def train_model(model, optimizer, criterion, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "# Train the LSTM model\n",
    "print(\"Training LSTM Model\")\n",
    "train_model(lstm_model, lstm_optimizer, criterion, train_loader, 150)\n",
    "\n",
    "# Train the GRU model\n",
    "print(\"Training GRU Model\")\n",
    "train_model(gru_model, gru_optimizer, criterion, train_loader, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use base models to predict the validation data, this will be used as input to the Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2)\n",
      "[[0.516736   0.46189052]\n",
      " [0.52425104 0.48631704]\n",
      " [0.52409965 0.50474036]\n",
      " [0.535297   0.49517587]\n",
      " [0.547378   0.52963537]\n",
      " [0.5369737  0.5083322 ]\n",
      " [0.5239233  0.49191448]\n",
      " [0.47097984 0.50310606]\n",
      " [0.47814897 0.48590773]\n",
      " [0.49187255 0.47363514]\n",
      " [0.4468163  0.43271756]\n",
      " [0.44914964 0.47728932]\n",
      " [0.46036085 0.49529105]\n",
      " [0.44215804 0.503229  ]\n",
      " [0.46840578 0.4733225 ]\n",
      " [0.46385634 0.49194252]\n",
      " [0.46563783 0.50629836]\n",
      " [0.49265054 0.4611489 ]\n",
      " [0.49618402 0.4764875 ]\n",
      " [0.46536583 0.43460697]\n",
      " [0.43636358 0.45869818]\n",
      " [0.4727506  0.46687996]\n",
      " [0.46085545 0.46997508]\n",
      " [0.45532218 0.48353973]\n",
      " [0.45749515 0.44683164]\n",
      " [0.48587045 0.46103504]\n",
      " [0.47340906 0.43495625]\n",
      " [0.47155273 0.40900496]\n",
      " [0.48620322 0.39225423]\n",
      " [0.44880024 0.43028203]\n",
      " [0.4385189  0.46465787]\n",
      " [0.46292764 0.431434  ]\n",
      " [0.4419427  0.3802687 ]\n",
      " [0.47270033 0.48156247]\n",
      " [0.49414676 0.4777984 ]\n",
      " [0.463803   0.4817617 ]\n",
      " [0.44387853 0.46524623]\n",
      " [0.49341908 0.46837053]\n",
      " [0.49157387 0.4932608 ]\n",
      " [0.47571436 0.49785218]]\n",
      "[1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
      "[[[0.51879042 0.01408232 0.52027643 0.5202685  0.52038831 0.58101153]\n",
      "  [0.51374066 0.01799015 0.51763308 0.51803827 0.51557285 0.54005474]\n",
      "  [0.50881654 0.01629193 0.50921804 0.50695348 0.50939476 0.44564706]\n",
      "  ...\n",
      "  [0.54551315 0.06733234 0.53041506 0.54735464 0.5362227  0.36500299]\n",
      "  [0.5643304  0.08075875 0.5539856  0.57173795 0.56110418 0.50310683]\n",
      "  [0.56438899 0.04008126 0.56984282 0.56818116 0.56331301 0.59878397]]\n",
      "\n",
      " [[0.51374066 0.01799015 0.51763308 0.51803827 0.51557285 0.54005474]\n",
      "  [0.50881654 0.01629193 0.50921804 0.50695348 0.50939476 0.44564706]\n",
      "  [0.52074164 0.05268348 0.51306552 0.52303755 0.51769698 0.45386514]\n",
      "  ...\n",
      "  [0.5643304  0.08075875 0.5539856  0.57173795 0.56110418 0.50310683]\n",
      "  [0.56438899 0.04008126 0.56984282 0.56818116 0.56331301 0.59878397]\n",
      "  [0.56537718 0.02442271 0.56316608 0.5700466  0.57001579 0.46789601]]\n",
      "\n",
      " [[0.50881654 0.01629193 0.50921804 0.50695348 0.50939476 0.44564706]\n",
      "  [0.52074164 0.05268348 0.51306552 0.52303755 0.51769698 0.45386514]\n",
      "  [0.50998896 0.0780375  0.52343953 0.52162826 0.50088078 0.51052314]\n",
      "  ...\n",
      "  [0.56438899 0.04008126 0.56984282 0.56818116 0.56331301 0.59878397]\n",
      "  [0.56537718 0.02442271 0.56316608 0.5700466  0.57001579 0.46789601]\n",
      "  [0.56848407 0.01057699 0.56733906 0.56677163 0.57211459 0.46789601]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.59224224 0.04459893 0.5875479  0.58652878 0.56994808 0.41631588]\n",
      "  [0.59349    0.02958468 0.5895142  0.59456253 0.59326392 0.50170374]\n",
      "  [0.60298663 0.01884456 0.59154224 0.59654403 0.59492266 0.83216405]\n",
      "  ...\n",
      "  [0.59972894 0.0180598  0.5903123  0.59389919 0.59556586 0.48186007]\n",
      "  [0.6011526  0.00728051 0.59988815 0.597655   0.60301334 0.47778443]\n",
      "  [0.6185295  0.03924288 0.59228504 0.61141777 0.60016978 0.42794144]]\n",
      "\n",
      " [[0.59349    0.02958468 0.5895142  0.59456253 0.59326392 0.50170374]\n",
      "  [0.60298663 0.01884456 0.59154224 0.59654403 0.59492266 0.83216405]\n",
      "  [0.60395801 0.01398076 0.59807712 0.59824365 0.60275102 0.71630919]\n",
      "  ...\n",
      "  [0.6011526  0.00728051 0.59988815 0.597655   0.60301334 0.47778443]\n",
      "  [0.6185295  0.03924288 0.59228504 0.61141777 0.60016978 0.42794144]\n",
      "  [0.6327408  0.06195497 0.61501098 0.63148153 0.62111592 0.60138971]]\n",
      "\n",
      " [[0.60298663 0.01884456 0.59154224 0.59654403 0.59492266 0.83216405]\n",
      "  [0.60395801 0.01398076 0.59807712 0.59824365 0.60275102 0.71630919]\n",
      "  [0.60605162 0.06240774 0.60168254 0.60000956 0.6047737  0.47090262]\n",
      "  ...\n",
      "  [0.6185295  0.03924288 0.59228504 0.61141777 0.60016978 0.42794144]\n",
      "  [0.6327408  0.06195497 0.61501098 0.63148153 0.62111592 0.60138971]\n",
      "  [0.62118417 0.0528356  0.62676203 0.62869585 0.61846709 0.39747441]]]\n"
     ]
    }
   ],
   "source": [
    "lstm_val_predictions = lstm_model(torch.tensor(x_val, dtype=torch.float32)).detach().numpy().reshape(-1,1)\n",
    "gru_val_predictions = gru_model(torch.tensor(x_val, dtype=torch.float32)).detach().numpy().reshape(-1,1)\n",
    "\n",
    "# lstm_pred = lstm_model.predict(X).reshape(-1, 1)\n",
    "# gru_pred = gru_model.predict(X).reshape(-1, 1)\n",
    "\n",
    "# Form and return new data set\n",
    "# new_X = np.hstack((lstm_pred, gru_pred))\n",
    "\n",
    "\n",
    "# Combine predictions to form new training data for the meta-learner\n",
    "meta_X_train = np.hstack((lstm_val_predictions, gru_val_predictions))#meta_X_train = np.concatenate((lstm_val_predictions, gru_val_predictions), axis=1)\n",
    "\n",
    "print(meta_X_train.shape)\n",
    "\n",
    "print(meta_X_train)\n",
    "\n",
    "print(y_val)\n",
    "\n",
    "print(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6756232619285584\n",
      "Epoch 2/100, Loss: 0.6516303896903992\n",
      "Epoch 3/100, Loss: 0.6331756591796875\n",
      "Epoch 4/100, Loss: 0.6265263080596923\n",
      "Epoch 5/100, Loss: 0.6329083502292633\n",
      "Epoch 6/100, Loss: 0.6360498666763306\n",
      "Epoch 7/100, Loss: 0.6349086105823517\n",
      "Epoch 8/100, Loss: 0.6335639119148254\n",
      "Epoch 9/100, Loss: 0.6326347768306733\n",
      "Epoch 10/100, Loss: 0.6319206595420838\n",
      "Epoch 11/100, Loss: 0.6314817726612091\n",
      "Epoch 12/100, Loss: 0.6314120769500733\n",
      "Epoch 13/100, Loss: 0.6316119968891144\n",
      "Epoch 14/100, Loss: 0.6318620920181275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Loss: 0.6320068717002869\n",
      "Epoch 16/100, Loss: 0.6320198059082032\n",
      "Epoch 17/100, Loss: 0.6319465339183807\n",
      "Epoch 18/100, Loss: 0.6318423330783844\n",
      "Epoch 19/100, Loss: 0.6317469656467438\n",
      "Epoch 20/100, Loss: 0.6316810250282288\n",
      "Epoch 21/100, Loss: 0.6316468834877014\n",
      "Epoch 22/100, Loss: 0.6316346049308776\n",
      "Epoch 23/100, Loss: 0.631629902124405\n",
      "Epoch 24/100, Loss: 0.6316215991973877\n",
      "Epoch 25/100, Loss: 0.6316048085689545\n",
      "Epoch 26/100, Loss: 0.6315743803977967\n",
      "Epoch 27/100, Loss: 0.6315429151058197\n",
      "Epoch 28/100, Loss: 0.6315185368061066\n",
      "Epoch 29/100, Loss: 0.6314856231212616\n",
      "Epoch 30/100, Loss: 0.6314568936824798\n",
      "Epoch 31/100, Loss: 0.6314382433891297\n",
      "Epoch 32/100, Loss: 0.6314112663269043\n",
      "Epoch 33/100, Loss: 0.6313881516456604\n",
      "Epoch 34/100, Loss: 0.6313630223274231\n",
      "Epoch 35/100, Loss: 0.6313347041606903\n",
      "Epoch 36/100, Loss: 0.6313071250915527\n",
      "Epoch 37/100, Loss: 0.6312850594520569\n",
      "Epoch 38/100, Loss: 0.6312565207481384\n",
      "Epoch 39/100, Loss: 0.6312348961830139\n",
      "Epoch 40/100, Loss: 0.6312107920646668\n",
      "Epoch 41/100, Loss: 0.6311801731586456\n",
      "Epoch 42/100, Loss: 0.6311567068099976\n",
      "Epoch 43/100, Loss: 0.6311272025108338\n",
      "Epoch 44/100, Loss: 0.6311045944690704\n",
      "Epoch 45/100, Loss: 0.6310742497444153\n",
      "Epoch 46/100, Loss: 0.6310521721839905\n",
      "Epoch 47/100, Loss: 0.6310264945030213\n",
      "Epoch 48/100, Loss: 0.6309930682182312\n",
      "Epoch 49/100, Loss: 0.6309612691402435\n",
      "Epoch 50/100, Loss: 0.6309368073940277\n",
      "Epoch 51/100, Loss: 0.6309069335460663\n",
      "Epoch 52/100, Loss: 0.6308877289295196\n",
      "Epoch 53/100, Loss: 0.6308602869510651\n",
      "Epoch 54/100, Loss: 0.6308192014694214\n",
      "Epoch 55/100, Loss: 0.6307824492454529\n",
      "Epoch 56/100, Loss: 0.630759733915329\n",
      "Epoch 57/100, Loss: 0.6307337701320648\n",
      "Epoch 58/100, Loss: 0.6306967556476593\n",
      "Epoch 59/100, Loss: 0.6306550443172455\n",
      "Epoch 60/100, Loss: 0.6306160092353821\n",
      "Epoch 61/100, Loss: 0.6305764675140381\n",
      "Epoch 62/100, Loss: 0.6305410921573639\n",
      "Epoch 63/100, Loss: 0.630511748790741\n",
      "Epoch 64/100, Loss: 0.6304776668548584\n",
      "Epoch 65/100, Loss: 0.6304393708705902\n",
      "Epoch 66/100, Loss: 0.6303927361965179\n",
      "Epoch 67/100, Loss: 0.6303529143333435\n",
      "Epoch 68/100, Loss: 0.6303118169307709\n",
      "Epoch 69/100, Loss: 0.6302628755569458\n",
      "Epoch 70/100, Loss: 0.6302221894264222\n",
      "Epoch 71/100, Loss: 0.630178427696228\n",
      "Epoch 72/100, Loss: 0.6301258385181427\n",
      "Epoch 73/100, Loss: 0.6300781667232513\n",
      "Epoch 74/100, Loss: 0.6300061225891114\n",
      "Epoch 75/100, Loss: 0.6299694299697876\n",
      "Epoch 76/100, Loss: 0.6299352586269379\n",
      "Epoch 77/100, Loss: 0.6299035310745239\n",
      "Epoch 78/100, Loss: 0.6298468232154846\n",
      "Epoch 79/100, Loss: 0.6297754645347595\n",
      "Epoch 80/100, Loss: 0.6296852231025696\n",
      "Epoch 81/100, Loss: 0.6296012699604034\n",
      "Epoch 82/100, Loss: 0.629570198059082\n",
      "Epoch 83/100, Loss: 0.6295136451721192\n",
      "Epoch 84/100, Loss: 0.6294531404972077\n",
      "Epoch 85/100, Loss: 0.6293428659439086\n",
      "Epoch 86/100, Loss: 0.629263573884964\n",
      "Epoch 87/100, Loss: 0.6292420744895935\n",
      "Epoch 88/100, Loss: 0.6291792690753937\n",
      "Epoch 89/100, Loss: 0.6290701329708099\n",
      "Epoch 90/100, Loss: 0.6289681792259216\n",
      "Epoch 91/100, Loss: 0.6288610875606537\n",
      "Epoch 92/100, Loss: 0.6284721076488495\n",
      "Epoch 93/100, Loss: 0.628677761554718\n",
      "Epoch 94/100, Loss: 0.6287432670593261\n",
      "Epoch 95/100, Loss: 0.6286247491836547\n",
      "Epoch 96/100, Loss: 0.6284533381462097\n",
      "Epoch 97/100, Loss: 0.6282246947288513\n",
      "Epoch 98/100, Loss: 0.6278736174106598\n",
      "Epoch 99/100, Loss: 0.6278795659542084\n",
      "Epoch 100/100, Loss: 0.6280198812484741\n"
     ]
    }
   ],
   "source": [
    "meta_model = MetaLearner()\n",
    "meta_criterion = nn.BCELoss()\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=0.008)\n",
    "\n",
    "meta_X_train_tensor = torch.tensor(meta_X_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "meta_train_dataset = TensorDataset(meta_X_train_tensor, y_val_tensor)\n",
    "meta_train_loader = DataLoader(meta_train_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "train_model(meta_model, meta_optimizer, meta_criterion, meta_train_loader, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n",
      "Precision: 0.65, Recall: 1.0, F1 Score: 0.787878787878788\n",
      "meta predictions [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Accuracy for lstm: 0.35\n",
      "Precision for lstm: 0.0, Recall for lstm: 0.0, F1 Score for lstm: 0.0\n",
      "lstm predictions [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy for gru: 0.365\n",
      "gru predictions [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Precision for gru: 0.5, Recall for gru: 0.038461538461538464, F1 Score for gru: 0.07142857142857144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karlo/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#  the test dataset will be input into the sub-models again to produce intermediate test data for the meta-learner. Afterward, the meta-learner will use the intermediate test predictions from the sub-models to make the final predictions.\n",
    "lstm_test_predictions = lstm_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "gru_test_predictions = gru_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "meta_X_test = np.concatenate((lstm_test_predictions, gru_test_predictions), axis=1)\n",
    "meta_X_test_tensor = torch.tensor(meta_X_test, dtype=torch.float32)\n",
    "\n",
    "meta_test_predictions = meta_model(meta_X_test_tensor).detach().numpy()\n",
    "\n",
    "# Evaluation metrics\n",
    "meta_test_predictions = np.round(meta_test_predictions)\n",
    "accuracy = np.mean(meta_test_predictions == y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, meta_test_predictions, average='binary')\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "print(\"meta predictions\", meta_test_predictions)\n",
    "\n",
    "\n",
    "lstm_test_predictions = np.round(lstm_test_predictions)\n",
    "accuracy = np.mean(lstm_test_predictions == y_test)\n",
    "print(f'Accuracy for lstm: {accuracy}')\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, lstm_test_predictions, average='binary')\n",
    "print(f'Precision for lstm: {precision}, Recall for lstm: {recall}, F1 Score for lstm: {f1}')\n",
    "print(\"lstm predictions\", lstm_test_predictions)\n",
    "\n",
    "gru_test_predictions = np.round(gru_test_predictions)\n",
    "accuracy = np.mean(gru_test_predictions == y_test)\n",
    "print(f'Accuracy for gru: {accuracy}')\n",
    "print(\"gru predictions\", gru_test_predictions)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, gru_test_predictions, average='binary')\n",
    "print(f'Precision for gru: {precision}, Recall for gru: {recall}, F1 Score for gru: {f1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
