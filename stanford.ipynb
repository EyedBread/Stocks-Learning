{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the level 1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Hyperparameters\n",
    "timesteps = 10  # Number of time steps\n",
    "num_features = 5  # Number of features\n",
    "n_epochs = 100  # Number of epochs\n",
    "learning_rate = 0.001\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(num_features, 60, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.lstm2 = nn.LSTM(60, 55, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.lstm3 = nn.LSTM(55, 40, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.lstm4 = nn.LSTM(40, 55, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(55, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        x = x[:, -1, :]  # Take the last output, remove this?\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(num_features, 60, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.gru2 = nn.GRU(60, 55, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.gru3 = nn.GRU(55, 40, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.gru4 = nn.GRU(40, 55, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(55, 1)  # Ensures the output is of size [batch_size, 1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.gru3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.gru4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc(x)   # Linear layer to map to 1 output\n",
    "        x = x[:, -1, :]  # Take the last output, remove this?\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a fully-connect neuralnetwork with three layers; the activation function for this model is the Rectified Linear Unit (ReLu).\n",
    "# NOTE: The paper doesn't specify the number of neurons in the hidden layers, so I'm basing on the stanford paper\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 30)\n",
    "        self.fc2 = nn.Linear(30, 25)\n",
    "        self.fc3 = nn.Linear(25, 20)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        # x = self.fc4(x)\n",
    "        x = self.sigmoid(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF TRAIN (83, 5)\n",
      "DF VAL (19, 5)\n",
      "DF TEST (19, 5)\n",
      "DF TRAIN    wsj_mean_compound  cnbc_mean_compound  fortune_mean_compound  \\\n",
      "0              0.296             -0.1366                 0.0000   \n",
      "1              0.000              0.0000                -0.2423   \n",
      "2              0.000              0.0000                 0.0000   \n",
      "3              0.000              0.0000                 0.0000   \n",
      "4              0.000              0.0000                 0.0000   \n",
      "\n",
      "   reuters_mean_compound    Adj Close  \n",
      "0                    0.0  2636.979980  \n",
      "1                    0.0  2651.500000  \n",
      "2                    0.0  2659.989990  \n",
      "3                    0.0  2664.110107  \n",
      "4                    0.0  2662.850098  \n",
      "10\n",
      "NEW\n",
      "x_train.shape (72, 10, 5)\n",
      "y_train.shape (72,)\n",
      "x_val.shape (8, 10, 5)\n",
      "y_val.shape (8,)\n",
      "x_test.shape (8, 10, 5)\n",
      "y_test.shape (8,)\n",
      "[[0.91548214 0.         0.82728766 0.10191495 0.19179757]\n",
      " [0.34690741 0.31576514 0.40678881 0.10191495 0.2415458 ]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.27063404]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.28475031]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.28043329]\n",
      " [0.01549494 0.31576514 0.82728766 0.10191495 0.24329318]\n",
      " [0.         0.31576514 0.82728766 0.10191495 0.32483647]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.37400167]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.34422836]\n",
      " [0.34690741 0.31576514 0.82728766 0.10191495 0.33662233]]\n",
      "[1 0 0 1 1]\n",
      "[[0.32028808 0.41294732 0.82825113 0.1733249  0.25994479]\n",
      " [0.31745532 0.4259708  0.75866177 0.09811817 0.20964784]\n",
      " [0.29671103 0.45758086 0.67728109 0.15906251 0.28433877]\n",
      " [0.29118965 0.50551591 0.77864232 0.18551763 0.25799164]\n",
      " [0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      " [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      " [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      " [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      " [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      " [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]]\n",
      "0\n",
      "[[0.32610957 0.498087   0.76944387 0.26759267 0.31394061]\n",
      " [0.38574976 0.50526396 0.94749536 0.19756792 0.31150816]\n",
      " [0.39698747 0.51372288 0.83485069 0.2221625  0.40014387]\n",
      " [0.31509484 0.51934937 0.80658625 0.18104715 0.48675784]\n",
      " [0.27395342 0.48169417 0.77699124 0.18770471 0.50268925]\n",
      " [0.40076703 0.50464802 0.82945302 0.10321635 0.51094605]\n",
      " [0.30328349 0.48949446 0.80394212 0.10627921 0.44694521]\n",
      " [0.40488323 0.50721916 0.79442986 0.14275055 0.4846675 ]\n",
      " [0.30483637 0.48481091 0.65770141 0.09699912 0.47668423]\n",
      " [0.33894627 0.4781492  0.77809782 0.14909903 0.45215308]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "split = 0.69  # Adjust to allocate space for validation set\n",
    "val_split = 0.16  # 15% for validation, and implicitly 15% for test due to remaining percentage\n",
    "sequence_length = 10\n",
    "normalise = True\n",
    "batch_size = 100\n",
    "input_dim = 5\n",
    "input_timesteps = 9\n",
    "neurons = 50\n",
    "epochs = 5\n",
    "prediction_len = 1\n",
    "dense_output = 1\n",
    "drop_out = 0\n",
    "\n",
    "# This approach does not normalize the compound scores in the range of 0 to 1\n",
    "def naive():\n",
    "  # Load data, modify cols whenever necessary\n",
    "  dataframe = pd.read_csv(\"data/original_dataset/source_price.csv\")\n",
    "\n",
    "  cols = ['Adj Close', 'wsj_mean_compound', 'cnbc_mean_compound', 'fortune_mean_compound', 'reuters_mean_compound']\n",
    "\n",
    "  len_dataframe = dataframe.shape[0]\n",
    "\n",
    "  # Split data into train, validation, and test\n",
    "  i_split = int(len(dataframe) * split)\n",
    "  i_val = int(len(dataframe) * (split + val_split))\n",
    "\n",
    "  data_train = dataframe.get(cols).values[:i_split]\n",
    "  data_val = dataframe.get(cols).values[i_split:i_val]\n",
    "  data_test = dataframe.get(cols).values[i_val:]\n",
    "\n",
    "  # print(data_train[0:5,0])\n",
    "\n",
    "  len_train = len(data_train)\n",
    "  len_val = len(data_val)\n",
    "  len_test = len(data_test)\n",
    "  len_train_windows = None\n",
    "\n",
    "  # Process train data\n",
    "  data_windows = []\n",
    "  for i in range(len_train - sequence_length):\n",
    "      data_windows.append(data_train[i:i+sequence_length])\n",
    "  data_windows = np.array(data_windows).astype(float)\n",
    "  window_data = data_windows\n",
    "  win_num = window_data.shape[0]\n",
    "  col_num = window_data.shape[2]\n",
    "  normalised_data = []\n",
    "  record_min = []\n",
    "  record_max = []\n",
    "\n",
    "  # Normalize train data\n",
    "  for win_i in range(win_num):\n",
    "      normalised_window = []\n",
    "      for col_i in range(0,1):\n",
    "        temp_col = window_data[win_i,:,col_i]\n",
    "        temp_min = min(temp_col)\n",
    "        record_min.append(temp_min)\n",
    "        temp_col = temp_col - temp_min\n",
    "        temp_max = max(temp_col)\n",
    "        record_max.append(temp_max)\n",
    "        temp_col = temp_col / temp_max\n",
    "        normalised_window.append(temp_col)\n",
    "      for col_i in range(1,col_num):\n",
    "        normalised_window.append(window_data[win_i,:,col_i])\n",
    "      normalised_window = np.array(normalised_window).T\n",
    "      normalised_data.append(normalised_window)\n",
    "  normalised_data = np.array(normalised_data)\n",
    "  x_train = normalised_data[:, :-1]\n",
    "  # Classification problem now\n",
    "  y_train = []\n",
    "  for i in range(len_train - sequence_length):\n",
    "      current_last = data_train[i+sequence_length-1, 0]\n",
    "      next_first = data_train[i+sequence_length, 0]\n",
    "      y_train.append(1 if next_first > current_last else 0)\n",
    "  y_train = np.array(y_train)\n",
    "\n",
    "  # Process validation data\n",
    "  data_windows = []\n",
    "  for i in range(len_val - sequence_length):\n",
    "      data_windows.append(data_val[i:i+sequence_length])\n",
    "  data_windows = np.array(data_windows).astype(float)\n",
    "  window_data = data_windows\n",
    "  win_num = window_data.shape[0]\n",
    "  normalised_data = []\n",
    "\n",
    "  # Normalize validation data\n",
    "  for win_i in range(win_num):\n",
    "      normalised_window = []\n",
    "      for col_i in range(0,1):\n",
    "        temp_col = window_data[win_i,:,col_i]\n",
    "        temp_min = min(temp_col)\n",
    "        temp_col = temp_col - temp_min\n",
    "        temp_max = max(temp_col)\n",
    "        temp_col = temp_col / temp_max\n",
    "        normalised_window.append(temp_col)\n",
    "      for col_i in range(1,col_num):\n",
    "        normalised_window.append(window_data[win_i,:,col_i])\n",
    "      normalised_window = np.array(normalised_window).T\n",
    "      normalised_data.append(normalised_window)\n",
    "  normalised_data = np.array(normalised_data)\n",
    "  x_val = normalised_data[:, :-1]\n",
    "  y_val = []\n",
    "  for i in range(len_val - sequence_length):\n",
    "      current_last = data_val[i+sequence_length-1, 0]\n",
    "      next_first = data_val[i+sequence_length, 0]\n",
    "      y_val.append(1 if next_first > current_last else 0)\n",
    "  y_val = np.array(y_val)\n",
    "\n",
    "  # Process test data\n",
    "  data_windows = []\n",
    "  for i in range(len_test - sequence_length):\n",
    "      data_windows.append(data_test[i:i+sequence_length])\n",
    "  data_windows = np.array(data_windows).astype(float)\n",
    "  y_test_ori = data_windows[:, -1, [0]]\n",
    "  window_data = data_windows\n",
    "  win_num = window_data.shape[0]\n",
    "  normalised_data = []\n",
    "\n",
    "  # Normalize test data\n",
    "  for win_i in range(win_num):\n",
    "      normalised_window = []\n",
    "      for col_i in range(0,1):\n",
    "        temp_col = window_data[win_i,:,col_i]\n",
    "        temp_min = min(temp_col)\n",
    "        temp_col = temp_col - temp_min\n",
    "        temp_max = max(temp_col)\n",
    "        temp_col = temp_col / temp_max\n",
    "        normalised_window.append(temp_col)\n",
    "      for col_i in range(1,col_num):\n",
    "        normalised_window.append(window_data[win_i,:,col_i])\n",
    "      normalised_window = np.array(normalised_window).T\n",
    "      normalised_data.append(normalised_window)\n",
    "  normalised_data = np.array(normalised_data)\n",
    "  x_test = normalised_data[:, :-1]\n",
    "  y_test = []\n",
    "  for i in range(len_test - sequence_length):\n",
    "      current_last = data_test[i+sequence_length-1, 0]\n",
    "      next_first = data_test[i+sequence_length, 0]\n",
    "      y_test.append(1 if next_first > current_last else 0)\n",
    "  y_test = np.array(y_test)\n",
    "  return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "def new():\n",
    "  df = pd.read_csv('data/original_dataset/source_price.csv')\n",
    "  # Partition data into training, validation and test sets. Training data should be from date 12/07/2017 to 04/09/2018, validation data (from 04/10/2018 to 05/04/2018), and test data (from 05/07/2018 to 06/01/2018)\n",
    "\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "  # Hardcodidly extracting the exact dates for the partitioning\n",
    "  df_train = df.loc[0:82]\n",
    "  df_val = df.loc[83:101]\n",
    "  df_test = df.loc[102:]\n",
    "\n",
    "  # print(df_val.head())\n",
    "  # print(df_val.tail())\n",
    "  # print(df_test.head())\n",
    "  # print(df_val)\n",
    "\n",
    "  df_train = df_train.drop(columns=['date'])\n",
    "  df_val = df_val.drop(columns=['date'])\n",
    "  df_test = df_test.drop(columns=['date'])\n",
    "\n",
    "  print(\"DF TRAIN\", df_train.shape)\n",
    "  print(\"DF VAL\", df_val.shape)\n",
    "  print(\"DF TEST\", df_test.shape)\n",
    "\n",
    "  sc = MinMaxScaler(feature_range=(0,1))\n",
    "  print(\"DF TRAIN\", df_train.head())\n",
    "  df_train = sc.fit_transform(df_train)\n",
    "  df_val = sc.transform(df_val)\n",
    "  df_test = sc.transform(df_test)\n",
    "\n",
    "  def create_sequences_numpy_classification(data, n_days):\n",
    "      X, y = [], []\n",
    "      for i in range(n_days, len(data) - 1): \n",
    "          # print(\"X\")\n",
    "          X.append(data[i-n_days:i])\n",
    "          # print(data[i-n_days:i])\n",
    "          y.append(1 if data[i][-1] - data[i-1][-1] > 0 else 0) #Classification task\n",
    "          # print(data[i][-1], data[i-1][-1])\n",
    "          # print(\"Y\")\n",
    "          # print(y[-1])\n",
    "          \n",
    "      # Delete the first column of X\n",
    "      # X = np.delete(X, 0, axis=2)\n",
    "      return np.array(X), np.array(y)\n",
    "\n",
    "  # df = \n",
    "  print(timesteps)\n",
    "  x_train, y_train = create_sequences_numpy_classification(df_train, 10)\n",
    "  x_val, y_val = create_sequences_numpy_classification(df_val, 10)\n",
    "  x_test, y_test = create_sequences_numpy_classification(df_test, 10)\n",
    "  return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = new()\n",
    "\n",
    "print(\"NEW\")\n",
    "\n",
    "print('x_train.shape', x_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('x_val.shape', x_val.shape)\n",
    "print('y_val.shape', y_val.shape)\n",
    "print('x_test.shape', x_test.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "print(y_train[0:5])\n",
    "\n",
    "print(x_val[0])\n",
    "\n",
    "print(y_val[0])\n",
    "\n",
    "print(x_test[0])\n",
    "\n",
    "print(y_test[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instatiate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Instantiate models\n",
    "lstm_model = LSTMModel()\n",
    "gru_model = GRUModel()\n",
    "meta_model = MetaLearner()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "lstm_optimizer = optim.RMSprop(lstm_model.parameters(), lr=0.0008) # 16 batch size, 150 epochs\n",
    "gru_optimizer = optim.RMSprop(gru_model.parameters(), lr=0.0008) # 16 batch size, 200 epochs\n",
    "base_models_batch_size = 16\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=0.008) # 100 epochs, 8 batch size\n",
    "meta_learner_batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 10, 5])\n",
      "torch.Size([72])\n",
      "Training LSTM Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Loss: 0.6986353993415833\n",
      "Epoch 2/150, Loss: 0.6938703894615174\n",
      "Epoch 3/150, Loss: 0.696661901473999\n",
      "Epoch 4/150, Loss: 0.6915799856185914\n",
      "Epoch 5/150, Loss: 0.6926833987236023\n",
      "Epoch 6/150, Loss: 0.6932829141616821\n",
      "Epoch 7/150, Loss: 0.6923085927963257\n",
      "Epoch 8/150, Loss: 0.6862512826919556\n",
      "Epoch 9/150, Loss: 0.6850280284881591\n",
      "Epoch 10/150, Loss: 0.6908631801605225\n",
      "Epoch 11/150, Loss: 0.6861680746078491\n",
      "Epoch 12/150, Loss: 0.6898617744445801\n",
      "Epoch 13/150, Loss: 0.6925292611122131\n",
      "Epoch 14/150, Loss: 0.6879832029342652\n",
      "Epoch 15/150, Loss: 0.6860392928123474\n",
      "Epoch 16/150, Loss: 0.687736427783966\n",
      "Epoch 17/150, Loss: 0.6863203048706055\n",
      "Epoch 18/150, Loss: 0.6891567945480347\n",
      "Epoch 19/150, Loss: 0.6891563296318054\n",
      "Epoch 20/150, Loss: 0.6858827948570252\n",
      "Epoch 21/150, Loss: 0.6810502767562866\n",
      "Epoch 22/150, Loss: 0.6862143635749817\n",
      "Epoch 23/150, Loss: 0.6702996611595153\n",
      "Epoch 24/150, Loss: 0.6837924242019653\n",
      "Epoch 25/150, Loss: 0.6820971488952636\n",
      "Epoch 26/150, Loss: 0.681379497051239\n",
      "Epoch 27/150, Loss: 0.6848581314086915\n",
      "Epoch 28/150, Loss: 0.6759158611297608\n",
      "Epoch 29/150, Loss: 0.6704132795333863\n",
      "Epoch 30/150, Loss: 0.6794520258903504\n",
      "Epoch 31/150, Loss: 0.6727011680603028\n",
      "Epoch 32/150, Loss: 0.6840547084808349\n",
      "Epoch 33/150, Loss: 0.6722532629966735\n",
      "Epoch 34/150, Loss: 0.6690901279449463\n",
      "Epoch 35/150, Loss: 0.6736960172653198\n",
      "Epoch 36/150, Loss: 0.6736009120941162\n",
      "Epoch 37/150, Loss: 0.6770206451416015\n",
      "Epoch 38/150, Loss: 0.6717403054237365\n",
      "Epoch 39/150, Loss: 0.6635895729064941\n",
      "Epoch 40/150, Loss: 0.6634854197502136\n",
      "Epoch 41/150, Loss: 0.6547049641609192\n",
      "Epoch 42/150, Loss: 0.6588536858558655\n",
      "Epoch 43/150, Loss: 0.661042582988739\n",
      "Epoch 44/150, Loss: 0.6725362658500671\n",
      "Epoch 45/150, Loss: 0.6567389488220214\n",
      "Epoch 46/150, Loss: 0.6489871382713318\n",
      "Epoch 47/150, Loss: 0.667964506149292\n",
      "Epoch 48/150, Loss: 0.6562552928924561\n",
      "Epoch 49/150, Loss: 0.6400566935539246\n",
      "Epoch 50/150, Loss: 0.6476616501808167\n",
      "Epoch 51/150, Loss: 0.6561867952346802\n",
      "Epoch 52/150, Loss: 0.6543423533439636\n",
      "Epoch 53/150, Loss: 0.6684525847434998\n",
      "Epoch 54/150, Loss: 0.65312340259552\n",
      "Epoch 55/150, Loss: 0.6539366722106934\n",
      "Epoch 56/150, Loss: 0.6461013197898865\n",
      "Epoch 57/150, Loss: 0.6419208288192749\n",
      "Epoch 58/150, Loss: 0.6632041573524475\n",
      "Epoch 59/150, Loss: 0.6425201773643494\n",
      "Epoch 60/150, Loss: 0.6344585180282593\n",
      "Epoch 61/150, Loss: 0.6518083691596985\n",
      "Epoch 62/150, Loss: 0.6532814741134644\n",
      "Epoch 63/150, Loss: 0.6302621126174927\n",
      "Epoch 64/150, Loss: 0.629606556892395\n",
      "Epoch 65/150, Loss: 0.6395549893379211\n",
      "Epoch 66/150, Loss: 0.6402111649513245\n",
      "Epoch 67/150, Loss: 0.6407071471214294\n",
      "Epoch 68/150, Loss: 0.6659843921661377\n",
      "Epoch 69/150, Loss: 0.6763214349746705\n",
      "Epoch 70/150, Loss: 0.6371153712272644\n",
      "Epoch 71/150, Loss: 0.6322747468948364\n",
      "Epoch 72/150, Loss: 0.6279167175292969\n",
      "Epoch 73/150, Loss: 0.6427662134170532\n",
      "Epoch 74/150, Loss: 0.6463893532752991\n",
      "Epoch 75/150, Loss: 0.6642942667007447\n",
      "Epoch 76/150, Loss: 0.6435059428215026\n",
      "Epoch 77/150, Loss: 0.6423574924468994\n",
      "Epoch 78/150, Loss: 0.674801504611969\n",
      "Epoch 79/150, Loss: 0.628937029838562\n",
      "Epoch 80/150, Loss: 0.6381374597549438\n",
      "Epoch 81/150, Loss: 0.6320211529731751\n",
      "Epoch 82/150, Loss: 0.6424852609634399\n",
      "Epoch 83/150, Loss: 0.6108466386795044\n",
      "Epoch 84/150, Loss: 0.6420426726341247\n",
      "Epoch 85/150, Loss: 0.6413638472557068\n",
      "Epoch 86/150, Loss: 0.6466362237930298\n",
      "Epoch 87/150, Loss: 0.6521045804023743\n",
      "Epoch 88/150, Loss: 0.6525267720222473\n",
      "Epoch 89/150, Loss: 0.6315133810043335\n",
      "Epoch 90/150, Loss: 0.634727680683136\n",
      "Epoch 91/150, Loss: 0.6258744597434998\n",
      "Epoch 92/150, Loss: 0.6321923017501831\n",
      "Epoch 93/150, Loss: 0.6105999231338501\n",
      "Epoch 94/150, Loss: 0.6318512320518493\n",
      "Epoch 95/150, Loss: 0.6340052008628845\n",
      "Epoch 96/150, Loss: 0.6265552878379822\n",
      "Epoch 97/150, Loss: 0.609922218322754\n",
      "Epoch 98/150, Loss: 0.605818235874176\n",
      "Epoch 99/150, Loss: 0.6052365779876709\n",
      "Epoch 100/150, Loss: 0.6283528327941894\n",
      "Epoch 101/150, Loss: 0.6292659878730774\n",
      "Epoch 102/150, Loss: 0.6063246250152587\n",
      "Epoch 103/150, Loss: 0.6019388139247894\n",
      "Epoch 104/150, Loss: 0.6244434475898742\n",
      "Epoch 105/150, Loss: 0.6647603273391723\n",
      "Epoch 106/150, Loss: 0.6245812892913818\n",
      "Epoch 107/150, Loss: 0.5876734733581543\n",
      "Epoch 108/150, Loss: 0.6153520345687866\n",
      "Epoch 109/150, Loss: 0.603482460975647\n",
      "Epoch 110/150, Loss: 0.6212809324264527\n",
      "Epoch 111/150, Loss: 0.5939009428024292\n",
      "Epoch 112/150, Loss: 0.6226396799087525\n",
      "Epoch 113/150, Loss: 0.5993902444839477\n",
      "Epoch 114/150, Loss: 0.6003381609916687\n",
      "Epoch 115/150, Loss: 0.5988831877708435\n",
      "Epoch 116/150, Loss: 0.6006121158599853\n",
      "Epoch 117/150, Loss: 0.6137504696846008\n",
      "Epoch 118/150, Loss: 0.6270416021347046\n",
      "Epoch 119/150, Loss: 0.6072463393211365\n",
      "Epoch 120/150, Loss: 0.60038982629776\n",
      "Epoch 121/150, Loss: 0.5973702430725097\n",
      "Epoch 122/150, Loss: 0.5880599617958069\n",
      "Epoch 123/150, Loss: 0.60748291015625\n",
      "Epoch 124/150, Loss: 0.6059054374694824\n",
      "Epoch 125/150, Loss: 0.5925629794597626\n",
      "Epoch 126/150, Loss: 0.6049937009811401\n",
      "Epoch 127/150, Loss: 0.6092498779296875\n",
      "Epoch 128/150, Loss: 0.5973055124282837\n",
      "Epoch 129/150, Loss: 0.5749230265617371\n",
      "Epoch 130/150, Loss: 0.5895085453987121\n",
      "Epoch 131/150, Loss: 0.6031928062438965\n",
      "Epoch 132/150, Loss: 0.6088619470596314\n",
      "Epoch 133/150, Loss: 0.5959474146366119\n",
      "Epoch 134/150, Loss: 0.5663015484809876\n",
      "Epoch 135/150, Loss: 0.5489424586296081\n",
      "Epoch 136/150, Loss: 0.5937583088874817\n",
      "Epoch 137/150, Loss: 0.6102358102798462\n",
      "Epoch 138/150, Loss: 0.5937785744667053\n",
      "Epoch 139/150, Loss: 0.6149907350540161\n",
      "Epoch 140/150, Loss: 0.5940665364265442\n",
      "Epoch 141/150, Loss: 0.6002156615257264\n",
      "Epoch 142/150, Loss: 0.5859095573425293\n",
      "Epoch 143/150, Loss: 0.5757260918617249\n",
      "Epoch 144/150, Loss: 0.6005813598632812\n",
      "Epoch 145/150, Loss: 0.5769474744796753\n",
      "Epoch 146/150, Loss: 0.6028388381004334\n",
      "Epoch 147/150, Loss: 0.529511171579361\n",
      "Epoch 148/150, Loss: 0.5359444856643677\n",
      "Epoch 149/150, Loss: 0.5501181006431579\n",
      "Epoch 150/150, Loss: 0.5746125817298889\n",
      "Training GRU Model\n",
      "Epoch 1/200, Loss: 0.70435072183609\n",
      "Epoch 2/200, Loss: 0.6994317650794983\n",
      "Epoch 3/200, Loss: 0.6886569738388062\n",
      "Epoch 4/200, Loss: 0.6962579369544983\n",
      "Epoch 5/200, Loss: 0.692974865436554\n",
      "Epoch 6/200, Loss: 0.6888986468315125\n",
      "Epoch 7/200, Loss: 0.6977191805839539\n",
      "Epoch 8/200, Loss: 0.6884847164154053\n",
      "Epoch 9/200, Loss: 0.6832849621772766\n",
      "Epoch 10/200, Loss: 0.6839230060577393\n",
      "Epoch 11/200, Loss: 0.6840995192527771\n",
      "Epoch 12/200, Loss: 0.6778073787689209\n",
      "Epoch 13/200, Loss: 0.6722841382026672\n",
      "Epoch 14/200, Loss: 0.6723479390144348\n",
      "Epoch 15/200, Loss: 0.6733779072761535\n",
      "Epoch 16/200, Loss: 0.6773028254508973\n",
      "Epoch 17/200, Loss: 0.6662280201911926\n",
      "Epoch 18/200, Loss: 0.6662508487701416\n",
      "Epoch 19/200, Loss: 0.6738861322402954\n",
      "Epoch 20/200, Loss: 0.6641349911689758\n",
      "Epoch 21/200, Loss: 0.6593555569648742\n",
      "Epoch 22/200, Loss: 0.661528742313385\n",
      "Epoch 23/200, Loss: 0.6629361987113953\n",
      "Epoch 24/200, Loss: 0.6656404495239258\n",
      "Epoch 25/200, Loss: 0.6596513986587524\n",
      "Epoch 26/200, Loss: 0.6617808699607849\n",
      "Epoch 27/200, Loss: 0.6575140357017517\n",
      "Epoch 28/200, Loss: 0.6505121946334839\n",
      "Epoch 29/200, Loss: 0.6526507019996644\n",
      "Epoch 30/200, Loss: 0.6487130641937255\n",
      "Epoch 31/200, Loss: 0.638522493839264\n",
      "Epoch 32/200, Loss: 0.6499549269676208\n",
      "Epoch 33/200, Loss: 0.6473374962806702\n",
      "Epoch 34/200, Loss: 0.6426272988319397\n",
      "Epoch 35/200, Loss: 0.6494040131568909\n",
      "Epoch 36/200, Loss: 0.6439430117607117\n",
      "Epoch 37/200, Loss: 0.6492554783821106\n",
      "Epoch 38/200, Loss: 0.6286741018295288\n",
      "Epoch 39/200, Loss: 0.6407931089401245\n",
      "Epoch 40/200, Loss: 0.6420938968658447\n",
      "Epoch 41/200, Loss: 0.6404260635375977\n",
      "Epoch 42/200, Loss: 0.6325203061103821\n",
      "Epoch 43/200, Loss: 0.6299056768417358\n",
      "Epoch 44/200, Loss: 0.6295981884002686\n",
      "Epoch 45/200, Loss: 0.6308973550796508\n",
      "Epoch 46/200, Loss: 0.6328400135040283\n",
      "Epoch 47/200, Loss: 0.6397994756698608\n",
      "Epoch 48/200, Loss: 0.6233211398124695\n",
      "Epoch 49/200, Loss: 0.6395964980125427\n",
      "Epoch 50/200, Loss: 0.6416232109069824\n",
      "Epoch 51/200, Loss: 0.6422167897224427\n",
      "Epoch 52/200, Loss: 0.6311614632606506\n",
      "Epoch 53/200, Loss: 0.6352877259254456\n",
      "Epoch 54/200, Loss: 0.6326491236686707\n",
      "Epoch 55/200, Loss: 0.6318709135055542\n",
      "Epoch 56/200, Loss: 0.6334863066673279\n",
      "Epoch 57/200, Loss: 0.6322914242744446\n",
      "Epoch 58/200, Loss: 0.632863438129425\n",
      "Epoch 59/200, Loss: 0.6390574693679809\n",
      "Epoch 60/200, Loss: 0.6275718331336975\n",
      "Epoch 61/200, Loss: 0.6150687217712403\n",
      "Epoch 62/200, Loss: 0.6326336026191711\n",
      "Epoch 63/200, Loss: 0.6234302759170532\n",
      "Epoch 64/200, Loss: 0.6134767055511474\n",
      "Epoch 65/200, Loss: 0.6243000507354737\n",
      "Epoch 66/200, Loss: 0.6178128242492675\n",
      "Epoch 67/200, Loss: 0.6307668089866638\n",
      "Epoch 68/200, Loss: 0.6185875654220581\n",
      "Epoch 69/200, Loss: 0.6291450381278991\n",
      "Epoch 70/200, Loss: 0.6140851497650146\n",
      "Epoch 71/200, Loss: 0.6158913254737854\n",
      "Epoch 72/200, Loss: 0.6040984034538269\n",
      "Epoch 73/200, Loss: 0.592681062221527\n",
      "Epoch 74/200, Loss: 0.5952400803565979\n",
      "Epoch 75/200, Loss: 0.6124560832977295\n",
      "Epoch 76/200, Loss: 0.5904487252235413\n",
      "Epoch 77/200, Loss: 0.6073719382286071\n",
      "Epoch 78/200, Loss: 0.6070980548858642\n",
      "Epoch 79/200, Loss: 0.5922831892967224\n",
      "Epoch 80/200, Loss: 0.5993401169776916\n",
      "Epoch 81/200, Loss: 0.5914868831634521\n",
      "Epoch 82/200, Loss: 0.6070012331008912\n",
      "Epoch 83/200, Loss: 0.5995617389678956\n",
      "Epoch 84/200, Loss: 0.5974223136901855\n",
      "Epoch 85/200, Loss: 0.6152723073959351\n",
      "Epoch 86/200, Loss: 0.5684124588966369\n",
      "Epoch 87/200, Loss: 0.5851022839546204\n",
      "Epoch 88/200, Loss: 0.5705647349357605\n",
      "Epoch 89/200, Loss: 0.5675303101539612\n",
      "Epoch 90/200, Loss: 0.5863901853561402\n",
      "Epoch 91/200, Loss: 0.5745098888874054\n",
      "Epoch 92/200, Loss: 0.5701065361499786\n",
      "Epoch 93/200, Loss: 0.5515177607536316\n",
      "Epoch 94/200, Loss: 0.6143499135971069\n",
      "Epoch 95/200, Loss: 0.5744811356067657\n",
      "Epoch 96/200, Loss: 0.5449001073837281\n",
      "Epoch 97/200, Loss: 0.5254380583763123\n",
      "Epoch 98/200, Loss: 0.5594750583171845\n",
      "Epoch 99/200, Loss: 0.555292272567749\n",
      "Epoch 100/200, Loss: 0.5636626303195953\n",
      "Epoch 101/200, Loss: 0.6590239644050598\n",
      "Epoch 102/200, Loss: 0.5348379492759705\n",
      "Epoch 103/200, Loss: 0.5472063958644867\n",
      "Epoch 104/200, Loss: 0.5209719777107239\n",
      "Epoch 105/200, Loss: 0.5335317194461823\n",
      "Epoch 106/200, Loss: 0.5340404510498047\n",
      "Epoch 107/200, Loss: 0.5043910026550293\n",
      "Epoch 108/200, Loss: 0.5889627873897553\n",
      "Epoch 109/200, Loss: 0.5382548093795776\n",
      "Epoch 110/200, Loss: 0.5294712960720063\n",
      "Epoch 111/200, Loss: 0.5275657057762146\n",
      "Epoch 112/200, Loss: 0.5198842525482178\n",
      "Epoch 113/200, Loss: 0.510507869720459\n",
      "Epoch 114/200, Loss: 0.5871906816959381\n",
      "Epoch 115/200, Loss: 0.5365721464157105\n",
      "Epoch 116/200, Loss: 0.5402352452278137\n",
      "Epoch 117/200, Loss: 0.5336318373680115\n",
      "Epoch 118/200, Loss: 0.5321037292480468\n",
      "Epoch 119/200, Loss: 0.4984520196914673\n",
      "Epoch 120/200, Loss: 0.547612601518631\n",
      "Epoch 121/200, Loss: 0.4914235770702362\n",
      "Epoch 122/200, Loss: 0.4812369287014008\n",
      "Epoch 123/200, Loss: 0.46980431079864504\n",
      "Epoch 124/200, Loss: 0.4996915996074677\n",
      "Epoch 125/200, Loss: 0.4845238447189331\n",
      "Epoch 126/200, Loss: 0.48912397027015686\n",
      "Epoch 127/200, Loss: 0.5202138602733613\n",
      "Epoch 128/200, Loss: 0.47964953184127807\n",
      "Epoch 129/200, Loss: 0.5004045605659485\n",
      "Epoch 130/200, Loss: 0.47068498134613035\n",
      "Epoch 131/200, Loss: 0.49651169776916504\n",
      "Epoch 132/200, Loss: 0.5035234868526459\n",
      "Epoch 133/200, Loss: 0.49101121425628663\n",
      "Epoch 134/200, Loss: 0.48738929629325867\n",
      "Epoch 135/200, Loss: 0.43476955890655516\n",
      "Epoch 136/200, Loss: 0.45240585803985595\n",
      "Epoch 137/200, Loss: 0.5451268315315246\n",
      "Epoch 138/200, Loss: 0.5453553557395935\n",
      "Epoch 139/200, Loss: 0.47047520279884336\n",
      "Epoch 140/200, Loss: 0.4602289617061615\n",
      "Epoch 141/200, Loss: 0.4431460976600647\n",
      "Epoch 142/200, Loss: 0.43676837980747224\n",
      "Epoch 143/200, Loss: 0.4239677369594574\n",
      "Epoch 144/200, Loss: 0.4341380774974823\n",
      "Epoch 145/200, Loss: 0.41407089233398436\n",
      "Epoch 146/200, Loss: 0.4249069571495056\n",
      "Epoch 147/200, Loss: 0.36960704028606417\n",
      "Epoch 148/200, Loss: 0.4096546173095703\n",
      "Epoch 149/200, Loss: 0.5029695928096771\n",
      "Epoch 150/200, Loss: 0.4205128014087677\n",
      "Epoch 151/200, Loss: 0.3932771384716034\n",
      "Epoch 152/200, Loss: 0.3704089879989624\n",
      "Epoch 153/200, Loss: 0.37146849632263185\n",
      "Epoch 154/200, Loss: 0.37691686153411863\n",
      "Epoch 155/200, Loss: 0.48658387660980223\n",
      "Epoch 156/200, Loss: 0.5856284976005555\n",
      "Epoch 157/200, Loss: 0.408056640625\n",
      "Epoch 158/200, Loss: 0.37844143211841585\n",
      "Epoch 159/200, Loss: 0.3753988564014435\n",
      "Epoch 160/200, Loss: 0.4396313488483429\n",
      "Epoch 161/200, Loss: 0.5002660274505615\n",
      "Epoch 162/200, Loss: 0.4103642672300339\n",
      "Epoch 163/200, Loss: 0.39774441719055176\n",
      "Epoch 164/200, Loss: 0.381451016664505\n",
      "Epoch 165/200, Loss: 0.376922607421875\n",
      "Epoch 166/200, Loss: 0.3756300538778305\n",
      "Epoch 167/200, Loss: 0.4257838040590286\n",
      "Epoch 168/200, Loss: 0.35681505799293517\n",
      "Epoch 169/200, Loss: 0.38066794276237487\n",
      "Epoch 170/200, Loss: 0.34676169157028197\n",
      "Epoch 171/200, Loss: 0.34517442584037783\n",
      "Epoch 172/200, Loss: 0.33556506782770157\n",
      "Epoch 173/200, Loss: 0.3522682324051857\n",
      "Epoch 174/200, Loss: 0.4547760307788849\n",
      "Epoch 175/200, Loss: 0.3701806902885437\n",
      "Epoch 176/200, Loss: 0.4618023008108139\n",
      "Epoch 177/200, Loss: 0.43337163925170896\n",
      "Epoch 178/200, Loss: 0.3773427128791809\n",
      "Epoch 179/200, Loss: 0.36873811334371565\n",
      "Epoch 180/200, Loss: 0.3802020996809006\n",
      "Epoch 181/200, Loss: 0.3804880201816559\n",
      "Epoch 182/200, Loss: 0.3359871610999107\n",
      "Epoch 183/200, Loss: 0.35491052865982053\n",
      "Epoch 184/200, Loss: 0.38975390791893005\n",
      "Epoch 185/200, Loss: 0.3723422557115555\n",
      "Epoch 186/200, Loss: 0.402023109793663\n",
      "Epoch 187/200, Loss: 0.33133944794535636\n",
      "Epoch 188/200, Loss: 0.3254108473658562\n",
      "Epoch 189/200, Loss: 0.37337722778320315\n",
      "Epoch 190/200, Loss: 0.3548843950033188\n",
      "Epoch 191/200, Loss: 0.43029159903526304\n",
      "Epoch 192/200, Loss: 0.4925323247909546\n",
      "Epoch 193/200, Loss: 0.34601748287677764\n",
      "Epoch 194/200, Loss: 0.3586642324924469\n",
      "Epoch 195/200, Loss: 0.33435468673706054\n",
      "Epoch 196/200, Loss: 0.41123010218143463\n",
      "Epoch 197/200, Loss: 0.4668519198894501\n",
      "Epoch 198/200, Loss: 0.3257889449596405\n",
      "Epoch 199/200, Loss: 0.36449127495288847\n",
      "Epoch 200/200, Loss: 0.3424966335296631\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors and create DataLoader\n",
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=base_models_batch_size, shuffle=False)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, optimizer, criterion, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "# Train the LSTM model\n",
    "print(\"Training LSTM Model\")\n",
    "train_model(lstm_model, lstm_optimizer, criterion, train_loader, 150)\n",
    "\n",
    "# Train the GRU model\n",
    "print(\"Training GRU Model\")\n",
    "train_model(gru_model, gru_optimizer, criterion, train_loader, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use base models to predict the validation data, this will be used as input to the Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2)\n",
      "[[0.454582   0.9168079 ]\n",
      " [0.44063616 0.98396456]\n",
      " [0.4357485  0.9908494 ]\n",
      " [0.46254846 0.8609845 ]\n",
      " [0.8470854  0.9867219 ]\n",
      " [0.8092524  0.9968315 ]\n",
      " [0.7287035  0.9963653 ]\n",
      " [0.8628865  0.9828859 ]]\n",
      "[0 1 1 1 0 1 0 0]\n",
      "[[[0.32028808 0.41294732 0.82825113 0.1733249  0.25994479]\n",
      "  [0.31745532 0.4259708  0.75866177 0.09811817 0.20964784]\n",
      "  [0.29671103 0.45758086 0.67728109 0.15906251 0.28433877]\n",
      "  [0.29118965 0.50551591 0.77864232 0.18551763 0.25799164]\n",
      "  [0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      "  [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]]\n",
      "\n",
      " [[0.31745532 0.4259708  0.75866177 0.09811817 0.20964784]\n",
      "  [0.29671103 0.45758086 0.67728109 0.15906251 0.28433877]\n",
      "  [0.29118965 0.50551591 0.77864232 0.18551763 0.25799164]\n",
      "  [0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      "  [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]]\n",
      "\n",
      " [[0.29671103 0.45758086 0.67728109 0.15906251 0.28433877]\n",
      "  [0.29118965 0.50551591 0.77864232 0.18551763 0.25799164]\n",
      "  [0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      "  [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]]\n",
      "\n",
      " [[0.29118965 0.50551591 0.77864232 0.18551763 0.25799164]\n",
      "  [0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      "  [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]\n",
      "  [0.3768951  0.51940223 0.67926641 0.32705394 0.29444584]]\n",
      "\n",
      " [[0.29693836 0.489379   0.81943248 0.13741159 0.33179172]\n",
      "  [0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]\n",
      "  [0.3768951  0.51940223 0.67926641 0.32705394 0.29444584]\n",
      "  [0.36808754 0.51518182 0.87763535 0.34183591 0.3046215 ]]\n",
      "\n",
      " [[0.38502544 0.46885804 0.97367736 0.13808754 0.42960853]\n",
      "  [0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]\n",
      "  [0.3768951  0.51940223 0.67926641 0.32705394 0.29444584]\n",
      "  [0.36808754 0.51518182 0.87763535 0.34183591 0.3046215 ]\n",
      "  [0.32870867 0.46383826 0.86430149 0.24956852 0.22972564]]\n",
      "\n",
      " [[0.3327085  0.42533342 0.94910584 0.21256231 0.43731744]\n",
      "  [0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]\n",
      "  [0.3768951  0.51940223 0.67926641 0.32705394 0.29444584]\n",
      "  [0.36808754 0.51518182 0.87763535 0.34183591 0.3046215 ]\n",
      "  [0.32870867 0.46383826 0.86430149 0.24956852 0.22972564]\n",
      "  [0.2607303  0.51050885 0.80422258 0.25600435 0.25285236]]\n",
      "\n",
      " [[0.29614421 0.44631568 0.81592048 0.21472825 0.38417733]\n",
      "  [0.21226419 0.41306848 0.88312378 0.18540914 0.30540945]\n",
      "  [0.42276637 0.46862395 0.79618596 0.05588622 0.30592388]\n",
      "  [0.49093152 0.27744264 0.53226164 0.39694699 0.18350648]\n",
      "  [0.41531708 0.4546165  0.86488311 0.2591299  0.20008866]\n",
      "  [0.3768951  0.51940223 0.67926641 0.32705394 0.29444584]\n",
      "  [0.36808754 0.51518182 0.87763535 0.34183591 0.3046215 ]\n",
      "  [0.32870867 0.46383826 0.86430149 0.24956852 0.22972564]\n",
      "  [0.2607303  0.51050885 0.80422258 0.25600435 0.25285236]\n",
      "  [0.34807525 0.51931866 0.90250017 0.22192608 0.18730908]]]\n"
     ]
    }
   ],
   "source": [
    "lstm_val_predictions = lstm_model(torch.tensor(x_val, dtype=torch.float32)).detach().numpy()\n",
    "gru_val_predictions = gru_model(torch.tensor(x_val, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "\n",
    "# Combine predictions to form new training data for the meta-learner\n",
    "meta_X_train = np.concatenate((lstm_val_predictions, gru_val_predictions), axis=1)\n",
    "\n",
    "print(meta_X_train.shape)\n",
    "\n",
    "print(meta_X_train)\n",
    "\n",
    "print(y_val)\n",
    "\n",
    "print(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6976877450942993\n",
      "Epoch 2/20, Loss: 0.6921930313110352\n",
      "Epoch 3/20, Loss: 0.6886435151100159\n",
      "Epoch 4/20, Loss: 0.6856529712677002\n",
      "Epoch 5/20, Loss: 0.6828100681304932\n",
      "Epoch 6/20, Loss: 0.6790404915809631\n",
      "Epoch 7/20, Loss: 0.6743289232254028\n",
      "Epoch 8/20, Loss: 0.6688950061798096\n",
      "Epoch 9/20, Loss: 0.6626771092414856\n",
      "Epoch 10/20, Loss: 0.6551073789596558\n",
      "Epoch 11/20, Loss: 0.6460602283477783\n",
      "Epoch 12/20, Loss: 0.6359153389930725\n",
      "Epoch 13/20, Loss: 0.625495433807373\n",
      "Epoch 14/20, Loss: 0.6137111783027649\n",
      "Epoch 15/20, Loss: 0.6015245914459229\n",
      "Epoch 16/20, Loss: 0.5903951525688171\n",
      "Epoch 17/20, Loss: 0.5790141820907593\n",
      "Epoch 18/20, Loss: 0.5701298117637634\n",
      "Epoch 19/20, Loss: 0.5626406073570251\n",
      "Epoch 20/20, Loss: 0.557481050491333\n"
     ]
    }
   ],
   "source": [
    "meta_model = MetaLearner()\n",
    "meta_criterion = nn.BCELoss()\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=0.008)\n",
    "\n",
    "meta_X_train_tensor = torch.tensor(meta_X_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "meta_train_dataset = TensorDataset(meta_X_train_tensor, y_val_tensor)\n",
    "meta_train_loader = DataLoader(meta_train_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "train_model(meta_model, meta_optimizer, meta_criterion, meta_train_loader, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.32610957  0.498087    0.76944387  0.26759267  0.31394061]\n",
      "  [ 0.38574976  0.50526396  0.94749536  0.19756792  0.31150816]\n",
      "  [ 0.39698747  0.51372288  0.83485069  0.2221625   0.40014387]\n",
      "  [ 0.31509484  0.51934937  0.80658625  0.18104715  0.48675784]\n",
      "  [ 0.27395342  0.48169417  0.77699124  0.18770471  0.50268925]\n",
      "  [ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]]\n",
      "\n",
      " [[ 0.38574976  0.50526396  0.94749536  0.19756792  0.31150816]\n",
      "  [ 0.39698747  0.51372288  0.83485069  0.2221625   0.40014387]\n",
      "  [ 0.31509484  0.51934937  0.80658625  0.18104715  0.48675784]\n",
      "  [ 0.27395342  0.48169417  0.77699124  0.18770471  0.50268925]\n",
      "  [ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]]\n",
      "\n",
      " [[ 0.39698747  0.51372288  0.83485069  0.2221625   0.40014387]\n",
      "  [ 0.31509484  0.51934937  0.80658625  0.18104715  0.48675784]\n",
      "  [ 0.27395342  0.48169417  0.77699124  0.18770471  0.50268925]\n",
      "  [ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]]\n",
      "\n",
      " [[ 0.31509484  0.51934937  0.80658625  0.18104715  0.48675784]\n",
      "  [ 0.27395342  0.48169417  0.77699124  0.18770471  0.50268925]\n",
      "  [ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]\n",
      "  [ 0.39636161  0.47074905  0.69749021  0.11735529  0.52177332]]\n",
      "\n",
      " [[ 0.27395342  0.48169417  0.77699124  0.18770471  0.50268925]\n",
      "  [ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]\n",
      "  [ 0.39636161  0.47074905  0.69749021  0.11735529  0.52177332]\n",
      "  [ 0.40165     0.40960394  0.72438152  0.10560386  0.50282643]]\n",
      "\n",
      " [[ 0.40076703  0.50464802  0.82945302  0.10321635  0.51094605]\n",
      "  [ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]\n",
      "  [ 0.39636161  0.47074905  0.69749021  0.11735529  0.52177332]\n",
      "  [ 0.40165     0.40960394  0.72438152  0.10560386  0.50282643]\n",
      "  [ 0.40509072  0.42541211  0.84733207  0.04562473  0.48079632]]\n",
      "\n",
      " [[ 0.30328349  0.48949446  0.80394212  0.10627921  0.44694521]\n",
      "  [ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]\n",
      "  [ 0.39636161  0.47074905  0.69749021  0.11735529  0.52177332]\n",
      "  [ 0.40165     0.40960394  0.72438152  0.10560386  0.50282643]\n",
      "  [ 0.40509072  0.42541211  0.84733207  0.04562473  0.48079632]\n",
      "  [ 0.24549445  0.47860006  0.78264948  0.02243377  0.37297449]]\n",
      "\n",
      " [[ 0.40488323  0.50721916  0.79442986  0.14275055  0.4846675 ]\n",
      "  [ 0.30483637  0.48481091  0.65770141  0.09699912  0.47668423]\n",
      "  [ 0.33894627  0.4781492   0.77809782  0.14909903  0.45215308]\n",
      "  [ 0.4040808   0.50170713  0.96342935  0.08163443  0.52081389]\n",
      "  [ 0.30182122  0.47585571  0.83703661  0.06964207  0.49145127]\n",
      "  [ 0.39636161  0.47074905  0.69749021  0.11735529  0.52177332]\n",
      "  [ 0.40165     0.40960394  0.72438152  0.10560386  0.50282643]\n",
      "  [ 0.40509072  0.42541211  0.84733207  0.04562473  0.48079632]\n",
      "  [ 0.24549445  0.47860006  0.78264948  0.02243377  0.37297449]\n",
      "  [ 0.31354731  0.4038811   0.69371981 -0.03926118  0.48997825]]]\n",
      "tensor([[0.4394, 0.9355],\n",
      "        [0.4219, 0.2783],\n",
      "        [0.4892, 0.0841],\n",
      "        [0.6448, 0.8489],\n",
      "        [0.6497, 0.8727],\n",
      "        [0.6488, 0.8441],\n",
      "        [0.6311, 0.8821],\n",
      "        [0.5839, 0.9093]])\n",
      "[[0.7494763 ]\n",
      " [0.47062907]\n",
      " [0.32353875]\n",
      " [0.39982492]\n",
      " [0.40654486]\n",
      " [0.39064735]\n",
      " [0.44193912]\n",
      " [0.53567195]]\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Accuracy: 0.5625\n",
      "Precision: 0.5, Recall: 0.3333333333333333, F1 Score: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#  the test dataset will be input into the sub-models again to produce intermediate test data for the meta-learner. Afterward, the meta-learner will use the intermediate test predictions from the sub-models to make the final predictions.\n",
    "print(x_test)\n",
    "lstm_test_predictions = lstm_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "gru_test_predictions = gru_model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "meta_X_test = np.concatenate((lstm_test_predictions, gru_test_predictions), axis=1)\n",
    "meta_X_test_tensor = torch.tensor(meta_X_test, dtype=torch.float32)\n",
    "print(meta_X_test_tensor)\n",
    "\n",
    "meta_test_predictions = meta_model(meta_X_test_tensor).detach().numpy()\n",
    "print(meta_test_predictions)\n",
    "\n",
    "# Evaluation metrics\n",
    "meta_test_predictions = np.round(meta_test_predictions)\n",
    "print(meta_test_predictions)\n",
    "accuracy = np.mean(meta_test_predictions == y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, meta_test_predictions, average='binary')\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
